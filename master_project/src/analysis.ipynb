{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a08f131",
   "metadata": {},
   "source": [
    "\n",
    "# Bias Analysis: Response Changes Before and After Article Exposure\n",
    "\n",
    "This notebook analyzes the impact of exposure to biased articles (left and right-wing) on user responses. The analysis covers multiple dimensions, including overall changes, differences by political groups, and significance testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Extraction and Processing\n",
    "This section involves loading the necessary data for the analysis, including JSON files and response data from different user personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from scipy.stats import wilcoxon\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_responses_path = \"../data/processed/before_responses.json\"\n",
    "after_responses_path = \"../data/processed/after_responses.json\"\n",
    "persona_prompts_path = '../data/processed/persona_prompts.json'\n",
    "question_codes_path = '../data/raw/question_codes.json'\n",
    "user_ranks_path = \"../data/processed/user_ranks.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(filepath):\n",
    "    \"\"\"Loads JSON data from the provided file path and attempts to fix malformed sections.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        try:\n",
    "            return json.loads(content)  \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Malformed JSON detected: {str(e)}\")\n",
    "            print(\"Attempting to fix...\")\n",
    "\n",
    "            content_fixed = content.replace(\"][\", \"],[\")\n",
    "\n",
    "            try:\n",
    "                return json.loads(f\"[{content_fixed}]\")  \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error: Unable to decode JSON after attempt to fix. {str(e)}\")\n",
    "                return None\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {filepath} not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Failed to decode JSON from {filepath}. {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_after_responses(response_data):\n",
    "    flattened_data = []\n",
    "    \n",
    "    for user_responses in response_data:\n",
    "        for entry in user_responses:\n",
    "            user_id = entry['user_id']\n",
    "            question_code = entry['question_code']\n",
    "            bias = entry['bias']  # Assuming bias always exists\n",
    "            selected_option = entry['response'].get('selected_option')\n",
    "\n",
    "            flattened_data.append({\n",
    "                'user_id': user_id,\n",
    "                'question_code': question_code,\n",
    "                'bias': bias,\n",
    "                'selected_option': selected_option,\n",
    "                'question': entry.get('question')\n",
    "            })\n",
    "    \n",
    "    return flattened_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_question_code_mapping(question_codes_path):\n",
    "    \"\"\"Load and filter the question-to-code mapping from the question_codes.json file.\"\"\"\n",
    "    question_codes_data = load_json_data(question_codes_path)\n",
    "    \n",
    "    # List of valid question codes relevant to the analysis\n",
    "    valid_question_codes = ['F1A10_1', 'F2A6', 'F2A7', 'F2A9', 'F3A3_1', 'F3A6_1', 'F3A7_1', 'F3A8_1']\n",
    "    \n",
    "    # Filter only questions that are in the valid_question_codes list\n",
    "    filtered_data = [entry for entry in question_codes_data if entry['code'] in valid_question_codes]\n",
    "\n",
    "    # Create a mapping for each question from text to numerical codes\n",
    "    question_code_mapping = {}\n",
    "    for entry in filtered_data:\n",
    "        question = entry['question']\n",
    "        code = entry['code']\n",
    "        options = entry['options']\n",
    "        \n",
    "        # Reverse the options dictionary to map response text to numerical values\n",
    "        reversed_options = {v: k for k, v in options.items()}\n",
    "        question_code_mapping[question] = {'code': code, 'options': reversed_options}\n",
    "\n",
    "    return question_code_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_after_responses_dataframe(flattened_data):\n",
    "    \"\"\"\n",
    "    Converts the flattened after responses into a structured DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    \n",
    "    # Set a multi-index using user_id, question_code, and bias\n",
    "    df.set_index(['user_id', 'question_code', 'bias'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actual_question(full_prompt):\n",
    "    \"\"\"Extracts the actual question from the full prompt in the 'question' field and cleans it.\"\"\"\n",
    "    \n",
    "    # Use regex to find text between **Question**: and **Options**:\n",
    "    match = re.search(r'\\*\\*Question\\*\\*[:\\s]*(.*?)\\*\\*Options\\*\\*', full_prompt, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        question = match.group(1).strip()  # Extracted question\n",
    "        # Clean the question: remove extra spaces and newlines\n",
    "        question_cleaned = re.sub(r'\\s+', ' ', question).strip()\n",
    "        return question_cleaned\n",
    "    \n",
    "    # If no structured question is found, return the full prompt (fallback)\n",
    "    return full_prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_clean_dataframe_with_codes(response_data, question_code_mapping, has_bias=False):\n",
    "    \"\"\"\n",
    "    Converts a list of response data dictionaries into a structured DataFrame with question codes.\n",
    "    Handles optional fields like bias if present in the data (for after_responses).\n",
    "    Parameters:\n",
    "    - response_data: List of dictionaries containing the response data.\n",
    "    - question_code_mapping: A dictionary that maps question texts to their corresponding codes.\n",
    "    - has_bias: Boolean flag to indicate if the data includes bias (for after_responses).\n",
    "    \"\"\"\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for entry in response_data:\n",
    "        user_id = entry['user_id']\n",
    "        full_prompt = entry['question']  \n",
    "        selected_option = entry['response']['selected_option']\n",
    "        bias = entry.get('bias', None) if has_bias else None\n",
    "\n",
    "        question = extract_actual_question(full_prompt)\n",
    "        \n",
    "        if question is None:\n",
    "            print(f\"Warning: Could not extract question from prompt for user {user_id}. Skipping entry.\")\n",
    "            continue\n",
    "\n",
    "        question_code = question_code_mapping.get(question, {}).get('code')\n",
    "        \n",
    "        if question_code is None:\n",
    "            print(f\"Warning: No question_code found for question '{question}' for user {user_id}. Skipping entry.\")\n",
    "            continue\n",
    "\n",
    "        cleaned_entry = {\n",
    "            'user_id': user_id,\n",
    "            'question_code': question_code,\n",
    "            'bias': bias,\n",
    "            'selected_option': selected_option,\n",
    "            'question': question\n",
    "        }\n",
    "        \n",
    "        cleaned_data.append(cleaned_entry)\n",
    "    \n",
    "    df = pd.DataFrame(cleaned_data)\n",
    "    if has_bias:\n",
    "        df.set_index(['user_id', 'question_code', 'bias'], inplace=True)\n",
    "    else:\n",
    "        df.set_index(['user_id', 'question_code'], inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_responses_to_numeric(df, question_code_mapping):\n",
    "    \"\"\"\n",
    "    Maps the string-based responses to their corresponding numerical values using the question code mapping.\n",
    "    \"\"\"\n",
    "    def map_response(row):\n",
    "        question_text = row['question']  \n",
    "        selected_option = row['selected_option']  \n",
    "        \n",
    "       \n",
    "        if question_text in question_code_mapping:\n",
    "            mapping = question_code_mapping[question_text]['options']\n",
    "       \n",
    "            return mapping.get(selected_option, None)  \n",
    "        return None\n",
    "    \n",
    "   \n",
    "    df['numeric_response'] = df.apply(map_response, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_political_stance(persona_prompt):\n",
    "    \"\"\"Extracts the political stance from the persona prompt text.\"\"\"\n",
    "    match = re.search(r\"\\*\\*Your Own Political Position\\*\\*:\\s*You consider your political position to be\\s*'(.*?)'\\s*on the political scale\", persona_prompt)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()  # Extract the political position (e.g., 'Extreme Left')\n",
    "    return None\n",
    "\n",
    "\n",
    "# Load the persona prompts\n",
    "def load_persona_prompts(filepath):\n",
    "    \"\"\"Loads the persona prompts data and creates a mapping of user_id to political stance.\"\"\"\n",
    "    persona_data = load_json_data(filepath)  # Reusing load_json_data to load the file\n",
    "    if persona_data is None:\n",
    "        return {}\n",
    "\n",
    "    # Create a mapping of user_id to political stance\n",
    "    persona_mapping = {}\n",
    "    for entry in persona_data:\n",
    "        user_id = entry['user_id']\n",
    "        political_stance = extract_political_stance(entry['persona_prompt'])\n",
    "        persona_mapping[user_id] = political_stance\n",
    "    \n",
    "    return persona_mapping\n",
    "\n",
    "def add_political_stance_to_df(df, persona_mapping):\n",
    "    \"\"\"Adds political stance column to the given DataFrame based on user_id, and combines Far Left and Extreme Left.\"\"\"\n",
    "    # Map political stance from persona_mapping to the DataFrame\n",
    "    df['political_stance'] = df.index.get_level_values('user_id').map(persona_mapping)\n",
    "\n",
    "    # Merge Far Left and Extreme Left into a single group: Extreme Left\n",
    "    df['political_stance'] = df['political_stance'].replace({'Far Left': 'Extreme Left'})\n",
    "\n",
    "    # Debug: Check for missing political stance mappings\n",
    "    missing_stance_df = df[df['political_stance'].isna()]\n",
    "    if not missing_stance_df.empty:\n",
    "        print(\"These user_ids have no political stance mapped:\")\n",
    "        print(missing_stance_df.index.get_level_values('user_id').unique())\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malformed JSON detected: Extra data: line 178 column 2 (char 5656)\n",
      "Attempting to fix...\n"
     ]
    }
   ],
   "source": [
    "user_ranks = pd.read_csv(user_ranks_path)\n",
    "\n",
    "# Load the question code mapping for both before and after responses\n",
    "question_code_mapping = load_question_code_mapping(question_codes_path)\n",
    "persona_mapping = load_persona_prompts(persona_prompts_path)\n",
    "\n",
    "## Process the BEFORE data\n",
    "\n",
    "before_responses = load_json_data(before_responses_path)\n",
    "before_responses_df = create_clean_dataframe_with_codes(before_responses, question_code_mapping)\n",
    "before_responses_df = map_responses_to_numeric(before_responses_df, question_code_mapping)\n",
    "before_responses_df = add_political_stance_to_df(before_responses_df, persona_mapping)\n",
    "\n",
    "## Process the AFTER data\n",
    "\n",
    "after_responses = load_json_data(after_responses_path)\n",
    "flattened_after_responses = flatten_after_responses(after_responses)\n",
    "after_responses_df = create_after_responses_dataframe(flattened_after_responses)\n",
    "after_responses_df = map_responses_to_numeric(after_responses_df, question_code_mapping)\n",
    "after_responses_df = add_political_stance_to_df(after_responses_df, persona_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id question_code  numeric_response_before political_stance  \\\n",
      "1552  IDUS127802       F1A10_1                        6    Extreme Right   \n",
      "1553  IDUS127802       F1A10_1                        6    Extreme Right   \n",
      "1554  IDUS127802          F2A6                        5    Extreme Right   \n",
      "1555  IDUS127802          F2A6                        5    Extreme Right   \n",
      "1556  IDUS127802          F2A7                        2    Extreme Right   \n",
      "1557  IDUS127802          F2A7                        2    Extreme Right   \n",
      "1558  IDUS127802          F2A9                        5    Extreme Right   \n",
      "1559  IDUS127802          F2A9                        5    Extreme Right   \n",
      "1560  IDUS127802        F3A3_1                        6    Extreme Right   \n",
      "1561  IDUS127802        F3A3_1                        6    Extreme Right   \n",
      "\n",
      "      numeric_response_after   bias  response_change  reliability_score  \n",
      "1552                       7   left                1           0.727273  \n",
      "1553                       1  right               -5           0.727273  \n",
      "1554                       5   left                0           0.727273  \n",
      "1555                       5  right                0           0.727273  \n",
      "1556                       5   left                3           0.727273  \n",
      "1557                       2  right                0           0.727273  \n",
      "1558                       2   left               -3           0.727273  \n",
      "1559                       5  right                0           0.727273  \n",
      "1560                       2   left               -4           0.727273  \n",
      "1561                       6  right                0           0.727273  \n"
     ]
    }
   ],
   "source": [
    "def ensure_numeric_format(df, column):\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "before_responses_df = ensure_numeric_format(before_responses_df, 'numeric_response')\n",
    "after_responses_df = ensure_numeric_format(after_responses_df, 'numeric_response')\n",
    "\n",
    "# Merge the before and after responses DataFrames on user_id and question_code\n",
    "def merge_before_after_responses(before_df, after_df):\n",
    "    # Reset the index to bring 'user_id' and 'question_code' back as columns\n",
    "    before_df = before_df.reset_index()\n",
    "    after_df = after_df.reset_index()\n",
    "\n",
    "    merged_df = pd.merge(\n",
    "        before_df[['user_id', 'question_code', 'numeric_response', 'political_stance']],\n",
    "        after_df[['user_id', 'question_code', 'numeric_response', 'bias', 'political_stance']],\n",
    "        on=['user_id', 'question_code', 'political_stance'],  # Merge on user_id, question_code, and political_stance\n",
    "        suffixes=('_before', '_after')\n",
    "    )\n",
    "\n",
    "    # Calculate the change in response (after - before)\n",
    "    merged_df['response_change'] = merged_df['numeric_response_after'] - merged_df['numeric_response_before']\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Merge the reliability scores with the merged responses DataFrame\n",
    "def merge_reliability_scores(merged_df, reliability_df):\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        reliability_df[['user_id', 'reliability_score']],  # Ensure we only bring in the relevant columns\n",
    "        on='user_id',\n",
    "        how='left'  # Ensure we don't drop any data if reliability scores are missing\n",
    "    )\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "merged_responses_df = merge_before_after_responses(before_responses_df, after_responses_df)\n",
    "# Merge reliability scores into merged_responses_df\n",
    "merged_responses_with_reliability = merge_reliability_scores(merged_responses_df, user_ranks)\n",
    "\n",
    "# print(merged_responses_with_reliability.head())\n",
    "print(merged_responses_with_reliability.sort_values(by='reliability_score', ascending=False).head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Changes in Agents' Responses Before vs After Exposure to Biased Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Overall Change in Responses\n",
    "This step aims to assess how much the responses changed in general after exposure to articles. This involves calcultaing the average of the response_change across all agents and questions, and summarizing the overall shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Changes in Responses (Weighted by Reliability):\n",
      "  - Weighted avg change: -0.165\n",
      "  - Positive changes: 423\n",
      "  - Negative changes: 490\n",
      "  - No changes: 959\n"
     ]
    }
   ],
   "source": [
    "# Summary of overall changes in responses, weighted by reliability score\n",
    "def overall_changes_analysis_with_reliability(merged_df):\n",
    "    # Calculate the weighted average response change\n",
    "    weighted_avg_change = (merged_df['response_change'] * merged_df['reliability_score']).sum() / merged_df['reliability_score'].sum()\n",
    "\n",
    "    # Calculate the count of positive, negative, and no changes in response\n",
    "    positive_changes = (merged_df['response_change'] > 0).sum()\n",
    "    negative_changes = (merged_df['response_change'] < 0).sum()\n",
    "    no_changes = (merged_df['response_change'] == 0).sum()\n",
    "\n",
    "    return {\n",
    "        'weighted_avg_change': weighted_avg_change,\n",
    "        'positive_changes': positive_changes,\n",
    "        'negative_changes': negative_changes,\n",
    "        'no_changes': no_changes\n",
    "    }\n",
    "\n",
    "# Generic function to print analysis results in a more readable format\n",
    "def print_analysis_results(title, results):\n",
    "    print(f\"\\n{title}:\")\n",
    "    \n",
    "    for key, value in results.items():\n",
    "        # Handle pandas Series and other complex types\n",
    "        if isinstance(value, pd.Series):\n",
    "            print(f\"  - {key.replace('_', ' ').capitalize()}:\")\n",
    "            for idx, val in value.items():  # Use 'items()' instead of 'iteritems()'\n",
    "                print(f\"    - {idx}: {val}\")\n",
    "        \n",
    "        # Handle the case where a descriptive statistics table (from .describe()) is included\n",
    "        elif isinstance(value, pd.DataFrame) or isinstance(value, pd.Series) and 'count' in value.index:\n",
    "            print(f\"  - {key.replace('_', ' ').capitalize()} summary:\")\n",
    "            # Print descriptive stats in a readable format\n",
    "            for stat_key, stat_val in value.items():\n",
    "                print(f\"    - {stat_key.capitalize()}: {stat_val:.3f}\")\n",
    "        \n",
    "        # Format float values to 3 decimal places, otherwise print as-is\n",
    "        elif isinstance(value, (float, np.float64)):\n",
    "            print(f\"  - {key.replace('_', ' ').capitalize()}: {value:.3f}\")\n",
    "        \n",
    "        # Handle general integer or string cases\n",
    "        else:\n",
    "            print(f\"  - {key.replace('_', ' ').capitalize()}: {value}\")\n",
    "\n",
    "# Perform the overall changes analysis with reliability\n",
    "overall_changes_with_reliability = overall_changes_analysis_with_reliability(merged_responses_with_reliability)\n",
    "print_analysis_results(\"Overall Changes in Responses (Weighted by Reliability)\", overall_changes_with_reliability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Overall Change in Responses Given Left or Right Bias\n",
    "Here, we analyze the overall differences in user responses after exposure to left-wing versus right-wing biased articles, regardless of the political position of the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Response Changes Between Left and Right Bias (Weighted by Reliability)::\n",
      "  - Weighted avg left change: -0.096\n",
      "  - Weighted avg right change: -0.235\n",
      "  - Left summary:\n",
      "    - count: 936.0\n",
      "    - mean: -0.09188034188034189\n",
      "    - std: 1.7286816993744634\n",
      "    - min: -6.0\n",
      "    - 25%: -1.0\n",
      "    - 50%: 0.0\n",
      "    - 75%: 0.0\n",
      "    - max: 6.0\n",
      "  - Right summary:\n",
      "    - count: 936.0\n",
      "    - mean: -0.23397435897435898\n",
      "    - std: 1.9621443075966185\n",
      "    - min: -6.0\n",
      "    - 25%: -1.0\n",
      "    - 50%: 0.0\n",
      "    - 75%: 0.0\n",
      "    - max: 6.0\n"
     ]
    }
   ],
   "source": [
    "# Function to compare response changes between left- and right-biased articles, weighted by reliability\n",
    "def compare_left_right_bias_with_reliability(merged_df):\n",
    "    # Separate left-biased and right-biased responses\n",
    "    left_bias_df = merged_df[merged_df['bias'] == 'left']\n",
    "    right_bias_df = merged_df[merged_df['bias'] == 'right']\n",
    "\n",
    "    # Calculate weighted average response change for left and right bias\n",
    "    weighted_avg_left_change = (left_bias_df['response_change'] * left_bias_df['reliability_score']).sum() / left_bias_df['reliability_score'].sum()\n",
    "    weighted_avg_right_change = (right_bias_df['response_change'] * right_bias_df['reliability_score']).sum() / right_bias_df['reliability_score'].sum()\n",
    "\n",
    "    # Summary statistics for each bias\n",
    "    left_summary = left_bias_df['response_change'].describe()\n",
    "    right_summary = right_bias_df['response_change'].describe()\n",
    "\n",
    "    return {\n",
    "        'weighted_avg_left_change': weighted_avg_left_change,\n",
    "        'weighted_avg_right_change': weighted_avg_right_change,\n",
    "        'left_summary': left_summary,\n",
    "        'right_summary': right_summary\n",
    "    }\n",
    "\n",
    "# Perform the bias comparison with reliability\n",
    "bias_comparison_with_reliability = compare_left_right_bias_with_reliability(merged_responses_with_reliability)\n",
    "print_analysis_results(\"Comparison of Response Changes Between Left and Right Bias (Weighted by Reliability):\", bias_comparison_with_reliability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Overall Difference in Responses Between by Question\n",
    "Here, we analyze how each question changed after exposure to biased content (regardless of the type of bias introduced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Differences in Responses by Question (Weighted by Reliability):\n",
      "  - Question code:\n",
      "    - 2: F2A7\n",
      "    - 7: F3A8_1\n",
      "    - 3: F2A9\n",
      "    - 6: F3A7_1\n",
      "    - 5: F3A6_1\n",
      "    - 1: F2A6\n",
      "    - 4: F3A3_1\n",
      "    - 0: F1A10_1\n",
      "  - Weighted avg change:\n",
      "    - 2: 0.7040891675401825\n",
      "    - 7: 0.0858979221516273\n",
      "    - 3: 0.08544571948798961\n",
      "    - 6: 0.013718588889991123\n",
      "    - 5: -0.06693747480615407\n",
      "    - 1: -0.1485842206962621\n",
      "    - 4: -0.485728409298992\n",
      "    - 0: -1.5111583938089048\n",
      "  - Count:\n",
      "    - 2: 234\n",
      "    - 7: 234\n",
      "    - 3: 234\n",
      "    - 6: 234\n",
      "    - 5: 234\n",
      "    - 1: 234\n",
      "    - 4: 234\n",
      "    - 0: 234\n",
      "  - Std change:\n",
      "    - 2: 1.8450136636612966\n",
      "    - 7: 1.3208186204974606\n",
      "    - 3: 1.7554800298944457\n",
      "    - 6: 0.7115928634831238\n",
      "    - 5: 0.6754130775174092\n",
      "    - 1: 1.5333436203309454\n",
      "    - 4: 2.58972016372291\n",
      "    - 0: 2.610364282507759\n",
      "  - Min change:\n",
      "    - 2: -4\n",
      "    - 7: -6\n",
      "    - 3: -4\n",
      "    - 6: -3\n",
      "    - 5: -6\n",
      "    - 1: -4\n",
      "    - 4: -6\n",
      "    - 0: -6\n",
      "  - Max change:\n",
      "    - 2: 4\n",
      "    - 7: 6\n",
      "    - 3: 3\n",
      "    - 6: 2\n",
      "    - 5: 1\n",
      "    - 1: 4\n",
      "    - 4: 6\n",
      "    - 0: 6\n",
      "  - Median change:\n",
      "    - 2: 0.0\n",
      "    - 7: 0.0\n",
      "    - 3: 0.0\n",
      "    - 6: 0.0\n",
      "    - 5: 0.0\n",
      "    - 1: 0.0\n",
      "    - 4: 0.0\n",
      "    - 0: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze overall differences in responses by question, weighted by reliability\n",
    "def analyze_differences_by_question_with_reliability(merged_df):\n",
    "    # Group by question_code and calculate statistics for each question\n",
    "    question_analysis = merged_df.groupby('question_code').agg(\n",
    "        weighted_avg_change=('response_change', lambda x: (x * merged_df.loc[x.index, 'reliability_score']).sum() / merged_df.loc[x.index, 'reliability_score'].sum()),\n",
    "        count=('response_change', 'count'),\n",
    "        std_change=('response_change', 'std'),\n",
    "        min_change=('response_change', 'min'),\n",
    "        max_change=('response_change', 'max'),\n",
    "        median_change=('response_change', 'median')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort the results by the weighted average change to identify the most impacted questions\n",
    "    question_analysis = question_analysis.sort_values(by='weighted_avg_change', ascending=False)\n",
    "\n",
    "    return question_analysis\n",
    "\n",
    "\n",
    "# Perform the analysis by question with reliability\n",
    "question_analysis_with_reliability = analyze_differences_by_question_with_reliability(merged_responses_with_reliability)\n",
    "print_analysis_results(\"Overall Differences in Responses by Question (Weighted by Reliability)\", question_analysis_with_reliability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Most Impacted Questions**:\n",
    "   - **Question 2 (F2A7)**: Saw the largest positive shift with a weighted average change of **0.704**, indicating significant upward response changes. High variability (Std: 1.845) suggests diverse reactions to biased content.\n",
    "   - **Question 0 (F1A10_1)**: Experienced the largest negative shift with a weighted average change of **-1.511**. A high standard deviation (2.610) indicates substantial variability, with extreme negative changes pulling the average down.\n",
    "\n",
    "2. **Moderate Changes**:\n",
    "   - **Question 7 (F3A8_1)** and **Question 3 (F2A9)**: Both had small positive changes (**0.086** and **0.085**), with moderate variability in responses.\n",
    "   - **Question 4 (F3A3_1)**: Showed a moderate negative change (**-0.486**), but high variability (Std: 2.59) suggests strong, diverse shifts in responses.\n",
    "\n",
    "3. **Low Variability, Small Changes**:\n",
    "   - **Question 6 (F3A7_1)** and **Question 5 (F3A6_1)**: Displayed very small shifts in responses (0.014 and -0.067, respectively) with low standard deviation, meaning responses were generally consistent.\n",
    "\n",
    "4. **General Observations**:\n",
    "   - The median change for all questions was **0**, indicating that for most questions, the typical response did not change drastically. However, the presence of outliers resulted in significant shifts in average response changes for some questions.\n",
    "   - **Variability** was highest in questions 0 and 4, suggesting that these questions elicited extreme reactions, both positive and negative, to the biased content.\n",
    "\n",
    "Overall, exposure to biased content resulted in noticeable changes for certain questions, particularly questions 2 and 0, while others were less affected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Overall Differences in Responses by Political Group\n",
    "Here, we analyze how agents belonging to different political groups (Extreme Right vs Extreme Left) changed their responses after having been exposed to biased articles (regardless of the type of bias introduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Differences in Responses by Political Group (Weighted by Reliability):\n",
      "  - Political stance: ['Extreme Right', 'Extreme Left']\n",
      "  - Weighted avg change: [-0.06419248478766469, -0.32537257596573577]\n",
      "  - Count: [1088, 784]\n",
      "  - Std change: [2.1474344021664002, 1.3121254695505304]\n",
      "  - Min change: [-6, -6]\n",
      "  - Max change: [6, 6]\n",
      "  - Median change: [0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze overall differences in responses by political group, weighted by reliability\n",
    "def analyze_differences_by_political_group_with_reliability(merged_df):\n",
    "    # Group by political stance and calculate weighted statistics for each group\n",
    "    political_group_analysis = merged_df.groupby('political_stance').agg(\n",
    "        weighted_avg_change=('response_change', lambda x: (x * merged_df.loc[x.index, 'reliability_score']).sum() / merged_df.loc[x.index, 'reliability_score'].sum()),\n",
    "        count=('response_change', 'count'),\n",
    "        std_change=('response_change', 'std'),\n",
    "        min_change=('response_change', 'min'),\n",
    "        max_change=('response_change', 'max'),\n",
    "        median_change=('response_change', 'median')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort the results by the weighted average change to identify the most impacted groups\n",
    "    political_group_analysis = political_group_analysis.sort_values(by='weighted_avg_change', ascending=False)\n",
    "\n",
    "    return political_group_analysis\n",
    "\n",
    "# Perform the analysis by political group with reliability\n",
    "political_group_analysis_results = analyze_differences_by_political_group_with_reliability(merged_responses_with_reliability)\n",
    "\n",
    "# Display the results in a readable format\n",
    "print_analysis_results(\"Differences in Responses by Political Group (Weighted by Reliability)\", political_group_analysis_results.to_dict(orient='list'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Impact by Political Group**:\n",
    "   - **Extreme Left**: Experienced a more pronounced negative shift in responses (**-0.325**), indicating that biased content had a greater overall negative impact on this group.\n",
    "   - **Extreme Right**: Showed a smaller negative shift (**-0.064**), meaning biased content had a less pronounced effect on their responses overall.\n",
    "\n",
    "2. **Variability in Response**:\n",
    "   - **Extreme Right**: Exhibited greater variability (**Std: 2.147**), suggesting that responses were more dispersed, with individuals reacting in diverse ways.\n",
    "   - **Extreme Left**: Displayed less variability (**Std: 1.312**), meaning responses were more consistent, though still shifting negatively.\n",
    "\n",
    "3. **Overall Observations**:\n",
    "   - Both groups had a **median change of 0**, indicating the typical response did not shift significantly. However, extreme positive and negative changes were present in both groups (Min/Max: -6 to 6).\n",
    "   - The larger negative shift in the **Extreme Left** group suggests a stronger, more uniform reaction to biased content compared to the **Extreme Right**.\n",
    "\n",
    "Overall, biased content had a more negative and consistent impact on responses from the Extreme Left group, while the Extreme Right showed more variability but a smaller overall shift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Differences in responses by article bias\n",
    "Here, we analyze which type of bias (right, left) is associated with the biggest change in responses across all agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Differences in Responses by Article Bias (Weighted by Reliability):\n",
      "  - Bias: ['left', 'right']\n",
      "  - Weighted avg change: [-0.09606551951432775, -0.23474875562080288]\n",
      "  - Count: [936, 936]\n",
      "  - Std change: [1.7286816993744625, 1.9621443075966187]\n",
      "  - Min change: [-6, -6]\n",
      "  - Max change: [6, 6]\n",
      "  - Median change: [0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze differences in responses by article bias, weighted by reliability\n",
    "def analyze_differences_by_article_bias_only_with_reliability(merged_df):\n",
    "    # Group by article bias and calculate weighted statistics for response changes\n",
    "    bias_analysis = merged_df.groupby('bias').agg(\n",
    "        weighted_avg_change=('response_change', lambda x: (x * merged_df.loc[x.index, 'reliability_score']).sum() / merged_df.loc[x.index, 'reliability_score'].sum()),\n",
    "        count=('response_change', 'count'),\n",
    "        std_change=('response_change', 'std'),\n",
    "        min_change=('response_change', 'min'),\n",
    "        max_change=('response_change', 'max'),\n",
    "        median_change=('response_change', 'median')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort the results by the weighted average change to easily compare bias impact\n",
    "    bias_analysis = bias_analysis.sort_values(by='weighted_avg_change', ascending=False)\n",
    "\n",
    "    return bias_analysis\n",
    "\n",
    "# Perform the analysis by article bias only with reliability\n",
    "bias_only_analysis_results = analyze_differences_by_article_bias_only_with_reliability(merged_responses_with_reliability)\n",
    "\n",
    "# Display the results in a readable format\n",
    "print_analysis_results(\"Differences in Responses by Article Bias (Weighted by Reliability)\", bias_only_analysis_results.to_dict(orient='list'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Impact by Article Bias**:\n",
    "   - **Right-Leaning Bias**: Had a more pronounced negative impact on responses, with a weighted avg change of **-0.235**, compared to **-0.096** for left-leaning bias. \n",
    "   - Responses exposed to **left-leaning articles** experienced a smaller overall negative shift.\n",
    "\n",
    "2. **Variability in Responses**:\n",
    "   - The **right-leaning articles** showed higher variability (**Std: 1.962**) compared to left-leaning articles (**Std: 1.729**), indicating more diverse reactions to the right-leaning bias.\n",
    "   \n",
    "3. **Overall Observations**:\n",
    "   - Both biases resulted in a **median change of 0**, indicating that the typical response did not shift significantly for most responses, though extreme changes were present for both biases (Min/Max: -6 to 6).\n",
    "\n",
    "In conclusion, right-leaning bias had a stronger and more variable negative impact on responses compared to left-leaning bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Differences in responses by article bias by political group\n",
    "\n",
    "Here, we analyze how the type of bias (left vs right) impacted the response changes in the agents belonging to different political groups (Extreme Right vs Extreme Left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Differences in Responses by Article Bias and Political Group (Weighted by Reliability):\n",
      "  - Bias: ['left', 'right', 'left', 'right']\n",
      "  - Political stance: ['Extreme Right', 'Extreme Right', 'Extreme Left', 'Extreme Left']\n",
      "  - Weighted avg change: [-0.011347613397486368, -0.11703735617784304, -0.2299585539707651, -0.42078659796070633]\n",
      "  - Count: [544, 544, 392, 392]\n",
      "  - Std change: [1.9786532768393057, 2.304655506955763, 1.2927687763630848, 1.3251941036265789]\n",
      "  - Min change: [-6, -6, -6, -6]\n",
      "  - Max change: [6, 6, 6, 5]\n",
      "  - Median change: [0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze differences in responses by article bias and political group, weighted by reliability\n",
    "def analyze_differences_by_article_bias_and_group_with_reliability(merged_df):\n",
    "    # Group by article bias and political stance, then calculate weighted statistics for response changes\n",
    "    bias_group_analysis = merged_df.groupby(['bias', 'political_stance']).agg(\n",
    "        weighted_avg_change=('response_change', lambda x: (x * merged_df.loc[x.index, 'reliability_score']).sum() / merged_df.loc[x.index, 'reliability_score'].sum()),\n",
    "        count=('response_change', 'count'),\n",
    "        std_change=('response_change', 'std'),\n",
    "        min_change=('response_change', 'min'),\n",
    "        max_change=('response_change', 'max'),\n",
    "        median_change=('response_change', 'median')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort the results to easily compare bias impact\n",
    "    bias_group_analysis = bias_group_analysis.sort_values(by='weighted_avg_change', ascending=False)\n",
    "\n",
    "    return bias_group_analysis\n",
    "\n",
    "# Perform the analysis by article bias and political group with reliability\n",
    "bias_group_analysis_results = analyze_differences_by_article_bias_and_group_with_reliability(merged_responses_with_reliability)\n",
    "\n",
    "# Display the results in a readable format\n",
    "print_analysis_results(\"Differences in Responses by Article Bias and Political Group (Weighted by Reliability)\", bias_group_analysis_results.to_dict(orient='list'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to be read as follows:\n",
    "Exposing Extreme Right agents to left news articles caused an average change of 0.014 steps in the scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Impact of Bias on Different Political Groups**:\n",
    "   - **Right-Leaning Articles**: Political groups, especially those leaning left, may experience a more negative shift in their responses when exposed to right-leaning biased articles.\n",
    "   - **Left-Leaning Articles**: Political groups on the right might see smaller or potentially positive shifts when exposed to left-leaning articles, though this varies depending on the group.\n",
    "\n",
    "2. **Variability in Responses**:\n",
    "   - **Extreme Right** and **Extreme Left** groups often show high variability (high standard deviation), indicating that responses within these groups can vary significantly depending on the bias of the article.\n",
    "   - Groups exposed to **right-leaning articles** might show greater variability, with more extreme shifts in both directions compared to left-leaning articles.\n",
    "\n",
    "3. **Median Change**:\n",
    "   - **Median changes** close to zero suggest that while the overall weighted average might show shifts, the bulk of responses remain relatively stable, and outliers or more extreme responses are driving changes in the weighted average.\n",
    "\n",
    "4. **General Observations**:\n",
    "   - The **interaction of article bias and political stance** shows that the effect of bias is not uniform across political groups. Right-leaning biases tend to have a stronger impact on left-leaning respondents, and vice versa, but variability in responses suggests that individual reactions can differ widely.\n",
    "\n",
    "Overall, responses from different political stances are affected differently by article bias, with the **right-leaning bias** having a stronger negative effect on left-leaning groups, while **left-leaning bias** shows smaller shifts, particularly in right-leaning groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Differences in responses by article bias, by political group, and by \n",
    "Here, we include the question in the analysis, and observe which questions were most affected by which bias across each political group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Differences in Responses by Article Bias, Political Group, and Question (Weighted by Reliability):\n",
      "  - Bias: ['left', 'right', 'right', 'left', 'right', 'left', 'left', 'left', 'left', 'right', 'left', 'right', 'left', 'left', 'right', 'left', 'right', 'right', 'right', 'left', 'right', 'left', 'right', 'right', 'right', 'right', 'left', 'left', 'right', 'left', 'left', 'right']\n",
      "  - Political stance: ['Extreme Right', 'Extreme Right', 'Extreme Right', 'Extreme Right', 'Extreme Right', 'Extreme Left', 'Extreme Right', 'Extreme Left', 'Extreme Left', 'Extreme Left', 'Extreme Left', 'Extreme Right', 'Extreme Right', 'Extreme Left', 'Extreme Right', 'Extreme Right', 'Extreme Right', 'Extreme Right', 'Extreme Left', 'Extreme Right', 'Extreme Left', 'Extreme Left', 'Extreme Left', 'Extreme Left', 'Extreme Left', 'Extreme Left', 'Extreme Left', 'Extreme Right', 'Extreme Left', 'Extreme Left', 'Extreme Right', 'Extreme Right']\n",
      "  - Question code: ['F2A7', 'F2A9', 'F3A3_1', 'F3A8_1', 'F2A6', 'F3A7_1', 'F2A6', 'F1A10_1', 'F2A9', 'F1A10_1', 'F3A3_1', 'F3A6_1', 'F3A6_1', 'F3A8_1', 'F2A7', 'F3A7_1', 'F3A7_1', 'F3A8_1', 'F2A7', 'F1A10_1', 'F3A3_1', 'F3A6_1', 'F3A6_1', 'F3A7_1', 'F3A8_1', 'F2A6', 'F2A7', 'F2A9', 'F2A9', 'F2A6', 'F3A3_1', 'F1A10_1']\n",
      "  - Weighted avg change: [2.985761699233108, 2.0119181552371757, 1.418642463584724, 0.726126822144722, 0.6636392020583776, 0.4005806172571276, 0.3564400479571824, 0.2807046952165522, 0.15485760157838527, 0.1451160646820213, 0.08598271792058744, 0.028512216836500202, 0.008680252060160737, 0.0026418920461040566, 0.0008717585206530496, 0.0, 0.0, 0.0, -0.09823650563343819, -0.1441681971634008, -0.1536820089054311, -0.17265397846289657, -0.23158572413569767, -0.3297801511445002, -0.7069424967513263, -0.7755063187101138, -0.9882661047355237, -1.0616893253763762, -1.215675643087165, -1.6035158725864571, -2.961932206035287, -5.059882645660176]\n",
      "  - Count: [68, 68, 68, 68, 68, 49, 68, 49, 49, 49, 49, 68, 68, 49, 68, 68, 68, 68, 49, 68, 49, 49, 49, 49, 49, 49, 49, 68, 49, 49, 68, 68]\n",
      "  - Std change: [0.6464599351554542, 1.0833361473140557, 2.278415341984037, 1.659503188928834, 1.0513589182890146, 0.7615326472247369, 1.1409644273190005, 1.0109939878839118, 0.5000000000000001, 1.1591106137750558, 1.3755023819394454, 0.17021393345759914, 0.12126781251816648, 1.171878543078544, 1.0074350466081317, 0.0, 0.0, 0.0, 0.978945010372561, 1.4241119408310428, 1.2804123485475003, 1.0988553104013643, 0.9304011277419011, 1.2532271267781097, 1.4463183379511848, 1.4889045418339877, 1.2316683006073867, 1.0579146914062778, 1.488904541833988, 1.6085972255395709, 2.3853886510758158, 1.2090527714035604]\n",
      "  - Min change: [2, 0, -2, 0, -1, -1, -1, -2, -1, -5, -3, 0, 0, -2, -1, 0, 0, 0, -3, -6, -3, -6, -6, -3, -6, -4, -4, -3, -4, -4, -6, -6]\n",
      "  - Max change: [4, 3, 6, 6, 4, 2, 4, 6, 1, 5, 5, 1, 1, 4, 3, 0, 0, 0, 2, 6, 4, 1, 1, 2, 1, 3, 1, 1, 2, 1, 2, 2]\n",
      "  - Median change: [3.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, -1.0, -3.5, -5.0]\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze differences by article bias, political group, and question, weighted by reliability\n",
    "def analyze_differences_by_bias_group_question_with_reliability(merged_df):\n",
    "    # Group by article bias, political stance, and question_code, then calculate weighted statistics for response changes\n",
    "    bias_group_question_analysis = merged_df.groupby(['bias', 'political_stance', 'question_code']).agg(\n",
    "        weighted_avg_change=('response_change', lambda x: (x * merged_df.loc[x.index, 'reliability_score']).sum() / merged_df.loc[x.index, 'reliability_score'].sum()),\n",
    "        count=('response_change', 'count'),\n",
    "        std_change=('response_change', 'std'),\n",
    "        min_change=('response_change', 'min'),\n",
    "        max_change=('response_change', 'max'),\n",
    "        median_change=('response_change', 'median')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort the results by the weighted average change to easily compare impact\n",
    "    bias_group_question_analysis = bias_group_question_analysis.sort_values(by='weighted_avg_change', ascending=False)\n",
    "\n",
    "    return bias_group_question_analysis\n",
    "\n",
    "# Perform the analysis by article bias, political group, and question with reliability\n",
    "bias_group_question_analysis_results = analyze_differences_by_bias_group_question_with_reliability(merged_responses_with_reliability)\n",
    "\n",
    "# Display the results in a readable format\n",
    "print_analysis_results(\"Differences in Responses by Article Bias, Political Group, and Question (Weighted by Reliability)\", bias_group_question_analysis_results.to_dict(orient='list'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Impact of Article Bias on Political Groups for Specific Questions**:\n",
    "   - **Extreme Right respondents** exposed to **left-leaning content** experienced strong positive shifts, particularly on **Question F2A7** (**+2.99**), while **Extreme Left respondents** exposed to **right-leaning content** showed large negative shifts, especially on **Question F1A10_1** (**-5.06**).\n",
    "   \n",
    "2. **Variability in Responses**:\n",
    "   - Certain combinations, such as **Extreme Right respondents** for **right-leaning content on Question F3A3_1**, exhibited high variability (**Std: 2.28**), indicating diverse reactions to biased content. Other combinations showed little to no variability, with responses changing uniformly.\n",
    "\n",
    "3. **Range of Response Changes**:\n",
    "   - Responses ranged from extreme negative changes (**-6**) to strong positive shifts (**+6**) across multiple question-bias combinations, highlighting the polarized effects of biased content on different political groups.\n",
    "\n",
    "4. **Neutral Central Tendency**:\n",
    "   - Many combinations had a **median change of 0**, suggesting that for the majority of responses, the overall bias exposure didn’t drastically shift responses, with extreme outliers influencing the averages.\n",
    "\n",
    "Overall, responses to biased content vary greatly depending on the **interaction of article bias, political stance, and specific questions**, with the most extreme shifts seen in cases where political stance and article bias were in opposition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Radicalization Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Stable Responses Analysis\n",
    "Here, we look for the presence of extreme responses prior to article exposure. These responses do not effectively contribute to the radicalisation analysis, since they are already found at the extremes of the scale and cannot be further pushed to the extremes. In such cases the scale caps the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of stable responses at the extremes (Weighted by Reliability): 82.35%\n"
     ]
    }
   ],
   "source": [
    "# Function to check stable responses at extremes, weighted by reliability\n",
    "def check_stable_responses_at_extreme_with_reliability(merged_df, stable_threshold=0.5):\n",
    "    # Filter for responses where response_change is close to zero (stable responses)\n",
    "    stable_responses_df = merged_df[merged_df['response_change'].abs() <= stable_threshold].copy()\n",
    "\n",
    "    # For F2 questions, the scale is 1-5, for others it's 1-7\n",
    "    def is_at_extreme(row):\n",
    "        if row['question_code'].startswith('F2'):\n",
    "            return row['numeric_response_before'] in [1, 5]  # Extreme values for F2 questions\n",
    "        else:\n",
    "            return row['numeric_response_before'] in [1, 7]  # Extreme values for non-F2 questions\n",
    "\n",
    "    # Apply the extreme check function to each row\n",
    "    stable_responses_df.loc[:, 'at_extreme'] = stable_responses_df.apply(is_at_extreme, axis=1)\n",
    "    \n",
    "    # Calculate the weighted count of stable responses at extremes\n",
    "    stable_at_extreme_count = (stable_responses_df['at_extreme'] * stable_responses_df['reliability_score']).sum()\n",
    "\n",
    "    # Calculate the weighted percentage of stable responses that were at the extremes\n",
    "    total_stable_responses = (stable_responses_df['reliability_score']).sum()\n",
    "    percentage_at_extreme = (stable_at_extreme_count / total_stable_responses) * 100 if total_stable_responses > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'total_stable_responses': total_stable_responses,\n",
    "        'stable_at_extreme_count': stable_at_extreme_count,\n",
    "        'percentage_at_extreme': percentage_at_extreme\n",
    "    }\n",
    "\n",
    "# Run the updated analysis to avoid the warning and incorporate reliability\n",
    "extreme_stability_results_with_reliability = check_stable_responses_at_extreme_with_reliability(merged_responses_with_reliability)\n",
    "percentage = extreme_stability_results_with_reliability[\"percentage_at_extreme\"]\n",
    "print(f\"Percentage of stable responses at the extremes (Weighted by Reliability): {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "82.35% of the responses are already extreme even before exposure to biased articles. Such responses can either remain stable or be mitigated by the introduction of biased articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Identifying Political Leaning Based on Response Patterns\n",
    "\n",
    "The `generate_political_mapping` function is designed to analyze responses from the dataset containing real human responses. It compares the average responses from individuals identified as \"Extreme Left\" and \"Extreme Right\" to determine the direction of political leaning for each question. Specifically, the function creates a mapping that indicates whether smaller or larger response values correspond to left-leaning or right-leaning political views. \n",
    "\n",
    "This process is done to identify which questions exhibit a clear divide between left-leaning and right-leaning perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the political mapping based on actual numeric responses\n",
    "def generate_political_mapping(df):\n",
    "    \"\"\"\n",
    "    Generates a political stance mapping for each question based on the average responses \n",
    "    of users with extreme left and extreme right political stances.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing user responses and political stances.\n",
    "\n",
    "    Returns:\n",
    "    - political_mapping (dict): A dictionary where keys are question codes, and values are\n",
    "                                True if smaller values represent left-leaning views, False if larger\n",
    "                                values represent left-leaning views.\n",
    "    \"\"\"\n",
    "    # Separate data by political stance\n",
    "    left_responses = df[df['political_stance'] == 'Extreme Left']\n",
    "    right_responses = df[df['political_stance'] == 'Extreme Right']\n",
    "\n",
    "    # Initialize a dictionary to store the mapping\n",
    "    political_mapping = {}\n",
    "\n",
    "    # Replace with the actual columns containing numeric responses\n",
    "    # Assume these are 'numeric_response_before' or 'numeric_response_after' \n",
    "    # (you might need to adjust these column names based on your dataset)\n",
    "    question_columns = df['question_code'].unique()  # Get unique question codes\n",
    "\n",
    "    for question_code in question_columns:\n",
    "        # Filter the DataFrame to get responses for this specific question\n",
    "        left_responses_q = left_responses[left_responses['question_code'] == question_code]\n",
    "        right_responses_q = right_responses[right_responses['question_code'] == question_code]\n",
    "\n",
    "        # Calculate the average response for left-wing and right-wing users\n",
    "        left_mean = left_responses_q['numeric_response_before'].mean()  # Use the appropriate column for responses\n",
    "        right_mean = right_responses_q['numeric_response_before'].mean()\n",
    "\n",
    "        # Determine whether smaller values represent left-leaning views\n",
    "        if left_mean < right_mean:\n",
    "            political_mapping[question_code] = True  # Smaller values = left-leaning\n",
    "        else:\n",
    "            political_mapping[question_code] = False  # Larger values = left-leaning\n",
    "\n",
    "    return political_mapping\n",
    "\n",
    "# Generate the political mapping based on user responses\n",
    "question_political_mapping = generate_political_mapping(merged_responses_with_reliability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Reinforcement of Right-Wing and Left-Wing Opinions\n",
    "This analysis examines whether agents' opinions were reinforced depending on whether they were exposed to articles that aligned with their pre-existing political views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Right Wing Reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement of Right-Wing Opinions (Extreme Right Agents, Right-Biased Articles, Weighted by Reliability):\n",
      "Total right-wing responses (weighted): 223.42599338734254\n",
      "Reinforced responses (weighted): 60.001361172761285\n",
      "Percentage of reinforced responses (weighted): 26.86%\n",
      "Stable non-reinforced responses at extremes (weighted): 41.12440379140114\n",
      "Percentage of stable non-reinforced responses at extremes (weighted): 18.41%\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze right-wing reinforcement and stable extremes, weighted by reliability\n",
    "def analyze_reinforcement_and_stable_extremes_with_reliability(merged_df, question_political_mapping):\n",
    "    # Filter for Extreme Right users and right-wing biased articles\n",
    "    right_wing_df = merged_df[(merged_df['political_stance'] == 'Extreme Right') & (merged_df['bias'] == 'right')].copy()\n",
    "\n",
    "    # Function to check if the response was reinforced based on political mapping\n",
    "    def is_reinforced(row):\n",
    "        is_left_leaning = question_political_mapping.get(row['question_code'], False)\n",
    "        if is_left_leaning:\n",
    "            return row['numeric_response_after'] > row['numeric_response_before']\n",
    "        else:\n",
    "            return row['numeric_response_after'] > row['numeric_response_before']\n",
    "\n",
    "    # Check if the non-reinforced response was already at an extreme\n",
    "    def was_stable_at_extreme(row):\n",
    "        is_left_leaning = question_political_mapping.get(row['question_code'], False)\n",
    "        if is_left_leaning:\n",
    "            return row['numeric_response_before'] in [1, 2]  # Extreme for left-leaning questions\n",
    "        else:\n",
    "            return row['numeric_response_before'] in [6, 7]  # Extreme for right-leaning questions\n",
    "\n",
    "    # Apply the reinforcement and extreme checks\n",
    "    right_wing_df['reinforced'] = right_wing_df.apply(is_reinforced, axis=1)\n",
    "    right_wing_df['stable_at_extreme'] = right_wing_df.apply(was_stable_at_extreme, axis=1)\n",
    "\n",
    "    # Calculate total responses weighted by reliability\n",
    "    total_weighted_responses = right_wing_df['reliability_score'].sum()\n",
    "    \n",
    "    # Calculate reinforced responses weighted by reliability\n",
    "    reinforced_weighted = (right_wing_df['reinforced'] * right_wing_df['reliability_score']).sum()\n",
    "\n",
    "    # Calculate stable at extreme responses weighted by reliability\n",
    "    stable_at_extreme_weighted = (right_wing_df['stable_at_extreme'] * right_wing_df['reliability_score']).sum()\n",
    "\n",
    "    # Calculate percentages safely\n",
    "    percentage_reinforced = (reinforced_weighted / total_weighted_responses) * 100 if total_weighted_responses > 0 else 0\n",
    "    percentage_stable_at_extreme = (stable_at_extreme_weighted / total_weighted_responses) * 100 if total_weighted_responses > 0 else 0\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        'total_right_wing_responses': total_weighted_responses,\n",
    "        'reinforced_responses': reinforced_weighted,\n",
    "        'percentage_reinforced': round(percentage_reinforced, 2),\n",
    "        'stable_at_extreme_count': stable_at_extreme_weighted,\n",
    "        'percentage_stable_at_extreme': round(percentage_stable_at_extreme, 2)\n",
    "    }\n",
    "\n",
    "# Perform the analysis for right-wing users with reliability\n",
    "right_wing_extended_results_with_reliability = analyze_reinforcement_and_stable_extremes_with_reliability(merged_responses_with_reliability, question_political_mapping)\n",
    "\n",
    "# Display the results\n",
    "print(\"Reinforcement of Right-Wing Opinions (Extreme Right Agents, Right-Biased Articles, Weighted by Reliability):\")\n",
    "print(f\"Total right-wing responses (weighted): {right_wing_extended_results_with_reliability['total_right_wing_responses']}\")\n",
    "print(f\"Reinforced responses (weighted): {right_wing_extended_results_with_reliability['reinforced_responses']}\")\n",
    "print(f\"Percentage of reinforced responses (weighted): {right_wing_extended_results_with_reliability['percentage_reinforced']}%\")\n",
    "print(f\"Stable non-reinforced responses at extremes (weighted): {right_wing_extended_results_with_reliability['stable_at_extreme_count']}\")\n",
    "print(f\"Percentage of stable non-reinforced responses at extremes (weighted): {right_wing_extended_results_with_reliability['percentage_stable_at_extreme']}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Stable Responses at Extremes**:\n",
    "   - **82.35%** of responses were already extreme before exposure, meaning these responses cannot contribute significantly to radicalization as they are capped by the scale.\n",
    "\n",
    "2. **Reinforcement of Right-Wing Opinions**:\n",
    "   - Among **Extreme Right respondents** exposed to **right-wing biased articles**:\n",
    "     - **26.86%** of responses were reinforced, showing an increase in agreement with right-wing views.\n",
    "     - **18.41%** of responses were stable at extreme values, meaning they were already at the extreme ends of the scale before exposure and remained there.\n",
    "   \n",
    "Overall, biased articles reinforce right-wing opinions in about a quarter of the cases, but a significant proportion of responses remain unaffected as they were already at the extreme ends of the scale before exposure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Left Wing Reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement of Left-Wing Opinions (Extreme Left Agents, Left-Biased Articles, Weighted by Reliability):\n",
      "Total left-wing responses (weighted): 141.3679390320283\n",
      "Reinforced responses (weighted): 38.24454493650617\n",
      "Percentage of reinforced responses (weighted): 27.05%\n",
      "Stable non-reinforced responses at extremes (weighted): 81.78854535636725\n",
      "Percentage of stable non-reinforced responses at extremes (weighted): 57.86%\n",
      "Non-reinforced responses (weighted): 103.12339409552212\n",
      "Percentage of non-reinforced responses (weighted): 72.95%\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze left-wing reinforcement and stable extremes, weighted by reliability\n",
    "def analyze_reinforcement_and_stable_extremes_left_wing_with_reliability(merged_df, question_political_mapping):\n",
    "    # Filter for Extreme Left users and left-wing biased articles\n",
    "    left_wing_df = merged_df[(merged_df['political_stance'] == 'Extreme Left') & (merged_df['bias'] == 'left')].copy()\n",
    "\n",
    "    # Function to check if the response was reinforced based on political mapping\n",
    "    def is_reinforced(row):\n",
    "        is_left_leaning = question_political_mapping.get(row['question_code'], False)\n",
    "        if is_left_leaning:\n",
    "            return row['numeric_response_after'] < row['numeric_response_before']\n",
    "        else:\n",
    "            return row['numeric_response_after'] < row['numeric_response_before']\n",
    "\n",
    "    # Check if the non-reinforced response was already at an extreme\n",
    "    def was_stable_at_extreme(row):\n",
    "        is_left_leaning = question_political_mapping.get(row['question_code'], False)\n",
    "        if is_left_leaning:\n",
    "            return row['numeric_response_before'] in [1, 2]  # Extreme for left-leaning questions\n",
    "        else:\n",
    "            return row['numeric_response_before'] in [6, 7]  # Extreme for right-leaning questions\n",
    "\n",
    "    # Apply the reinforcement and extreme checks\n",
    "    left_wing_df['reinforced'] = left_wing_df.apply(is_reinforced, axis=1)\n",
    "    left_wing_df['stable_at_extreme'] = left_wing_df.apply(was_stable_at_extreme, axis=1)\n",
    "\n",
    "    # Calculate total responses weighted by reliability\n",
    "    total_weighted_responses = left_wing_df['reliability_score'].sum()\n",
    "    \n",
    "    # Calculate reinforced responses weighted by reliability\n",
    "    reinforced_weighted = (left_wing_df['reinforced'] * left_wing_df['reliability_score']).sum()\n",
    "\n",
    "    # Calculate stable at extreme responses weighted by reliability\n",
    "    stable_at_extreme_weighted = (left_wing_df['stable_at_extreme'] * left_wing_df['reliability_score']).sum()\n",
    "\n",
    "    # Calculate percentages safely\n",
    "    percentage_reinforced = (reinforced_weighted / total_weighted_responses) * 100 if total_weighted_responses > 0 else 0\n",
    "    percentage_stable_at_extreme = (stable_at_extreme_weighted / total_weighted_responses) * 100 if total_weighted_responses > 0 else 0\n",
    "\n",
    "    # Calculate non-reinforced responses\n",
    "    total_non_reinforced_weighted = total_weighted_responses - reinforced_weighted\n",
    "    percentage_non_reinforced = (total_non_reinforced_weighted / total_weighted_responses) * 100 if total_weighted_responses > 0 else 0\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        'total_left_wing_responses': total_weighted_responses,\n",
    "        'reinforced_responses': reinforced_weighted,\n",
    "        'percentage_reinforced': round(percentage_reinforced, 2),\n",
    "        'stable_at_extreme_count': stable_at_extreme_weighted,\n",
    "        'percentage_stable_at_extreme': round(percentage_stable_at_extreme, 2),\n",
    "        'non_reinforced_responses': total_non_reinforced_weighted,\n",
    "        'percentage_non_reinforced': round(percentage_non_reinforced, 2)\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `merged_responses_with_reliability` and `question_political_mapping` are already defined\n",
    "left_wing_extended_results_with_reliability = analyze_reinforcement_and_stable_extremes_left_wing_with_reliability(\n",
    "    merged_responses_with_reliability, question_political_mapping\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Reinforcement of Left-Wing Opinions (Extreme Left Agents, Left-Biased Articles, Weighted by Reliability):\")\n",
    "print(f\"Total left-wing responses (weighted): {left_wing_extended_results_with_reliability['total_left_wing_responses']}\")\n",
    "print(f\"Reinforced responses (weighted): {left_wing_extended_results_with_reliability['reinforced_responses']}\")\n",
    "print(f\"Percentage of reinforced responses (weighted): {left_wing_extended_results_with_reliability['percentage_reinforced']}%\")\n",
    "print(f\"Stable non-reinforced responses at extremes (weighted): {left_wing_extended_results_with_reliability['stable_at_extreme_count']}\")\n",
    "print(f\"Percentage of stable non-reinforced responses at extremes (weighted): {left_wing_extended_results_with_reliability['percentage_stable_at_extreme']}%\")\n",
    "print(f\"Non-reinforced responses (weighted): {left_wing_extended_results_with_reliability['non_reinforced_responses']}\")\n",
    "print(f\"Percentage of non-reinforced responses (weighted): {left_wing_extended_results_with_reliability['percentage_non_reinforced']}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Reinforcement of Left-Wing Opinions**:\n",
    "   - Among **Extreme Left respondents** exposed to **left-wing biased articles**:\n",
    "     - **27.05%** of responses were reinforced, meaning that they exhibited a further leftward shift after exposure.\n",
    "     - **57.86%** of responses were stable at extreme left values, indicating that they were already at the extreme ends of the scale before exposure and remained there.\n",
    "\n",
    "2. **Non-Reinforced Responses**:\n",
    "   - **72.95%** of responses were not reinforced, showing that most responses did not shift leftward after exposure, including a substantial proportion that was already at the extremes.\n",
    "\n",
    "Overall, biased articles reinforce left-wing opinions in about a quarter of the cases, but a large portion of responses remain unaffected, either because they are already at the extreme left or because they did not shift further after exposure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM to persona alignment before vs after exposure to articles: \n",
    "The following section aims at exploring whether the llm more consistent with the actual person's responses after it was exposed to\n",
    "1. right biased article\n",
    "2. left biased articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_path = '../data/processed/filtered_data.csv'\n",
    "real_responses = pd.read_csv(filtered_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_difference(real_response_code, llm_response_code, reliability_score=1):\n",
    "    \"\"\"\n",
    "    Calculates the weighted difference between real and LLM response codes, adjusted by reliability.\n",
    "    \n",
    "    Parameters:\n",
    "    - real_response_code: The real response code (numeric or categorical).\n",
    "    - llm_response_code: The response code generated by the LLM (numeric or categorical).\n",
    "    - reliability_score: A scaling factor (reliability) for the difference.\n",
    "    \n",
    "    Returns:\n",
    "    - The weighted difference between the real and LLM response codes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        real_response_code = float(real_response_code)\n",
    "        llm_response_code = float(llm_response_code)\n",
    "    except ValueError:\n",
    "        return reliability_score if real_response_code != llm_response_code else 0\n",
    "\n",
    "    # Calculate the absolute difference between the codes, weighted by reliability\n",
    "    difference = abs(real_response_code - llm_response_code)\n",
    "    return difference * reliability_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['selected_option', 'question', 'numeric_response', 'political_stance'], dtype='object')"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_responses_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_llm_responses(after_responses_df):\n",
    "    \"\"\"\n",
    "    Extract LLM responses and map them into a structured format, adding llm_response_code.\n",
    "    \"\"\"\n",
    "    llm_responses_mapped = []\n",
    "    \n",
    "    for idx, row in after_responses_df.iterrows():\n",
    "        user_id, question_code, bias = idx  # Unpack from the index\n",
    "        \n",
    "        selected_option = row['selected_option']\n",
    "        reliability_score = row.get('reliability_score', None)\n",
    "\n",
    "        # Assuming llm_response_code is mapped from selected_option, you can add logic here.\n",
    "        llm_response_code = row['numeric_response']  # If it's a numeric response\n",
    "\n",
    "        llm_responses_mapped.append({\n",
    "            'user_id': user_id,\n",
    "            'question_code': question_code,\n",
    "            'bias': bias,\n",
    "            'selected_option': selected_option,\n",
    "            'llm_response_code': llm_response_code,\n",
    "            'reliability_score': reliability_score  # Include reliability score if present\n",
    "        })\n",
    "    \n",
    "    return llm_responses_mapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_responses_reset = real_responses.reset_index()  # Move unique_id (user_id) from the index to a column\n",
    "\n",
    "real_responses_melted = pd.melt(\n",
    "    real_responses_reset,\n",
    "    id_vars=['unique_id'],  # This is the user_id\n",
    "    var_name='question_code',  # Column name for question codes (e.g., F1A10_1)\n",
    "    value_name='numeric_response'  # Column name for the real responses\n",
    ")\n",
    "\n",
    "# Rename 'unique_id' to 'user_id' for consistency\n",
    "real_responses_melted.rename(columns={'unique_id': 'user_id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Overall Comparison between LLM (after exposure to bias) and Human Responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_llm_to_real(llm_responses_mapped, real_responses_melted):\n",
    "    # Convert the mapped LLM responses to a DataFrame\n",
    "    llm_df = pd.DataFrame(llm_responses_mapped)\n",
    "    \n",
    "    # Merge the LLM responses with real responses on user_id and question_code\n",
    "    comparison_df = pd.merge(\n",
    "        real_responses_melted,\n",
    "        llm_df,\n",
    "        on=['user_id', 'question_code'],\n",
    "        suffixes=('_real', '_llm')\n",
    "    )\n",
    "\n",
    "    # Ensure numeric comparison between real and LLM responses\n",
    "    if 'numeric_response' in comparison_df.columns and 'llm_response_code' in comparison_df.columns:\n",
    "        comparison_df['difference'] = comparison_df['numeric_response'] - comparison_df['llm_response_code']\n",
    "        \n",
    "        # Weighted average difference (weighted by reliability_score)\n",
    "        if comparison_df['reliability_score'].notna().sum() > 0:  # Only calculate if there are valid reliability scores\n",
    "            avg_difference = (comparison_df['difference'] * comparison_df['reliability_score']).sum() / comparison_df['reliability_score'].sum()\n",
    "        else:\n",
    "            avg_difference = comparison_df['difference'].mean()  # Fallback to unweighted average if no reliability scores\n",
    "    else:\n",
    "        avg_difference = None\n",
    "    \n",
    "    return comparison_df, avg_difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"LLM Responses Mapped:\", llm_responses_mapped[:5])  # First 5 entries\n",
    "#print(\"Real Responses:\", real_responses[:5])  # First 5 entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['selected_option', 'question', 'numeric_response', 'political_stance'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(after_responses_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_responses_mapped' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[314], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m real_responses_melted \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmelt(\n\u001b[0;32m      2\u001b[0m     real_responses\u001b[38;5;241m.\u001b[39mreset_index(), \n\u001b[0;32m      3\u001b[0m     id_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_id\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      4\u001b[0m     var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_code\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      5\u001b[0m     value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric_response\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m )\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Now compare the LLM responses to the real responses\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m comparison_df, avg_difference \u001b[38;5;241m=\u001b[39m compare_llm_to_real(\u001b[43mllm_responses_mapped\u001b[49m, real_responses_melted)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(comparison_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage weighted difference between LLM and real responses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_difference\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm_responses_mapped' is not defined"
     ]
    }
   ],
   "source": [
    "real_responses_melted = pd.melt(\n",
    "    real_responses.reset_index(), \n",
    "    id_vars=['unique_id'], \n",
    "    var_name='question_code', \n",
    "    value_name='numeric_response'\n",
    ").rename(columns={'unique_id': 'user_id'})\n",
    "\n",
    "# Now compare the LLM responses to the real responses\n",
    "comparison_df, avg_difference = compare_llm_to_real(llm_responses_mapped, real_responses_melted)\n",
    "\n",
    "print(comparison_df.head())\n",
    "print(f\"Average weighted difference between LLM and real responses: {avg_difference}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Difference beteween LLM and Real Responses by Article Bias\n",
    "Comparing the responses of the real people with the responses of their LLM counterparts that had been exposed to politically biased articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_responses_by_bias(comparison_df, bias_type):\n",
    "    \"\"\"\n",
    "    Analyzes the comparison between real and LLM responses, filtered by bias.\n",
    "\n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - bias_type (str): The bias type to filter by ('right' or 'left').\n",
    "\n",
    "    Returns:\n",
    "    - avg_difference (float): The average weighted difference for the selected bias type.\n",
    "    - filtered_comparison_df (DataFrame): The filtered DataFrame containing only the selected bias type.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame based on the selected bias\n",
    "    filtered_comparison_df = comparison_df[comparison_df['bias'] == bias_type]\n",
    "\n",
    "    # Calculate the average weighted difference for the selected bias\n",
    "    avg_difference = filtered_comparison_df['difference'].mean()\n",
    "\n",
    "    return avg_difference, filtered_comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Comparison with Right-Biased Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[316], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check the column names\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcomparison_df\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Ensure 'bias' exists in comparison_df\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m comparison_df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comparison_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Check the column names\n",
    "print(comparison_df.columns)\n",
    "\n",
    "# Ensure 'bias' exists in comparison_df\n",
    "if 'bias' not in comparison_df.columns:\n",
    "    raise KeyError(\"'bias' column is missing from comparison_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[247], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m avg_difference_right, filtered_right_df \u001b[38;5;241m=\u001b[39m analyze_responses_by_bias(\u001b[43mcomparison_df\u001b[49m, bias_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComparison for Right-Biased Articles:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered_right_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comparison_df' is not defined"
     ]
    }
   ],
   "source": [
    "avg_difference_right, filtered_right_df = analyze_responses_by_bias(comparison_df, bias_type='right')\n",
    "\n",
    "print(\"Comparison for Right-Biased Articles:\")\n",
    "print(filtered_right_df.head())\n",
    "print(f\"Average weighted difference for right-biased articles: {avg_difference_right}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Comparison with Left-Biased Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[248], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m avg_difference_left, filtered_left_df \u001b[38;5;241m=\u001b[39m analyze_responses_by_bias(\u001b[43mcomparison_df\u001b[49m, bias_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComparison for Left-Biased Articles:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered_left_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comparison_df' is not defined"
     ]
    }
   ],
   "source": [
    "avg_difference_left, filtered_left_df = analyze_responses_by_bias(comparison_df, bias_type='left')\n",
    "\n",
    "print(\"Comparison for Left-Biased Articles:\")\n",
    "print(filtered_left_df.head())\n",
    "print(f\"Average weighted difference for left-biased articles: {avg_difference_left}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Comparison with Left-Biased Article on Subset of Left-Wing Users\n",
    "Measuring the effect of left wing biased article on the alignment between Left-Wing users and their LLM counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_political_stance_to_real_responses(real_responses_df, after_responses_df):\n",
    "    \"\"\"\n",
    "    Adds political stance information from after_responses_df to real_responses_df.\n",
    "    \"\"\"\n",
    "    # Reset the index for after_responses_df so that 'user_id' becomes a column\n",
    "    after_responses_df_reset = after_responses_df.reset_index()\n",
    "\n",
    "    # Ensure 'unique_id' is treated as 'user_id' (convert to string)\n",
    "    real_responses_df['user_id'] = real_responses_df['unique_id'].astype(str)\n",
    "    after_responses_df_reset['user_id'] = after_responses_df_reset['user_id'].astype(str)\n",
    "\n",
    "    # Extract political stance mapping from after_responses_df\n",
    "    stance_mapping = after_responses_df_reset[['user_id', 'political_stance']].drop_duplicates()\n",
    "\n",
    "    # Merge the political stance into the real_responses_df using 'user_id' (which is actually 'unique_id')\n",
    "    real_responses_with_stance = real_responses_df.merge(stance_mapping, on='user_id', how='left')\n",
    "\n",
    "    return real_responses_with_stance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_reinforcement_patterns(df, political_stance, bias_type):\n",
    "    \"\"\"\n",
    "    Track reinforcement patterns for users with a given political stance exposed to biased articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the real and LLM responses, along with political stance and bias.\n",
    "    - political_stance (str): The political stance to filter by (e.g., 'Extreme Left' or 'Extreme Right').\n",
    "    - bias_type (str): The type of article bias to filter by (e.g., 'left' or 'right').\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with statistics about reinforcement patterns.\n",
    "    \"\"\"\n",
    "    # Filter for the appropriate users and bias\n",
    "    filtered_df = df[(df['political_stance'] == political_stance) & (df['bias'] == bias_type)].copy()\n",
    "\n",
    "    # Determine reinforcement for each response\n",
    "    filtered_df.loc[:, 'reinforced'] = filtered_df.apply(is_reinforced, axis=1)\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_responses = len(filtered_df)\n",
    "    reinforced_responses = filtered_df['reinforced'].sum()\n",
    "    non_reinforced_responses = total_responses - reinforced_responses\n",
    "\n",
    "    return {\n",
    "        'total_responses': total_responses,\n",
    "        'reinforced_responses': reinforced_responses,\n",
    "        'percentage_reinforced': (reinforced_responses / total_responses * 100) if total_responses else 0,\n",
    "        'non_reinforced_responses': non_reinforced_responses\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses_for_left_users_and_left_bias(comparison_df, real_responses_with_stance):\n",
    "    \"\"\"\n",
    "    Compares the responses of left-wing real people with their LLM counterparts after exposure to left-wing articles.\n",
    "\n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - real_responses_with_stance (DataFrame): The real responses DataFrame with political stance.\n",
    "\n",
    "    Returns:\n",
    "    - avg_difference (float): The average weighted difference for left-wing users exposed to left-biased articles.\n",
    "    - filtered_comparison_df (DataFrame): The filtered DataFrame containing left-wing users and left-biased articles.\n",
    "    \"\"\"\n",
    "    # Ensure 'user_id' is a string in both DataFrames\n",
    "    comparison_df['user_id'] = comparison_df['user_id'].astype(str)\n",
    "    real_responses_with_stance['user_id'] = real_responses_with_stance['user_id'].astype(str)\n",
    "    \n",
    "    # Filter for left-wing users\n",
    "    left_wing_users = real_responses_with_stance[real_responses_with_stance['political_stance'] == 'Extreme Left']['user_id'].unique()\n",
    "\n",
    "    # Filter the comparison_df for left-wing users and left-biased articles\n",
    "    filtered_comparison_df = comparison_df[\n",
    "        (comparison_df['user_id'].isin(left_wing_users)) & \n",
    "        (comparison_df['bias'] == 'left')\n",
    "    ]\n",
    "\n",
    "    avg_difference = filtered_comparison_df['difference'].mean()\n",
    "\n",
    "    return avg_difference, filtered_comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. Comparison with Right-Biased Article on Subset of Left-Wing \n",
    "Measuring the effect of right wing biased article on the alignment between Left-Wing users and their LLM counterparts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses_for_left_users_and_right_bias(comparison_df, real_responses_with_stance):\n",
    "    \"\"\"\n",
    "    Compares the responses of left-wing real people with their LLM counterparts after exposure to right-wing articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - real_responses_with_stance (DataFrame): The real responses DataFrame with political stance.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_difference (float): The average weighted difference for left-wing users exposed to right-biased articles.\n",
    "    - filtered_comparison_df (DataFrame): The filtered DataFrame containing left-wing users and right-biased articles.\n",
    "    \"\"\"\n",
    "    real_responses_with_stance = real_responses_with_stance.set_index('user_id')\n",
    "    \n",
    "    left_wing_users = real_responses_with_stance[real_responses_with_stance['political_stance'] == 'Extreme Left'].index.unique()\n",
    "    \n",
    "    print(f\"Found {len(left_wing_users)} left-wing users:\")\n",
    "\n",
    "    comparison_df['user_id'] = comparison_df['user_id'].astype(str).str.strip()  \n",
    "    \n",
    "    filtered_comparison_df = comparison_df[\n",
    "        (comparison_df['user_id'].isin(left_wing_users)) &\n",
    "        (comparison_df['bias'] == 'right')\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(filtered_comparison_df)} entries with right bias for left-wing users.\")\n",
    "\n",
    "    avg_difference = filtered_comparison_df['difference'].mean()\n",
    "    \n",
    "    return avg_difference, filtered_comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['unique_id', 'F1A10_1', 'F2A7', 'F2A9', 'F3A3_1', 'F3A6_1', 'F3A7_1',\n",
      "       'F3A8_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(real_responses.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     unique_id  F1A10_1  F2A7  F2A9  F3A3_1  F3A6_1  F3A7_1  F3A8_1  \\\n",
      "3   IDUS104424        5     3     4       7       1       2       2   \n",
      "6   IDUS104915        4     3     2       4       7       6       5   \n",
      "7   IDUS105157        5     3     3       5       7       7       7   \n",
      "10  IDUS106103        4     5     3       5       2       3       3   \n",
      "11  IDUS107320        7     1     5       4       7       7       7   \n",
      "\n",
      "       user_id political_stance  \n",
      "3   IDUS104424     Extreme Left  \n",
      "6   IDUS104915     Extreme Left  \n",
      "7   IDUS105157     Extreme Left  \n",
      "10  IDUS106103     Extreme Left  \n",
      "11  IDUS107320     Extreme Left  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[322], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(real_responses_with_stance[real_responses_with_stance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolitical_stance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtreme Left\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Check if there are left-biased articles in the comparison_df\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcomparison_df\u001b[49m[comparison_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comparison_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Add political stance to real_responses\n",
    "real_responses_with_stance = add_political_stance_to_real_responses(real_responses, after_responses_df)\n",
    "\n",
    "# Check if left-wing users exist\n",
    "print(real_responses_with_stance[real_responses_with_stance['political_stance'] == 'Extreme Left'].head())\n",
    "\n",
    "# Check if there are left-biased articles in the comparison_df\n",
    "print(comparison_df[comparison_df['bias'] == 'left'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Comparison with Right-Biased Article on Subset of Right-Wing \n",
    "Measuring the effect of right wing biased article on the alignment between Right-Wing users and their LLM counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses_for_right_users_and_right_bias(comparison_df, real_responses_with_stance):\n",
    "    \"\"\"\n",
    "    Compares the responses of right-wing real people with their LLM counterparts after exposure to right-wing articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - real_responses_with_stance (DataFrame): The real responses DataFrame with political stance.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_difference (float): The average weighted difference for right-wing users exposed to right-biased articles.\n",
    "    - filtered_comparison_df (DataFrame): The filtered DataFrame containing right-wing users and right-biased articles.\n",
    "    \"\"\"\n",
    "    real_responses_with_stance = real_responses_with_stance.set_index('user_id')\n",
    "    \n",
    "    right_wing_users = real_responses_with_stance[real_responses_with_stance['political_stance'] == 'Extreme Right'].index.unique()\n",
    "    \n",
    "    print(f\"Found {len(right_wing_users)} right-wing users:\")\n",
    "\n",
    "    comparison_df['user_id'] = comparison_df['user_id'].astype(str).str.strip()  \n",
    "    \n",
    "    filtered_comparison_df = comparison_df[\n",
    "        (comparison_df['user_id'].isin(right_wing_users)) &\n",
    "        (comparison_df['bias'] == 'right')\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(filtered_comparison_df)} entries with right bias for right-wing users.\")\n",
    "\n",
    "    avg_difference = filtered_comparison_df['difference'].mean()\n",
    "    \n",
    "    return avg_difference, filtered_comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Comparison with Left-Biased Article on Subset of Right-Wing \n",
    "Measuring the effect of right wing biased article on the alignment between Right-Wing users and their LLM counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses_for_right_users_and_left_bias(comparison_df, real_responses_with_stance):\n",
    "    \"\"\"\n",
    "    Compares the responses of right-wing real people with their LLM counterparts after exposure to left-wing articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - real_responses_with_stance (DataFrame): The real responses DataFrame with political stance.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_difference (float): The average weighted difference for right-wing users exposed to left-biased articles.\n",
    "    - filtered_comparison_df (DataFrame): The filtered DataFrame containing right-wing users and left-biased articles.\n",
    "    \"\"\"\n",
    "    # Step 1: Ensure the correct index for real_responses_with_stance is set to 'user_id'\n",
    "    real_responses_with_stance = real_responses_with_stance.set_index('user_id')\n",
    "    \n",
    "    # Step 2: Filter for right-wing users from real_responses_with_stance\n",
    "    right_wing_users = real_responses_with_stance[real_responses_with_stance['political_stance'] == 'Extreme Right'].index.unique()\n",
    "    \n",
    "    print(f\"Found {len(right_wing_users)} right-wing users:\")\n",
    "\n",
    "    # Step 3: Clean and ensure user ID matching (if necessary)\n",
    "    comparison_df['user_id'] = comparison_df['user_id'].astype(str).str.strip()  # Ensure 'user_id' is a string and strip whitespace\n",
    "    \n",
    "    # Step 4: Filter the comparison_df for those users and left-biased articles\n",
    "    filtered_comparison_df = comparison_df[\n",
    "        (comparison_df['user_id'].isin(right_wing_users)) &\n",
    "        (comparison_df['bias'] == 'left')\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(filtered_comparison_df)} entries with left bias for right-wing users.\")\n",
    "\n",
    "    # Step 5: Calculate the average weighted difference for right-wing users exposed to left-biased articles\n",
    "    avg_difference = filtered_comparison_df['difference'].mean()\n",
    "    \n",
    "    return avg_difference, filtered_comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison_results(comparison_df, real_responses, after_responses_df):\n",
    "    \"\"\"\n",
    "    Compares and displays results for left-wing and right-wing users exposed to both left- and right-biased articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - real_responses (DataFrame): The real responses DataFrame that lacks political stance.\n",
    "    - after_responses_df (DataFrame): The after responses DataFrame that contains user_id and political_stance.\n",
    "    \"\"\"\n",
    "    # Step 1: Add political stance to real_responses\n",
    "    real_responses_with_stance = add_political_stance_to_real_responses(real_responses, after_responses_df)\n",
    "\n",
    "    # ---- Right-Wing Users ----\n",
    "    print(\"\\n--- Right-Wing Users Comparisons ---\\n\")\n",
    "\n",
    "    # Compare responses of right-wing users with right-biased articles\n",
    "    avg_difference_right, filtered_right_df = compare_responses_for_right_users_and_right_bias(comparison_df, real_responses_with_stance)\n",
    "    print(\"Comparison for Right-Wing Users Exposed to Right-Biased Articles:\")\n",
    "    print(f\"Average weighted difference for right-wing users and right-biased articles: {avg_difference_right}\")\n",
    "    print()\n",
    "\n",
    "    # Compare responses of right-wing users with left-biased articles\n",
    "    avg_difference_right_left, filtered_right_left_df = compare_responses_for_right_users_and_left_bias(comparison_df, real_responses_with_stance)\n",
    "    print(\"Comparison for Right-Wing Users Exposed to Left-Biased Articles:\")\n",
    "    print(f\"Average weighted difference for right-wing users and left-biased articles: {avg_difference_right_left}\")\n",
    "    print()\n",
    "\n",
    "    # ---- Left-Wing Users ----\n",
    "    print(\"\\n--- Left-Wing Users Comparisons ---\\n\")\n",
    "\n",
    "    # Compare responses of left-wing users with left-biased articles\n",
    "    avg_difference_left, filtered_left_df = compare_responses_for_left_users_and_left_bias(comparison_df, real_responses_with_stance)\n",
    "    print(\"Comparison for Left-Wing Users Exposed to Left-Biased Articles:\")\n",
    "    print(f\"Average weighted difference for left-wing users and left-biased articles: {avg_difference_left}\")\n",
    "    print()\n",
    "\n",
    "    # Compare responses of left-wing users with right-biased articles\n",
    "    avg_difference_left_right, filtered_left_right_df = compare_responses_for_left_users_and_right_bias(comparison_df, real_responses_with_stance)\n",
    "    print(\"Comparison for Left-Wing Users Exposed to Right-Biased Articles:\")\n",
    "    print(f\"Average weighted difference for left-wing users and right-biased articles: {avg_difference_left_right}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[326], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m display_comparison_results(\u001b[43mcomparison_df\u001b[49m, real_responses, after_responses_df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comparison_df' is not defined"
     ]
    }
   ],
   "source": [
    "display_comparison_results(comparison_df, real_responses, after_responses_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Wing Users Exposed to Left-Biased Articles Human Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radicalisation LLMs and Humans \n",
    "Instead of comparing LLMs to LLMs for radicalisation analysis, as done before, we now compare LLMs and humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_reinforced(row):\n",
    "    \"\"\"\n",
    "    Determines if the LLM response is a reinforcement of the user's original stance.\n",
    "    Reinforcement happens if the LLM response moves in the direction of the original political stance.  # this needs to be further clarified: what does it mean to move in the direction of the original political stance? \n",
    "    \n",
    "    For Extreme Left:\n",
    "      - Moving towards smaller numbers (more extreme left) is considered reinforcement.\n",
    "    For Extreme Right:\n",
    "      - Moving towards larger numbers (more extreme right) is considered reinforcement.\n",
    "    \n",
    "    Returns True if reinforcement is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    if row['political_stance'] == 'Extreme Left' and row['bias'] == 'left':\n",
    "        return row['llm_response_code'] < row['real_response_code']  # moving towards more extreme left\n",
    "    elif row['political_stance'] == 'Extreme Right' and row['bias'] == 'right':\n",
    "        return row['llm_response_code'] > row['real_response_code']  # moving towards more extreme right\n",
    "    return False  # No reinforcement otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_reinforcement_patterns(df, political_stance, bias_type):\n",
    "    \"\"\"\n",
    "    Track reinforcement patterns for users with a given political stance exposed to biased articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the real and LLM responses, along with political stance and bias.\n",
    "    - political_stance (str): The political stance to filter by (e.g., 'Extreme Left' or 'Extreme Right').\n",
    "    - bias_type (str): The type of article bias to filter by (e.g., 'left' or 'right').\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with statistics about reinforcement patterns.\n",
    "    \"\"\"\n",
    "    # Filter for the appropriate users and bias\n",
    "    filtered_df = df[(df['political_stance'] == political_stance) & (df['bias'] == bias_type)].copy()\n",
    "\n",
    "    # Determine reinforcement for each response\n",
    "    filtered_df.loc[:, 'reinforced'] = filtered_df.apply(is_reinforced, axis=1)\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_responses = len(filtered_df)\n",
    "    reinforced_responses = filtered_df['reinforced'].sum()\n",
    "    non_reinforced_responses = total_responses - reinforced_responses\n",
    "\n",
    "    return {\n",
    "        'total_responses': total_responses,\n",
    "        'reinforced_responses': reinforced_responses,\n",
    "        'percentage_reinforced': (reinforced_responses / total_responses * 100) if total_responses else 0,\n",
    "        'non_reinforced_responses': non_reinforced_responses\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement patterns for left-wing users exposed to left-biased articles:\n",
      "{'total_responses': 392, 'reinforced_responses': np.int64(109), 'percentage_reinforced': np.float64(27.806122448979593), 'non_reinforced_responses': np.int64(283)}\n",
      "Reinforcement patterns for right-wing users exposed to right-biased articles:\n",
      "{'total_responses': 544, 'reinforced_responses': np.int64(148), 'percentage_reinforced': np.float64(27.205882352941174), 'non_reinforced_responses': np.int64(396)}\n"
     ]
    }
   ],
   "source": [
    "# Ensure columns are correctly named in the DataFrame\n",
    "merged_responses_df.rename(columns={\n",
    "    'numeric_response_after': 'llm_response_code',\n",
    "    'numeric_response_before': 'real_response_code'\n",
    "}, inplace=True)\n",
    "\n",
    "# Example usage: Track reinforcement patterns for left-wing users exposed to left-biased articles\n",
    "reinforcement_stats_left = track_reinforcement_patterns(merged_responses_df, 'Extreme Left', 'left')\n",
    "reinforcement_stats_right = track_reinforcement_patterns(merged_responses_df, 'Extreme Right', 'right')\n",
    "\n",
    "# Output the reinforcement stats\n",
    "print(\"Reinforcement patterns for left-wing users exposed to left-biased articles:\")\n",
    "print(reinforcement_stats_left)\n",
    "\n",
    "print(\"Reinforcement patterns for right-wing users exposed to right-biased articles:\")\n",
    "print(reinforcement_stats_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'question_code', 'real_response_code', 'political_stance',\n",
      "       'llm_response_code', 'bias', 'response_change'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_responses_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_alignment_by_question(merged_df):\n",
    "    \"\"\"\n",
    "    Analyzes alignment between LLM and human responses question by question after exposure to biased articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - merged_df (DataFrame): The merged DataFrame containing real and LLM responses, political stance, and bias.\n",
    "    \n",
    "    Returns:\n",
    "    - alignment_summary (DataFrame): A summary DataFrame showing alignment for each question.\n",
    "    \"\"\"\n",
    "    # Create a summary list to store alignment data per question\n",
    "    alignment_data = []\n",
    "\n",
    "    # Get unique question codes\n",
    "    questions = merged_df['question_code'].unique()\n",
    "    \n",
    "    # Iterate over each question and analyze the alignment\n",
    "    for question in questions:\n",
    "        # Filter the DataFrame by question\n",
    "        question_df = merged_df[merged_df['question_code'] == question].copy()\n",
    "\n",
    "        # Calculate the absolute difference (alignment measure) between LLM and human responses\n",
    "        question_df['alignment'] = abs(question_df['llm_response_code'] - question_df['real_response_code'])\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_alignment = question_df['alignment'].mean()\n",
    "        mae = question_df['alignment'].mean()  # Mean Absolute Error\n",
    "        mse = (question_df['alignment'] ** 2).mean()  # Mean Squared Error\n",
    "        \n",
    "        # Track bias-specific alignment (e.g., left or right bias)\n",
    "        for bias in ['left', 'right']:\n",
    "            bias_df = question_df[question_df['bias'] == bias]\n",
    "            avg_alignment_bias = bias_df['alignment'].mean()\n",
    "            mae_bias = bias_df['alignment'].mean()\n",
    "            mse_bias = (bias_df['alignment'] ** 2).mean()\n",
    "\n",
    "            alignment_data.append({\n",
    "                'question_code': question,\n",
    "                'bias': bias,\n",
    "                'avg_alignment': avg_alignment_bias,\n",
    "                'mae': mae_bias,\n",
    "                'mse': mse_bias,\n",
    "                'total_responses': len(bias_df)\n",
    "            })\n",
    "\n",
    "    # Convert the alignment data to a DataFrame for analysis\n",
    "    alignment_summary = pd.DataFrame(alignment_data)\n",
    "\n",
    "    return alignment_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  question_code   bias  avg_alignment       mae        mse  total_responses\n",
      "0       F1A10_1   left       0.666667  0.666667   1.623932              117\n",
      "1       F1A10_1  right       3.136752  3.136752  16.094017              117\n",
      "2          F2A6   left       1.153846  1.153846   2.777778              117\n",
      "3          F2A6  right       0.923077  0.923077   1.965812              117\n",
      "4          F2A7   left       2.290598  2.290598   6.564103              117\n"
     ]
    }
   ],
   "source": [
    "alignment_summary = analyze_alignment_by_question(merged_responses_df)\n",
    "print(alignment_summary.head())  # View the first few rows of the summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stance</th>\n",
       "      <th>question_code</th>\n",
       "      <th>bias</th>\n",
       "      <th>avg_alignment</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>total_responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F1A10_1</td>\n",
       "      <td>left</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>1.061224</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F1A10_1</td>\n",
       "      <td>right</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.326531</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F2A6</td>\n",
       "      <td>left</td>\n",
       "      <td>-1.469388</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>4.693878</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F2A6</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.693878</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.653061</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F2A7</td>\n",
       "      <td>left</td>\n",
       "      <td>-1.061224</td>\n",
       "      <td>1.306122</td>\n",
       "      <td>2.612245</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F2A7</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F2A9</td>\n",
       "      <td>left</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F2A9</td>\n",
       "      <td>right</td>\n",
       "      <td>-1.306122</td>\n",
       "      <td>1.510204</td>\n",
       "      <td>3.877551</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F3A3_1</td>\n",
       "      <td>left</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F3A3_1</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.163265</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>1.632653</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F3A6_1</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.204082</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>1.224490</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F3A6_1</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.265306</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F3A7_1</td>\n",
       "      <td>left</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F3A7_1</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.367347</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>1.673469</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F3A8_1</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.040816</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>1.346939</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Extreme Left</td>\n",
       "      <td>F3A8_1</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.693878</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>2.530612</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F1A10_1</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>2.029412</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F1A10_1</td>\n",
       "      <td>right</td>\n",
       "      <td>-5.029412</td>\n",
       "      <td>5.088235</td>\n",
       "      <td>26.735294</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F2A6</td>\n",
       "      <td>left</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.397059</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F2A6</td>\n",
       "      <td>right</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>1.470588</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F2A7</td>\n",
       "      <td>left</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.411765</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F2A7</td>\n",
       "      <td>right</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F2A9</td>\n",
       "      <td>left</td>\n",
       "      <td>-1.014706</td>\n",
       "      <td>1.102941</td>\n",
       "      <td>2.132353</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F2A9</td>\n",
       "      <td>right</td>\n",
       "      <td>2.073529</td>\n",
       "      <td>2.073529</td>\n",
       "      <td>5.455882</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F3A3_1</td>\n",
       "      <td>left</td>\n",
       "      <td>-2.735294</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>13.088235</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F3A3_1</td>\n",
       "      <td>right</td>\n",
       "      <td>1.632353</td>\n",
       "      <td>2.102941</td>\n",
       "      <td>7.779412</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F3A6_1</td>\n",
       "      <td>left</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F3A6_1</td>\n",
       "      <td>right</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F3A7_1</td>\n",
       "      <td>left</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F3A7_1</td>\n",
       "      <td>right</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F3A8_1</td>\n",
       "      <td>left</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>3.191176</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>F3A8_1</td>\n",
       "      <td>right</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stance question_code   bias  avg_alignment       mae        mse  \\\n",
       "0    Extreme Left       F1A10_1   left       0.244898  0.367347   1.061224   \n",
       "1    Extreme Left       F1A10_1  right       0.102041  0.428571   1.326531   \n",
       "2    Extreme Left          F2A6   left      -1.469388  1.714286   4.693878   \n",
       "3    Extreme Left          F2A6  right      -0.693878  1.142857   2.653061   \n",
       "4    Extreme Left          F2A7   left      -1.061224  1.306122   2.612245   \n",
       "5    Extreme Left          F2A7  right      -0.142857  0.632653   0.959184   \n",
       "6    Extreme Left          F2A9   left       0.142857  0.265306   0.265306   \n",
       "7    Extreme Left          F2A9  right      -1.306122  1.510204   3.877551   \n",
       "8    Extreme Left        F3A3_1   left       0.061224  0.959184   1.857143   \n",
       "9    Extreme Left        F3A3_1  right      -0.163265  0.897959   1.632653   \n",
       "10   Extreme Left        F3A6_1   left      -0.204082  0.408163   1.224490   \n",
       "11   Extreme Left        F3A6_1  right      -0.265306  0.306122   0.918367   \n",
       "12   Extreme Left        F3A7_1   left       0.408163  0.530612   0.734694   \n",
       "13   Extreme Left        F3A7_1  right      -0.367347  0.897959   1.673469   \n",
       "14   Extreme Left        F3A8_1   left      -0.040816  0.693878   1.346939   \n",
       "15   Extreme Left        F3A8_1  right      -0.693878  0.897959   2.530612   \n",
       "16  Extreme Right       F1A10_1   left      -0.176471  0.882353   2.029412   \n",
       "17  Extreme Right       F1A10_1  right      -5.029412  5.088235  26.735294   \n",
       "18  Extreme Right          F2A6   left       0.338235  0.750000   1.397059   \n",
       "19  Extreme Right          F2A6  right       0.617647  0.764706   1.470588   \n",
       "20  Extreme Right          F2A7   left       3.000000  3.000000   9.411765   \n",
       "21  Extreme Right          F2A7  right       0.000000  0.647059   1.000000   \n",
       "22  Extreme Right          F2A9   left      -1.014706  1.102941   2.132353   \n",
       "23  Extreme Right          F2A9  right       2.073529  2.073529   5.455882   \n",
       "24  Extreme Right        F3A3_1   left      -2.735294  2.941176  13.088235   \n",
       "25  Extreme Right        F3A3_1  right       1.632353  2.102941   7.779412   \n",
       "26  Extreme Right        F3A6_1   left       0.014706  0.014706   0.014706   \n",
       "27  Extreme Right        F3A6_1  right       0.029412  0.029412   0.029412   \n",
       "28  Extreme Right        F3A7_1   left       0.000000  0.000000   0.000000   \n",
       "29  Extreme Right        F3A7_1  right       0.000000  0.000000   0.000000   \n",
       "30  Extreme Right        F3A8_1   left       0.691176  0.691176   3.191176   \n",
       "31  Extreme Right        F3A8_1  right       0.000000  0.000000   0.000000   \n",
       "\n",
       "    total_responses  \n",
       "0                49  \n",
       "1                49  \n",
       "2                49  \n",
       "3                49  \n",
       "4                49  \n",
       "5                49  \n",
       "6                49  \n",
       "7                49  \n",
       "8                49  \n",
       "9                49  \n",
       "10               49  \n",
       "11               49  \n",
       "12               49  \n",
       "13               49  \n",
       "14               49  \n",
       "15               49  \n",
       "16               68  \n",
       "17               68  \n",
       "18               68  \n",
       "19               68  \n",
       "20               68  \n",
       "21               68  \n",
       "22               68  \n",
       "23               68  \n",
       "24               68  \n",
       "25               68  \n",
       "26               68  \n",
       "27               68  \n",
       "28               68  \n",
       "29               68  \n",
       "30               68  \n",
       "31               68  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for Extreme Left and Extreme Right real users\n",
    "extreme_left_responses = merged_responses_df[merged_responses_df['political_stance'] == 'Extreme Left']\n",
    "extreme_right_responses = merged_responses_df[merged_responses_df['political_stance'] == 'Extreme Right']\n",
    "\n",
    "# Function to calculate alignment question by question for each group\n",
    "def calculate_alignment_by_stance(df, stance_name):\n",
    "    results = []\n",
    "    for question in df['question_code'].unique():\n",
    "        for bias in ['left', 'right']:\n",
    "            filtered_df = df[(df['question_code'] == question) & (df['bias'] == bias)]\n",
    "            if not filtered_df.empty:\n",
    "                avg_alignment = filtered_df['response_change'].mean()\n",
    "                mae = filtered_df['response_change'].abs().mean()\n",
    "                mse = (filtered_df['response_change'] ** 2).mean()\n",
    "                total_responses = len(filtered_df)\n",
    "                results.append({\n",
    "                    'stance': stance_name,\n",
    "                    'question_code': question,\n",
    "                    'bias': bias,\n",
    "                    'avg_alignment': avg_alignment,\n",
    "                    'mae': mae,\n",
    "                    'mse': mse,\n",
    "                    'total_responses': total_responses\n",
    "                })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Calculate alignment for extreme left and extreme right users\n",
    "extreme_left_alignment = calculate_alignment_by_stance(extreme_left_responses, 'Extreme Left')\n",
    "extreme_right_alignment = calculate_alignment_by_stance(extreme_right_responses, 'Extreme Right')\n",
    "\n",
    "# Combine the results\n",
    "combined_alignment_results = pd.concat([extreme_left_alignment, extreme_right_alignment], ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "combined_alignment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Responses Alignment Evaluation (as reference)\n",
    "```\n",
    "---- LLM Alignment Evaluation ----\n",
    "Exact Match Accuracy: 24.68%\n",
    "Interpretation: The LLM exactly matches real people's responses. Higher is desirable.\n",
    "'Close Enough' Accuracy (within 2 steps): 72.22%\n",
    "Interpretation: The LLM response is close (within the defined step range) to real people's responses.\n",
    "Mean Squared Error (MSE): 5.29\n",
    "Interpretation: A high MSE indicates some large mismatches between LLM and real people's responses.\n",
    "Mean Absolute Error (MAE): 1.73\n",
    "Interpretation: On average, LLM responses are about 1.73 steps away from real people's responses.\n",
    "Categorical Accuracy: 41.77%\n",
    "Interpretation: The LLM matches the general sentiment (positive, neutral, negative, or concern level) about this percentage of the time.\n",
    "-----------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "# Step 1: Separate the responses by bias (left and right)\n",
    "after_responses_left_bias = after_responses_df[after_responses_df.index.get_level_values('bias') == 'left']\n",
    "after_responses_right_bias = after_responses_df[after_responses_df.index.get_level_values('bias') == 'right']\n",
    "\n",
    "# Step 2: Align with before_responses_df based on 'user_id' and 'question_code'\n",
    "# Reset the index to prepare for merging\n",
    "before_responses_reset = before_responses_df.reset_index()\n",
    "\n",
    "# Merge left-biased responses with before responses\n",
    "left_bias_aligned = before_responses_reset.merge(\n",
    "    after_responses_left_bias.reset_index(),\n",
    "    on=['user_id', 'question_code'],\n",
    "    suffixes=('_before', '_after')\n",
    ")\n",
    "\n",
    "# Merge right-biased responses with before responses\n",
    "right_bias_aligned = before_responses_reset.merge(\n",
    "    after_responses_right_bias.reset_index(),\n",
    "    on=['user_id', 'question_code'],\n",
    "    suffixes=('_before', '_after')\n",
    ")\n",
    "\n",
    "# Step 1: Reset the index for real_responses_df to prepare for merging\n",
    "real_responses_reset = real_responses.reset_index()\n",
    "\n",
    "# Step 2: Align real responses with LLM before responses\n",
    "# Merge real responses with LLM's before exposure responses\n",
    "before_responses_aligned = real_responses_reset.merge(\n",
    "    before_responses_df.reset_index(),\n",
    "    left_on=['unique_id'], \n",
    "    right_on=['user_id'],\n",
    "    suffixes=('_real', '_llm_before')\n",
    ")\n",
    "\n",
    "# Step 3: Align real responses with LLM after responses (left bias)\n",
    "left_bias_responses_aligned = real_responses_reset.merge(\n",
    "    after_responses_left_bias.reset_index(),\n",
    "    left_on=['unique_id'], \n",
    "    right_on=['user_id'],\n",
    "    suffixes=('_real', '_llm_after')\n",
    ")\n",
    "\n",
    "# Step 4: Align real responses with LLM after responses (right bias)\n",
    "right_bias_responses_aligned = real_responses_reset.merge(\n",
    "    after_responses_right_bias.reset_index(),\n",
    "    left_on=['unique_id'], \n",
    "    right_on=['user_id'],\n",
    "    suffixes=('_real', '_llm_after')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter left-biased articles shown to extreme-left users\n",
    "left_bias_extreme_left = merged_responses_df[\n",
    "    (merged_responses_df['bias'] == 'left') & \n",
    "    (merged_responses_df['political_stance'] == 'Extreme Left')\n",
    "]\n",
    "\n",
    "# Filter left-biased articles shown to extreme-right users\n",
    "left_bias_extreme_right = merged_responses_df[\n",
    "    (merged_responses_df['bias'] == 'left') & \n",
    "    (merged_responses_df['political_stance'] == 'Extreme Right')\n",
    "]\n",
    "\n",
    "# Filter right-biased articles shown to extreme-left users\n",
    "right_bias_extreme_left = merged_responses_df[\n",
    "    (merged_responses_df['bias'] == 'right') & \n",
    "    (merged_responses_df['political_stance'] == 'Extreme Left')\n",
    "]\n",
    "\n",
    "# Filter right-biased articles shown to extreme-right users\n",
    "right_bias_extreme_right = merged_responses_df[\n",
    "    (merged_responses_df['bias'] == 'right') & \n",
    "    (merged_responses_df['political_stance'] == 'Extreme Right')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Bias, Extreme Left Users:\n",
      "       user_id question_code  real_response_code political_stance  \\\n",
      "48  IDUS104424       F1A10_1                   7     Extreme Left   \n",
      "50  IDUS104424          F2A6                   5     Extreme Left   \n",
      "52  IDUS104424          F2A7                   3     Extreme Left   \n",
      "54  IDUS104424          F2A9                   5     Extreme Left   \n",
      "56  IDUS104424        F3A3_1                   5     Extreme Left   \n",
      "\n",
      "    llm_response_code  bias  response_change  \n",
      "48                  7  left                0  \n",
      "50                  2  left               -3  \n",
      "52                  1  left               -2  \n",
      "54                  5  left                0  \n",
      "56                  7  left                2  \n",
      "Left Bias, Extreme Right Users:\n",
      "      user_id question_code  real_response_code political_stance  \\\n",
      "0  IDUS103408       F1A10_1                   7    Extreme Right   \n",
      "2  IDUS103408          F2A6                   4    Extreme Right   \n",
      "4  IDUS103408          F2A7                   1    Extreme Right   \n",
      "6  IDUS103408          F2A9                   2    Extreme Right   \n",
      "8  IDUS103408        F3A3_1                   1    Extreme Right   \n",
      "\n",
      "   llm_response_code  bias  response_change  \n",
      "0                  5  left               -2  \n",
      "2                  4  left                0  \n",
      "4                  5  left                4  \n",
      "6                  2  left                0  \n",
      "8                  1  left                0  \n",
      "Right Bias, Extreme Left Users:\n",
      "       user_id question_code  real_response_code political_stance  \\\n",
      "49  IDUS104424       F1A10_1                   7     Extreme Left   \n",
      "51  IDUS104424          F2A6                   5     Extreme Left   \n",
      "53  IDUS104424          F2A7                   3     Extreme Left   \n",
      "55  IDUS104424          F2A9                   5     Extreme Left   \n",
      "57  IDUS104424        F3A3_1                   5     Extreme Left   \n",
      "\n",
      "    llm_response_code   bias  response_change  \n",
      "49                  7  right                0  \n",
      "51                  5  right                0  \n",
      "53                  2  right               -1  \n",
      "55                  5  right                0  \n",
      "57                  6  right                1  \n",
      "Right Bias, Extreme Right Users:\n",
      "      user_id question_code  real_response_code political_stance  \\\n",
      "1  IDUS103408       F1A10_1                   7    Extreme Right   \n",
      "3  IDUS103408          F2A6                   4    Extreme Right   \n",
      "5  IDUS103408          F2A7                   1    Extreme Right   \n",
      "7  IDUS103408          F2A9                   2    Extreme Right   \n",
      "9  IDUS103408        F3A3_1                   1    Extreme Right   \n",
      "\n",
      "   llm_response_code   bias  response_change  \n",
      "1                  1  right               -6  \n",
      "3                  5  right                1  \n",
      "5                  2  right                1  \n",
      "7                  5  right                3  \n",
      "9                  5  right                4  \n"
     ]
    }
   ],
   "source": [
    "# Check if the filtered DataFrames contain data\n",
    "print(\"Left Bias, Extreme Left Users:\")\n",
    "print(left_bias_extreme_left.head())\n",
    "\n",
    "print(\"Left Bias, Extreme Right Users:\")\n",
    "print(left_bias_extreme_right.head())\n",
    "\n",
    "print(\"Right Bias, Extreme Left Users:\")\n",
    "print(right_bias_extreme_left.head())\n",
    "\n",
    "print(\"Right Bias, Extreme Right Users:\")\n",
    "print(right_bias_extreme_right.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            F1A10_1  F2A7  F2A9  F3A3_1  F3A6_1  F3A7_1  F3A8_1\n",
      "unique_id                                                      \n",
      "IDUS103408        2     3     2       6       4       4       7\n",
      "IDUS103554        7     2     5       7       1       1       1\n",
      "IDUS103826        4     1     4       7       1       3       4\n",
      "IDUS104424        5     3     4       7       1       2       2\n",
      "IDUS104578        6     2     4       6       1       3       2\n"
     ]
    }
   ],
   "source": [
    "# Set unique_id as the index for real_responses\n",
    "real_responses.set_index('unique_id', inplace=True)\n",
    "\n",
    "# Drop the redundant user_id column if necessary\n",
    "real_responses.drop(columns=['user_id'], inplace=True, errors='ignore')\n",
    "\n",
    "# Check the updated structure\n",
    "print(real_responses.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae_mse_per_stance(df, real_responses_df, stance):\n",
    "    mae_values = []\n",
    "    mse_values = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Get the real response from real_responses_df using the user_id and question_code\n",
    "        user_id = row['user_id']\n",
    "        question_code = row['question_code']\n",
    "        \n",
    "        # Check if real_response can be found\n",
    "        if user_id in real_responses_df.index and question_code in real_responses_df.columns:\n",
    "            real_response = real_responses_df.loc[user_id, question_code]\n",
    "            llm_response = row['llm_response_code']\n",
    "            \n",
    "            # Only compute if both real and LLM responses are present\n",
    "            if pd.notna(real_response) and pd.notna(llm_response):\n",
    "                mae_values.append(abs(real_response - llm_response))\n",
    "                mse_values.append((real_response - llm_response) ** 2)\n",
    "            else:\n",
    "                print(f\"Skipping calculation for user {user_id}, question {question_code} due to missing response.\")\n",
    "        else:\n",
    "            continue\n",
    "            print(f\"Real response for user {user_id}, question {question_code} not found.\")\n",
    "    \n",
    "    # Calculate MAE and MSE\n",
    "    if mae_values and mse_values:\n",
    "        mae = sum(mae_values) / len(mae_values)\n",
    "        mse = sum(mse_values) / len(mse_values)\n",
    "    else:\n",
    "        mae, mse = None, None\n",
    "    \n",
    "    print(f\"MAE for {stance} users: {mae}\")\n",
    "    print(f\"MSE for {stance} users: {mse}\")\n",
    "    return mae, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['F1A10_1', 'F2A7', 'F2A9', 'F3A3_1', 'F3A6_1', 'F3A7_1', 'F3A8_1'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(real_responses.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left-Biased Articles\n",
      "MAE for Extreme Left users: 1.9912536443148687\n",
      "MSE for Extreme Left users: 6.364431486880466\n"
     ]
    }
   ],
   "source": [
    "print(\"Left-Biased Articles\")\n",
    "mae_left_extreme_left, mse_left_extreme_left = calculate_mae_mse_per_stance(left_bias_extreme_left, real_responses, \"Extreme Left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Significance Test\n",
    "\n",
    "Check whether the articles cause a significant shift in opinion: if so, which type of bias is more impactful? And which groups are most affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge before and after responses for extreme right users exposed to left-biased articles\n",
    "left_bias_extreme_right_aligned = before_responses_df.merge(\n",
    "    after_responses_left_bias, \n",
    "    on=['user_id', 'question_code'], \n",
    "    suffixes=('_before', '_after')\n",
    ")\n",
    "\n",
    "# Now filter for Extreme Right users\n",
    "left_bias_extreme_right = left_bias_extreme_right_aligned[\n",
    "    left_bias_extreme_right_aligned['political_stance_before'] == 'Extreme Right'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_bias_extreme_right_aligned = before_responses_df.merge(\n",
    "    after_responses_right_bias, \n",
    "    on=['user_id', 'question_code'], \n",
    "    suffixes=('_before', '_after')\n",
    ")\n",
    "\n",
    "right_bias_extreme_right = right_bias_extreme_right_aligned[\n",
    "    right_bias_extreme_right_aligned['political_stance_before'] == 'Extreme Right'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def perform_non_extreme_wilcoxon_test(df, before_column, after_column, group_name, default_min=1, default_max=7):\n",
    "    \"\"\"\n",
    "    Perform the Wilcoxon Signed-Rank Test excluding users with extreme values that did not change.\n",
    "    If `question_code` exists, determine the extreme values based on the question type (e.g., F2 questions use 1-5 scale).\n",
    "    If `question_code` does not exist, default to the given min and max values (default 1 to 7).\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the data to test.\n",
    "    - before_column (str): The column representing the 'before' values.\n",
    "    - after_column (str): The column representing the 'after' values.\n",
    "    - group_name (str): Name of the group being tested (for logging purposes).\n",
    "    - default_min (int): Default minimum value for the response scale (default is 1).\n",
    "    - default_max (int): Default maximum value for the response scale (default is 7).\n",
    "    \n",
    "    Returns:\n",
    "    - p_value (float): The p-value from the Wilcoxon test.\n",
    "    \"\"\"\n",
    "    # Remove any rows with NaN values in the relevant columns\n",
    "    df_clean = df[[before_column, after_column]].dropna()\n",
    "\n",
    "    # If 'question_code' exists in the DataFrame, use it to determine the scale\n",
    "    if 'question_code' in df.columns:\n",
    "        def get_extreme_values(question_code):\n",
    "            \"\"\"Determine the extreme values for the question based on its code.\"\"\"\n",
    "            if question_code.startswith('F2'):\n",
    "                return 1, 5  # Scale is 1 to 5 for F2 questions\n",
    "            else:\n",
    "                return 1, 7  # Scale is 1 to 7 for all other questions\n",
    "\n",
    "        # Add the 'question_code' to df_clean for filtering\n",
    "        df_clean = df_clean.join(df[['question_code']])\n",
    "\n",
    "        # Filter out users with extreme values (no change) based on their question type\n",
    "        non_extreme_indices = []\n",
    "        for index, row in df_clean.iterrows():\n",
    "            question_code = row['question_code']\n",
    "            min_value, max_value = get_extreme_values(question_code)\n",
    "            \n",
    "            # Check if the response is non-extreme or if it changed\n",
    "            if not (\n",
    "                (row[before_column] == min_value and row[after_column] == min_value) or \n",
    "                (row[before_column] == max_value and row[after_column] == max_value)\n",
    "            ):\n",
    "                non_extreme_indices.append(index)\n",
    "\n",
    "    else:\n",
    "        # If 'question_code' does not exist, use default min and max values\n",
    "        min_value, max_value = default_min, default_max\n",
    "        \n",
    "        # Filter out users with extreme values (no change) based on the default scale\n",
    "        non_extreme_indices = df_clean[~(\n",
    "            ((df_clean[before_column] == min_value) & (df_clean[after_column] == min_value)) | \n",
    "            ((df_clean[before_column] == max_value) & (df_clean[after_column] == max_value))\n",
    "        )].index\n",
    "\n",
    "    # Create a DataFrame with non-extreme responses\n",
    "    df_non_extreme = df_clean.loc[non_extreme_indices]\n",
    "\n",
    "    # Perform the Wilcoxon Signed-Rank test\n",
    "    if len(df_non_extreme) > 0:  # Make sure we have data to test\n",
    "        test_statistic, p_value = wilcoxon(df_non_extreme[before_column], df_non_extreme[after_column])\n",
    "        print(f\"Wilcoxon test for {group_name}:\")\n",
    "        print(f\"Test statistic: {test_statistic}, p-value: {p_value}\")\n",
    "        return p_value\n",
    "    else:\n",
    "        print(f\"No non-extreme data available for {group_name}.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon test for Extreme Right - Left-Biased Articles (Non-Extreme):\n",
      "Test statistic: 17275.5, p-value: 0.5517817005545443\n",
      "p-value for Extreme Right users (Left-Biased Articles, Non-Extreme): 0.5517817005545443\n"
     ]
    }
   ],
   "source": [
    "# Perform Wilcoxon test for Extreme Right users exposed to left-biased articles (non-extreme responses)\n",
    "p_value_extreme_right_left_bias_non_extreme = perform_non_extreme_wilcoxon_test(\n",
    "    left_bias_extreme_right,  # Assuming this DataFrame holds the left-bias responses for Extreme Right users\n",
    "    'numeric_response_before', \n",
    "    'numeric_response_after', \n",
    "    'Extreme Right - Left-Biased Articles (Non-Extreme)'\n",
    ")\n",
    "\n",
    "print(f\"p-value for Extreme Right users (Left-Biased Articles, Non-Extreme): {p_value_extreme_right_left_bias_non_extreme}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['numeric_response_before', 'numeric_response_after'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[346], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perform Wilcoxon test for Extreme Left users exposed to left-biased articles (non-extreme responses)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m p_value_extreme_left_left_bias_non_extreme \u001b[38;5;241m=\u001b[39m \u001b[43mperform_non_extreme_wilcoxon_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleft_bias_extreme_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# DataFrame with left-biased articles for Extreme Left users\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumeric_response_before\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Correct column for the \"before\" responses\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumeric_response_after\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# After responses (predicted by LLM)\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExtreme Left - Left-Biased Articles (Non-Extreme)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp-value for Extreme Left users (Left-Biased Articles, Non-Extreme): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_value_extreme_left_left_bias_non_extreme\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[344], line 21\u001b[0m, in \u001b[0;36mperform_non_extreme_wilcoxon_test\u001b[1;34m(df, before_column, after_column, group_name, default_min, default_max)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mPerform the Wilcoxon Signed-Rank Test excluding users with extreme values that did not change.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mIf `question_code` exists, determine the extreme values based on the question type (e.g., F2 questions use 1-5 scale).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m- p_value (float): The p-value from the Wilcoxon test.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Remove any rows with NaN values in the relevant columns\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbefore_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# If 'question_code' exists in the DataFrame, use it to determine the scale\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_code\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['numeric_response_before', 'numeric_response_after'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Perform Wilcoxon test for Extreme Left users exposed to left-biased articles (non-extreme responses)\n",
    "p_value_extreme_left_left_bias_non_extreme = perform_non_extreme_wilcoxon_test(\n",
    "    left_bias_extreme_left,  # DataFrame with left-biased articles for Extreme Left users\n",
    "    'numeric_response_before',  # Correct column for the \"before\" responses\n",
    "    'numeric_response_after',     # After responses (predicted by LLM)\n",
    "    'Extreme Left - Left-Biased Articles (Non-Extreme)'\n",
    ")\n",
    "\n",
    "print(f\"p-value for Extreme Left users (Left-Biased Articles, Non-Extreme): {p_value_extreme_left_left_bias_non_extreme}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon test for Extreme Right - Right-Biased Articles (Non-Extreme):\n",
      "Test statistic: 16244.0, p-value: 0.8621700120632942\n",
      "p-value for Extreme Right users (Right-Biased Articles, Non-Extreme): 0.8621700120632942\n"
     ]
    }
   ],
   "source": [
    "p_value_extreme_right_right_bias_non_extreme = perform_non_extreme_wilcoxon_test(\n",
    "    right_bias_extreme_right,  # DataFrame with right-bias responses for Extreme Right users\n",
    "    'numeric_response_before',  # Correct column for the \"before\" responses\n",
    "    'numeric_response_after',   # Correct column for the \"after\" responses\n",
    "    'Extreme Right - Right-Biased Articles (Non-Extreme)'\n",
    ")\n",
    "\n",
    "# Print the p-value result\n",
    "print(f\"p-value for Extreme Right users (Right-Biased Articles, Non-Extreme): {p_value_extreme_right_right_bias_non_extreme}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'question_code', 'real_response_code', 'political_stance',\n",
       "       'llm_response_code', 'bias', 'response_change'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_bias_extreme_left.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['numeric_response_before', 'numeric_response_after'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[349], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perform Wilcoxon test for Extreme Left users exposed to right-biased articles (non-extreme responses)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m p_value_extreme_left_right_bias_non_extreme \u001b[38;5;241m=\u001b[39m \u001b[43mperform_non_extreme_wilcoxon_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mright_bias_extreme_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# DataFrame with right-bias responses for Extreme Left users\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumeric_response_before\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Before responses (actual user responses before article exposure)\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumeric_response_after\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# After responses (predicted by LLM or after exposure to the article)\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExtreme Left - Right-Biased Articles (Non-Extreme)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp-value for Extreme Left users (Right-Biased Articles, Non-Extreme): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_value_extreme_left_right_bias_non_extreme\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[344], line 21\u001b[0m, in \u001b[0;36mperform_non_extreme_wilcoxon_test\u001b[1;34m(df, before_column, after_column, group_name, default_min, default_max)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mPerform the Wilcoxon Signed-Rank Test excluding users with extreme values that did not change.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mIf `question_code` exists, determine the extreme values based on the question type (e.g., F2 questions use 1-5 scale).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m- p_value (float): The p-value from the Wilcoxon test.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Remove any rows with NaN values in the relevant columns\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbefore_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# If 'question_code' exists in the DataFrame, use it to determine the scale\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_code\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['numeric_response_before', 'numeric_response_after'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Perform Wilcoxon test for Extreme Left users exposed to right-biased articles (non-extreme responses)\n",
    "p_value_extreme_left_right_bias_non_extreme = perform_non_extreme_wilcoxon_test(\n",
    "    right_bias_extreme_left,  # DataFrame with right-bias responses for Extreme Left users\n",
    "    'numeric_response_before',     # Before responses (actual user responses before article exposure)\n",
    "    'numeric_response_after',      # After responses (predicted by LLM or after exposure to the article)\n",
    "    'Extreme Left - Right-Biased Articles (Non-Extreme)'\n",
    ")\n",
    "\n",
    "print(f\"p-value for Extreme Left users (Right-Biased Articles, Non-Extreme): {p_value_extreme_left_right_bias_non_extreme}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Results of the Significance Test: Analyzing Opinion Shifts\n",
    "\n",
    "The Wilcoxon Signed-Rank test was performed to determine whether biased articles cause significant shifts in opinions among extreme political groups, focusing on non-extreme responses.\n",
    "\n",
    "#### Key Results:\n",
    "1. **Extreme Right users exposed to left-biased articles**:\n",
    "   - **p-value: 0.5518** → No significant shift in opinions.\n",
    "   \n",
    "2. **Extreme Left users exposed to left-biased articles**:\n",
    "   - **p-value: 0.0001** → Highly significant shift in opinions.\n",
    "   \n",
    "3. **Extreme Right users exposed to right-biased articles**:\n",
    "   - **p-value: 0.8622** → No significant shift in opinions.\n",
    "\n",
    "4. **Extreme Left users exposed to right-biased articles**:\n",
    "   - **p-value: 7.68e-11** → Extremely significant shift in opinions.\n",
    "\n",
    "#### Insights:\n",
    "- **Extreme Left users** are highly susceptible to opinion shifts when exposed to both left- and right-biased articles. **Left-biased content** reinforces their views, while **right-biased content** causes significant shifts, likely in opposition.\n",
    "- **Extreme Right users** show **no significant opinion shifts** when exposed to either left- or right-biased content, indicating that their opinions remain more stable.\n",
    "\n",
    "This suggests that the **type of bias** has a **differential impact** based on political orientation, with **Extreme Left users** being more affected overall, while **Extreme Right users** demonstrate more resistance to opinion shifts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Direction of Change in Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_direction_of_change(df, group_name):\n",
    "    \"\"\"\n",
    "    Analyzes the direction of response changes based on different question scales.\n",
    "    Automatically detects the scale based on the question code:\n",
    "    - 1-7 scale (e.g., F1 questions where 1 = completely agree and 7 = completely disagree)\n",
    "    - 1-5 scale (e.g., F2 questions where 1 = not concerned at all and 5 = very concerned)\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing before and after responses.\n",
    "    - group_name (str): Name of the group for logging purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def interpret_change(row):\n",
    "        \"\"\"\n",
    "        Determines the direction of the response change based on the question type.\n",
    "        - For F1 questions (1-7 scale): A lower number indicates more agreement, so a decrease is positive.\n",
    "        - For F2 questions (1-5 scale): Higher numbers indicate more concern, so an increase is positive.\n",
    "        \"\"\"\n",
    "        # Check if the question code starts with \"F2\" (which uses a 1-5 scale)\n",
    "        if row['question_code'].startswith('F2'):\n",
    "            # 1-5 scale (concern-based): Increase is positive (more concern), decrease is negative\n",
    "            if row['numeric_response_after'] > row['numeric_response_before']:\n",
    "                return \"positive\"\n",
    "            elif row['numeric_response_after'] < row['numeric_response_before']:\n",
    "                return \"negative\"\n",
    "            else:\n",
    "                return \"no change\"\n",
    "        else:\n",
    "            # Default to 1-7 scale: Decrease is positive (more agreement), increase is negative\n",
    "            if row['numeric_response_after'] < row['numeric_response_before']:\n",
    "                return \"positive\"\n",
    "            elif row['numeric_response_after'] > row['numeric_response_before']:\n",
    "                return \"negative\"\n",
    "            else:\n",
    "                return \"no change\"\n",
    "\n",
    "    # Apply the interpretation function to each row using .loc to avoid SettingWithCopyWarning\n",
    "    df.loc[:, 'change_direction'] = df.apply(interpret_change, axis=1)\n",
    "    \n",
    "    # Calculate the counts of positive, negative, and no changes\n",
    "    positive_changes = (df['change_direction'] == \"positive\").sum()\n",
    "    negative_changes = (df['change_direction'] == \"negative\").sum()\n",
    "    no_changes = (df['change_direction'] == \"no change\").sum()\n",
    "    \n",
    "    # Calculate the mean response change (just for general reference) using .loc\n",
    "    df.loc[:, 'response_change'] = df['numeric_response_after'] - df['numeric_response_before']\n",
    "    mean_change = df['response_change'].mean()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Analysis of response changes for {group_name}:\")\n",
    "    print(f\"Mean change: {mean_change}\")\n",
    "    print(f\"Number of positive changes: {positive_changes}\")\n",
    "    print(f\"Number of negative changes: {negative_changes}\")\n",
    "    print(f\"Number of no changes: {no_changes}\")\n",
    "    \n",
    "    return mean_change, positive_changes, negative_changes, no_changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of response changes for Extreme Left - Right-Biased Articles:\n",
      "Mean change: -0.4413265306122449\n",
      "Number of positive changes: 91\n",
      "Number of negative changes: 107\n",
      "Number of no changes: 194\n",
      "Analysis of response changes for Extreme Left - Left-Biased Articles:\n",
      "Mean change: -0.23979591836734693\n",
      "Number of positive changes: 62\n",
      "Number of negative changes: 129\n",
      "Number of no changes: 201\n",
      "Analysis of response changes for Extreme Right - Right-Biased Articles:\n",
      "Mean change: -0.08455882352941177\n",
      "Number of positive changes: 189\n",
      "Number of negative changes: 67\n",
      "Number of no changes: 288\n",
      "Analysis of response changes for Extreme Right - Left-Biased Articles:\n",
      "Mean change: 0.014705882352941176\n",
      "Number of positive changes: 171\n",
      "Number of negative changes: 97\n",
      "Number of no changes: 276\n"
     ]
    }
   ],
   "source": [
    "# Analyze the direction of change for Extreme Left users exposed to right-biased articles\n",
    "mean_change_extreme_left_right_bias, pos_changes_extreme_left_right_bias, neg_changes_extreme_left_right_bias, no_changes_extreme_left_right_bias = analyze_direction_of_change(\n",
    "    right_bias_extreme_left,  # DataFrame for right-biased articles and Extreme Left users\n",
    "    \"Extreme Left - Right-Biased Articles\"\n",
    ")\n",
    "\n",
    "# Analyze the direction of change for Extreme Left users exposed to left-biased articles\n",
    "mean_change_extreme_left_left_bias, pos_changes_extreme_left_left_bias, neg_changes_extreme_left_left_bias, no_changes_extreme_left_left_bias = analyze_direction_of_change(\n",
    "    left_bias_extreme_left,  # DataFrame for left-biased articles and Extreme Left users\n",
    "    \"Extreme Left - Left-Biased Articles\"\n",
    ")\n",
    "\n",
    "# Analyze the direction of change for Extreme Right users exposed to right-biased articles\n",
    "mean_change_extreme_right_right_bias, pos_changes_extreme_right_right_bias, neg_changes_extreme_right_right_bias, no_changes_extreme_right_right_bias = analyze_direction_of_change(\n",
    "    right_bias_extreme_right,  # DataFrame for right-biased articles and Extreme Right users\n",
    "    \"Extreme Right - Right-Biased Articles\"\n",
    ")\n",
    "\n",
    "# Analyze the direction of change for Extreme Right users exposed to left-biased articles\n",
    "mean_change_extreme_right_left_bias, pos_changes_extreme_right_left_bias, neg_changes_extreme_right_left_bias, no_changes_extreme_right_left_bias = analyze_direction_of_change(\n",
    "    left_bias_extreme_right,  # DataFrame for left-biased articles and Extreme Right users\n",
    "    \"Extreme Right - Left-Biased Articles\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction of Change Analysis: Extreme Left and Extreme Right\n",
    "\n",
    "From the Wilcoxon test, we found that agents with an **Extreme Left stance** tended to show significant shifts in opinion. We now analyze the direction of response changes for both **Extreme Left** and **Extreme Right users** exposed to **right- and left-biased articles**, accounting for the different question scales.\n",
    "\n",
    "#### Key Results for Extreme Left Users:\n",
    "\n",
    "1. **Extreme Left users exposed to right-biased articles**:\n",
    "   - **Mean change**: -0.44\n",
    "   - **Positive changes**: 91\n",
    "   - **Negative changes**: 107\n",
    "   - **No changes**: 194\n",
    "   \n",
    "   **Interpretation**: \n",
    "   - The overall **negative shift** indicates that most Extreme Left users reacted negatively to right-biased content, likely **moving further away** from alignment with right-leaning viewpoints. However, a notable number of positive changes (91) show that some users became more aligned with right-wing content.\n",
    "\n",
    "2. **Extreme Left users exposed to left-biased articles**:\n",
    "   - **Mean change**: -0.24\n",
    "   - **Positive changes**: 62\n",
    "   - **Negative changes**: 129\n",
    "   - **No changes**: 201\n",
    "   \n",
    "   **Interpretation**: \n",
    "   - While left-biased content should reinforce the views of Extreme Left users, the overall **negative shift** suggests that a significant portion of users moved away from the content’s viewpoint. This may indicate **polarization** or **fatigue** with the content. However, the positive changes show that left-biased content reinforced views for some users.\n",
    "\n",
    "#### Key Results for Extreme Right Users:\n",
    "\n",
    "1. **Extreme Right users exposed to right-biased articles**:\n",
    "   - **Mean change**: -0.08\n",
    "   - **Positive changes**: 189\n",
    "   - **Negative changes**: 67\n",
    "   - **No changes**: 288\n",
    "   \n",
    "   **Interpretation**: \n",
    "   - The overall **small negative shift** suggests that most Extreme Right users did not significantly change their views when exposed to right-biased content, but some became more aligned with right-wing viewpoints (189 positive changes). However, 288 users showed **no change**, indicating a strong **stability** in their opinions when exposed to content that aligns with their views.\n",
    "\n",
    "2. **Extreme Right users exposed to left-biased articles**:\n",
    "   - **Mean change**: 0.01\n",
    "   - **Positive changes**: 171\n",
    "   - **Negative changes**: 97\n",
    "   - **No changes**: 276\n",
    "   \n",
    "   **Interpretation**: \n",
    "   - The **slight positive shift** indicates that some Extreme Right users became more aligned with left-leaning content. However, most showed **no significant change** (276 users), and there were still more positive changes (171) than negative, suggesting that **left-biased articles had limited impact** on shifting their views.\n",
    "\n",
    "#### Conclusion:\n",
    "- **Right-biased articles** generally caused a **polarizing shift** for **Extreme Left users**, pushing them away from right-wing viewpoints, while for **Extreme Right users**, they mainly reinforced existing views.\n",
    "- **Left-biased articles** had a mixed impact on both groups. For **Extreme Left users**, it produced **polarization** or **fatigue**, while for **Extreme Right users**, it resulted in limited but slightly positive shifts in alignment with left-leaning content, though most opinions remained stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Reliability Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the shift analysis above while also including the reliability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_analyze_with_reliability(df, reliability_df, reliability_threshold=0.44):\n",
    "    \"\"\"\n",
    "    Filters and analyzes direction of opinion changes for a given DataFrame of user responses,\n",
    "    weighted by reliability scores. The function merges user reliability data, filters out\n",
    "    unreliable users based on an average reliability threshold, and analyzes direction of \n",
    "    response changes for reliable users only.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing user responses for a particular group. \n",
    "                      Should include 'user_id', 'question_code', 'numeric_response_before', \n",
    "                      'numeric_response_after', 'response_change', and 'bias'.\n",
    "    - reliability_df (DataFrame): The DataFrame containing user reliability scores. Should \n",
    "                      include 'user_id' and 'reliability_score'.\n",
    "    - reliability_threshold (float, optional): The minimum average reliability score for a \n",
    "                      user to be considered 'reliable'. Default is 0.44.\n",
    "\n",
    "    Returns:\n",
    "    - result (tuple): A tuple containing:\n",
    "        - mean_change (float): The weighted mean of response changes for reliable users.\n",
    "        - positive_changes (float): The weighted count of positive response changes.\n",
    "        - negative_changes (float): The weighted count of negative response changes.\n",
    "        - no_changes (float): The weighted count of responses where no change occurred.\n",
    "    \n",
    "    Workflow:\n",
    "    1. Merge the user reliability data into the user responses DataFrame.\n",
    "    2. Calculate each user's average reliability score.\n",
    "    3. Filter the DataFrame to retain only users whose average reliability score is above \n",
    "       the specified threshold.\n",
    "    4. Perform a direction of change analysis, weighting response changes by the reliability score.\n",
    "    5. Return a summary of the analysis including mean change, positive, negative, and no change counts.\n",
    "    \n",
    "    Example Usage:\n",
    "    mean_change, pos_changes, neg_changes, no_changes = filter_and_analyze_with_reliability(\n",
    "        right_bias_extreme_left, \n",
    "        reliability_df, \n",
    "        reliability_threshold=0.44\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Step 1: Merge the user reliability data into the user responses DataFrame\n",
    "    df = df.merge(reliability_df[['user_id', 'reliability_score']], on='user_id', how='left')\n",
    "\n",
    "    # Step 2: Calculate each user's average reliability score\n",
    "    user_avg_reliability = df.groupby('user_id')['reliability_score'].mean()\n",
    "\n",
    "    # Step 3: Filter for users with an average reliability score greater than the threshold\n",
    "    reliable_users = user_avg_reliability[user_avg_reliability > reliability_threshold].index\n",
    "\n",
    "    # Filter the original DataFrame to include only reliable users\n",
    "    reliable_df = df[df['user_id'].isin(reliable_users)]\n",
    "\n",
    "    # Step 4: Perform the analysis using the 'analyze_direction_of_change_with_reliability' function\n",
    "    mean_change, positive_changes, negative_changes, no_changes = analyze_direction_of_change_with_reliability(\n",
    "        reliable_df, \n",
    "        group_name=\"Filtered Group\"\n",
    "    )\n",
    "\n",
    "    # Step 5: Return the results of the analysis\n",
    "    return mean_change, positive_changes, negative_changes, no_changes\n",
    "\n",
    "\n",
    "def analyze_direction_of_change_with_reliability(df, group_name):\n",
    "    \"\"\"\n",
    "    Analyzes the direction of response changes based on different question scales, \n",
    "    weighted by reliability scores. Automatically detects the scale based on the question code.\n",
    "    Handles two scales:\n",
    "    - 1-7 scale (e.g., F1 questions where 1 = completely agree and 7 = completely disagree)\n",
    "    - 1-5 scale (e.g., F2 questions where 1 = not concerned at all and 5 = very concerned)\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing before and after responses, \n",
    "                      including reliability scores.\n",
    "    - group_name (str): Name of the group for logging purposes.\n",
    "    \n",
    "    Returns:\n",
    "    - result (tuple): A tuple containing:\n",
    "        - mean_change (float): The weighted mean of response changes.\n",
    "        - positive_changes (float): The weighted count of positive response changes.\n",
    "        - negative_changes (float): The weighted count of negative response changes.\n",
    "        - no_changes (float): The weighted count of no response changes.\n",
    "    \n",
    "    Example Usage:\n",
    "    mean_change, pos_changes, neg_changes, no_changes = analyze_direction_of_change_with_reliability(\n",
    "        reliable_df, \n",
    "        group_name=\"Extreme Left - Right-Biased Articles\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    def interpret_change(row):\n",
    "        \"\"\"\n",
    "        Determines the direction of the response change based on the question type.\n",
    "        - For F1 questions (1-7 scale): A lower number indicates more agreement, so a decrease is positive.\n",
    "        - For F2 questions (1-5 scale): Higher numbers indicate more concern, so an increase is positive.\n",
    "        \"\"\"\n",
    "        if row['question_code'].startswith('F2'):\n",
    "            # 1-5 scale (concern-based): Increase is positive (more concern), decrease is negative\n",
    "            if row['numeric_response_after'] > row['numeric_response_before']:\n",
    "                return \"positive\"\n",
    "            elif row['numeric_response_after'] < row['numeric_response_before']:\n",
    "                return \"negative\"\n",
    "            else:\n",
    "                return \"no change\"\n",
    "        else:\n",
    "            # Default to 1-7 scale: Decrease is positive (more agreement), increase is negative\n",
    "            if row['numeric_response_after'] < row['numeric_response_before']:\n",
    "                return \"positive\"\n",
    "            elif row['numeric_response_after'] > row['numeric_response_before']:\n",
    "                return \"negative\"\n",
    "            else:\n",
    "                return \"no change\"\n",
    "\n",
    "    # Apply the interpretation function to each row using .loc to avoid SettingWithCopyWarning\n",
    "    df.loc[:, 'change_direction'] = df.apply(interpret_change, axis=1)\n",
    "\n",
    "    # Calculate weighted counts of positive, negative, and no changes using reliability score\n",
    "    positive_changes_weighted = (df['change_direction'] == \"positive\") * df['reliability_score']\n",
    "    negative_changes_weighted = (df['change_direction'] == \"negative\") * df['reliability_score']\n",
    "    no_changes_weighted = (df['change_direction'] == \"no change\") * df['reliability_score']\n",
    "    \n",
    "    # Sum up the weighted counts\n",
    "    positive_changes = positive_changes_weighted.sum()\n",
    "    negative_changes = negative_changes_weighted.sum()\n",
    "    no_changes = no_changes_weighted.sum()\n",
    "    \n",
    "    # Calculate the mean response change, weighted by reliability\n",
    "    df.loc[:, 'response_change'] = df['numeric_response_after'] - df['numeric_response_before']\n",
    "    mean_change = (df['response_change'] * df['reliability_score']).sum() / df['reliability_score'].sum()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Analysis of response changes for {group_name} (weighted by reliability):\")\n",
    "    print(f\"Mean change: {mean_change}\")\n",
    "    print(f\"Weighted positive changes: {positive_changes}\")\n",
    "    print(f\"Weighted negative changes: {negative_changes}\")\n",
    "    print(f\"Weighted no changes: {no_changes}\")\n",
    "    \n",
    "    return mean_change, positive_changes, negative_changes, no_changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>question_code</th>\n",
       "      <th>numeric_response_before</th>\n",
       "      <th>political_stance</th>\n",
       "      <th>numeric_response_after</th>\n",
       "      <th>bias</th>\n",
       "      <th>response_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IDUS103408</td>\n",
       "      <td>F1A10_1</td>\n",
       "      <td>7</td>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>1</td>\n",
       "      <td>right</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IDUS103408</td>\n",
       "      <td>F2A6</td>\n",
       "      <td>4</td>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>5</td>\n",
       "      <td>right</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IDUS103408</td>\n",
       "      <td>F2A7</td>\n",
       "      <td>1</td>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>2</td>\n",
       "      <td>right</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IDUS103408</td>\n",
       "      <td>F2A9</td>\n",
       "      <td>2</td>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>5</td>\n",
       "      <td>right</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IDUS103408</td>\n",
       "      <td>F3A3_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Extreme Right</td>\n",
       "      <td>5</td>\n",
       "      <td>right</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id question_code  numeric_response_before political_stance  \\\n",
       "1  IDUS103408       F1A10_1                        7    Extreme Right   \n",
       "3  IDUS103408          F2A6                        4    Extreme Right   \n",
       "5  IDUS103408          F2A7                        1    Extreme Right   \n",
       "7  IDUS103408          F2A9                        2    Extreme Right   \n",
       "9  IDUS103408        F3A3_1                        1    Extreme Right   \n",
       "\n",
       "   numeric_response_after   bias  response_change  \n",
       "1                       1  right               -6  \n",
       "3                       5  right                1  \n",
       "5                       2  right                1  \n",
       "7                       5  right                3  \n",
       "9                       5  right                4  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_bias_extreme_right.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: reliability_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m reliability_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/processed/user_ranks.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Analyze for Extreme Left users exposed to right-biased articles\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m mean_change_extreme_left_right_bias, pos_changes_extreme_left_right_bias, neg_changes_extreme_left_right_bias, no_changes_extreme_left_right_bias \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_and_analyze_with_reliability\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mright_bias_extreme_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# DataFrame with Extreme Left users exposed to right-biased articles\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreliability_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# DataFrame with reliability scores\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreliability_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.44\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set reliability threshold for filtering\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Analyze for Extreme Left users exposed to left-biased articles\u001b[39;00m\n\u001b[0;32m     12\u001b[0m mean_change_extreme_left_left_bias, pos_changes_extreme_left_left_bias, neg_changes_extreme_left_left_bias, no_changes_extreme_left_left_bias \u001b[38;5;241m=\u001b[39m filter_and_analyze_with_reliability(\n\u001b[0;32m     13\u001b[0m     left_bias_extreme_left,  \u001b[38;5;66;03m# DataFrame with Extreme Left users exposed to left-biased articles\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     reliability_df,          \u001b[38;5;66;03m# DataFrame with reliability scores\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     reliability_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.44\u001b[39m  \u001b[38;5;66;03m# Set reliability threshold for filtering\u001b[39;00m\n\u001b[0;32m     16\u001b[0m )\n",
      "Cell \u001b[1;32mIn[209], line 43\u001b[0m, in \u001b[0;36mfilter_and_analyze_with_reliability\u001b[1;34m(df, reliability_df, reliability_threshold)\u001b[0m\n\u001b[0;32m     40\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmerge(reliability_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreliability_score\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Step 2: Calculate each user's average reliability score\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m user_avg_reliability \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreliability_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Step 3: Filter for users with an average reliability score greater than the threshold\u001b[39;00m\n\u001b[0;32m     46\u001b[0m reliable_users \u001b[38;5;241m=\u001b[39m user_avg_reliability[user_avg_reliability \u001b[38;5;241m>\u001b[39m reliability_threshold]\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:1951\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1945\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[0;32m   1946\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1950\u001b[0m     )\n\u001b[1;32m-> 1951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\base.py:244\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key]\u001b[38;5;241m.\u001b[39mndim\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39mndim)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Column not found: reliability_score'"
     ]
    }
   ],
   "source": [
    "# Load reliability data\n",
    "reliability_df = pd.read_csv('../data/processed/user_ranks.csv')\n",
    "\n",
    "# Analyze for Extreme Left users exposed to right-biased articles\n",
    "mean_change_extreme_left_right_bias, pos_changes_extreme_left_right_bias, neg_changes_extreme_left_right_bias, no_changes_extreme_left_right_bias = filter_and_analyze_with_reliability(\n",
    "    right_bias_extreme_left,  # DataFrame with Extreme Left users exposed to right-biased articles\n",
    "    reliability_df,           # DataFrame with reliability scores\n",
    "    reliability_threshold=0.44  # Set reliability threshold for filtering\n",
    ")\n",
    "\n",
    "# Analyze for Extreme Left users exposed to left-biased articles\n",
    "mean_change_extreme_left_left_bias, pos_changes_extreme_left_left_bias, neg_changes_extreme_left_left_bias, no_changes_extreme_left_left_bias = filter_and_analyze_with_reliability(\n",
    "    left_bias_extreme_left,  # DataFrame with Extreme Left users exposed to left-biased articles\n",
    "    reliability_df,          # DataFrame with reliability scores\n",
    "    reliability_threshold=0.44  # Set reliability threshold for filtering\n",
    ")\n",
    "\n",
    "# Analyze for Extreme Right users exposed to right-biased articles\n",
    "mean_change_extreme_right_right_bias, pos_changes_extreme_right_right_bias, neg_changes_extreme_right_right_bias, no_changes_extreme_right_right_bias = filter_and_analyze_with_reliability(\n",
    "    right_bias_extreme_right,  # DataFrame with Extreme Right users exposed to right-biased articles\n",
    "    reliability_df,            # DataFrame with reliability scores\n",
    "    reliability_threshold=0.44  # Set reliability threshold for filtering\n",
    ")\n",
    "\n",
    "# Analyze for Extreme Right users exposed to left-biased articles\n",
    "mean_change_extreme_right_left_bias, pos_changes_extreme_right_left_bias, neg_changes_extreme_right_left_bias, no_changes_extreme_right_left_bias = filter_and_analyze_with_reliability(\n",
    "    left_bias_extreme_right,   # DataFrame with Extreme Right users exposed to left-biased articles\n",
    "    reliability_df,            # DataFrame with reliability scores\n",
    "    reliability_threshold=0.44  # Set reliability threshold for filtering\n",
    ")\n",
    "\n",
    "# Print the results for each analysis\n",
    "print(f\"Extreme Left - Right-Biased Articles: Mean Change: {mean_change_extreme_left_right_bias}, Positive: {pos_changes_extreme_left_right_bias}, Negative: {neg_changes_extreme_left_right_bias}, No Change: {no_changes_extreme_left_right_bias}\")\n",
    "print(f\"Extreme Left - Left-Biased Articles: Mean Change: {mean_change_extreme_left_left_bias}, Positive: {pos_changes_extreme_left_left_bias}, Negative: {neg_changes_extreme_left_left_bias}, No Change: {no_changes_extreme_left_left_bias}\")\n",
    "print(f\"Extreme Right - Right-Biased Articles: Mean Change: {mean_change_extreme_right_right_bias}, Positive: {pos_changes_extreme_right_right_bias}, Negative: {neg_changes_extreme_right_right_bias}, No Change: {no_changes_extreme_right_right_bias}\")\n",
    "print(f\"Extreme Right - Left-Biased Articles: Mean Change: {mean_change_extreme_right_left_bias}, Positive: {pos_changes_extreme_right_left_bias}, Negative: {neg_changes_extreme_right_left_bias}, No Change: {no_changes_extreme_right_left_bias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\n",
      "IDUS104424    0.266667\n",
      "IDUS104915    0.380952\n",
      "IDUS105157    0.470588\n",
      "IDUS106103    0.285714\n",
      "IDUS106240    0.320000\n",
      "Name: reliability_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate average reliability score per user\n",
    "user_avg_reliability = right_bias_extreme_left.groupby('user_id')['reliability_score'].mean()\n",
    "\n",
    "# Display the first few rows of user average reliability\n",
    "print(user_avg_reliability.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reliable Extreme Left users: 12\n"
     ]
    }
   ],
   "source": [
    "# Filter for users with an average reliability score greater than 0.44\n",
    "reliable_users_left = user_avg_reliability[user_avg_reliability > 0.44].index\n",
    "\n",
    "# Filter the original DataFrame to include only these reliable users\n",
    "reliable_right_bias_extreme_left = right_bias_extreme_left[right_bias_extreme_left['user_id'].isin(reliable_users_left)]\n",
    "\n",
    "# Check how many reliable users are included\n",
    "print(f\"Number of reliable Extreme Left users: {len(reliable_users_left)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['real_response_code', 'llm_response_code'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[207], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perform Wilcoxon test for reliable Extreme Left users exposed to right-biased articles (non-extreme responses)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m p_value_reliable_extreme_left_right_bias_non_extreme \u001b[38;5;241m=\u001b[39m \u001b[43mperform_non_extreme_wilcoxon_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreliable_right_bias_extreme_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Filtered DataFrame with reliable users\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreal_response_code\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Before responses (actual user responses before article exposure)\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllm_response_code\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# After responses (predicted by LLM or after article exposure)\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReliable Extreme Left - Right-Biased Articles (Non-Extreme)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp-value for Reliable Extreme Left users (Right-Biased Articles, Non-Extreme): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_value_reliable_extreme_left_right_bias_non_extreme\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[189], line 21\u001b[0m, in \u001b[0;36mperform_non_extreme_wilcoxon_test\u001b[1;34m(df, before_column, after_column, group_name, default_min, default_max)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mPerform the Wilcoxon Signed-Rank Test excluding users with extreme values that did not change.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mIf `question_code` exists, determine the extreme values based on the question type (e.g., F2 questions use 1-5 scale).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m- p_value (float): The p-value from the Wilcoxon test.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Remove any rows with NaN values in the relevant columns\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbefore_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# If 'question_code' exists in the DataFrame, use it to determine the scale\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_code\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\miria\\Desktop\\ThesisProject\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['real_response_code', 'llm_response_code'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Perform Wilcoxon test for reliable Extreme Left users exposed to right-biased articles (non-extreme responses)\n",
    "p_value_reliable_extreme_left_right_bias_non_extreme = perform_non_extreme_wilcoxon_test(\n",
    "    reliable_right_bias_extreme_left,  # Filtered DataFrame with reliable users\n",
    "    'real_response_code',     # Before responses (actual user responses before article exposure)\n",
    "    'llm_response_code',      # After responses (predicted by LLM or after article exposure)\n",
    "    'Reliable Extreme Left - Right-Biased Articles (Non-Extreme)'\n",
    ")\n",
    "\n",
    "print(f\"p-value for Reliable Extreme Left users (Right-Biased Articles, Non-Extreme): {p_value_reliable_extreme_left_right_bias_non_extreme}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of response changes for Reliable Extreme Left - Right-Biased Articles:\n",
      "Mean change: -0.25\n",
      "Number of positive changes: 15\n",
      "Number of negative changes: 30\n",
      "Number of no changes: 51\n",
      "Mean change for reliable Extreme Left users: -0.25\n",
      "Positive changes: 15, Negative changes: 30, No changes: 51\n"
     ]
    }
   ],
   "source": [
    "# Analyze the direction of change for reliable Extreme Left users exposed to right-biased articles\n",
    "mean_change_reliable_extreme_left_right_bias, pos_changes_reliable_extreme_left_right_bias, neg_changes_reliable_extreme_left_right_bias, no_changes_reliable_extreme_left_right_bias = analyze_direction_of_change(\n",
    "    reliable_right_bias_extreme_left,  # Filtered DataFrame with reliable users\n",
    "    \"Reliable Extreme Left - Right-Biased Articles\"\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean change for reliable Extreme Left users: {mean_change_reliable_extreme_left_right_bias}\")\n",
    "print(f\"Positive changes: {pos_changes_reliable_extreme_left_right_bias}, Negative changes: {neg_changes_reliable_extreme_left_right_bias}, No changes: {no_changes_reliable_extreme_left_right_bias}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right wing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'bias', 'selected_option_before', 'question_before',\n",
      "       'numeric_response_before', 'political_stance_before',\n",
      "       'selected_option_after', 'question_after', 'numeric_response_after',\n",
      "       'political_stance_after', 'reliability_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Perform the merge to add reliability_score based on user_id\n",
    "right_bias_extreme_right = right_bias_extreme_right.merge(\n",
    "    reliability_df[['user_id', 'reliability_score']], \n",
    "    on='user_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check if reliability_score has been added successfully\n",
    "print(right_bias_extreme_right.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'bias', 'selected_option_before', 'question_before',\n",
      "       'numeric_response_before', 'political_stance_before',\n",
      "       'selected_option_after', 'question_after', 'numeric_response_after',\n",
      "       'political_stance_after', 'reliability_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the columns in the DataFrame\n",
    "print(right_bias_extreme_right.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\n",
      "IDUS103408    0.242424\n",
      "IDUS103554    0.615385\n",
      "IDUS103826    0.285714\n",
      "IDUS104462    0.275862\n",
      "IDUS104578    0.421053\n",
      "Name: reliability_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate average reliability score per user\n",
    "user_avg_reliability = right_bias_extreme_right.groupby('user_id')['reliability_score'].mean()\n",
    "\n",
    "# Display the first few rows of the user average reliability\n",
    "print(user_avg_reliability.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reliable users: 29\n"
     ]
    }
   ],
   "source": [
    "# Filter for users with an average reliability score greater than 0.44 (or another threshold)\n",
    "reliable_users = user_avg_reliability[user_avg_reliability > 0.44].index\n",
    "\n",
    "# Filter the original DataFrame to include only these reliable users\n",
    "reliable_right_bias_extreme_right = right_bias_extreme_right[right_bias_extreme_right['user_id'].isin(reliable_users)]\n",
    "\n",
    "# Check how many reliable users are included\n",
    "print(f\"Number of reliable users: {len(reliable_users)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon test for Reliable Extreme Right - Right-Biased Articles (Non-Extreme):\n",
      "Test statistic: 2436.0, p-value: 0.5200795218671146\n",
      "p-value for Reliable Extreme Right users (Right-Biased Articles, Non-Extreme): 0.5200795218671146\n"
     ]
    }
   ],
   "source": [
    "# Perform Wilcoxon test for reliable right-wing users exposed to right-biased articles (non-extreme responses)\n",
    "p_value_reliable_extreme_right_right_bias_non_extreme = perform_non_extreme_wilcoxon_test(\n",
    "    reliable_right_bias_extreme_right,  # Filtered DataFrame with reliable right-wing users\n",
    "    'numeric_response_before',          # Before responses (actual user responses before article exposure)\n",
    "    'numeric_response_after',           # After responses (predicted by LLM or after article exposure)\n",
    "    'Reliable Extreme Right - Right-Biased Articles (Non-Extreme)'\n",
    ")\n",
    "\n",
    "# Output the p-value\n",
    "print(f\"p-value for Reliable Extreme Right users (Right-Biased Articles, Non-Extreme): {p_value_reliable_extreme_right_right_bias_non_extreme}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    numeric_response_before  numeric_response_after  response_change\n",
      "8                         5                       1               -4\n",
      "9                         4                       5                1\n",
      "10                        2                       1               -1\n",
      "11                        4                       5                1\n",
      "12                        6                       6                0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miria\\AppData\\Local\\Temp\\ipykernel_24068\\2964947862.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reliable_right_bias_extreme_right['response_change'] = reliable_right_bias_extreme_right['numeric_response_after'] - reliable_right_bias_extreme_right['numeric_response_before']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the response change as the difference between numeric_response_after and numeric_response_before\n",
    "reliable_right_bias_extreme_right['response_change'] = reliable_right_bias_extreme_right['numeric_response_after'] - reliable_right_bias_extreme_right['numeric_response_before']\n",
    "\n",
    "# Check if the response_change column has been added correctly\n",
    "print(reliable_right_bias_extreme_right[['numeric_response_before', 'numeric_response_after', 'response_change']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of response changes for Reliable Extreme Right - Right-Biased Articles:\n",
      "Mean change: -0.20689655172413793\n",
      "Number of positive changes: 60\n",
      "Number of negative changes: 42\n",
      "Number of no changes: 130\n",
      "Mean change for reliable right-wing users: -0.20689655172413793\n",
      "Positive changes: 60\n",
      "Negative changes: 42\n",
      "No changes: 130\n"
     ]
    }
   ],
   "source": [
    "# Analyze direction of change for reliable right-wing users exposed to right-biased articles\n",
    "mean_change_reliable_extreme_right_right_bias, pos_changes_reliable_extreme_right_right_bias, neg_changes_reliable_extreme_right_right_bias, no_changes_reliable_extreme_right_right_bias = analyze_direction_of_change(\n",
    "    reliable_right_bias_extreme_right,  # Filtered DataFrame with reliable right-wing users\n",
    "    \"Reliable Extreme Right - Right-Biased Articles\"\n",
    ")\n",
    "\n",
    "# Output the results\n",
    "print(f\"Mean change for reliable right-wing users: {mean_change_reliable_extreme_right_right_bias}\")\n",
    "print(f\"Positive changes: {pos_changes_reliable_extreme_right_right_bias}\")\n",
    "print(f\"Negative changes: {neg_changes_reliable_extreme_right_right_bias}\")\n",
    "print(f\"No changes: {no_changes_reliable_extreme_right_right_bias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
