{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Results:\n",
    "This notebook includes the following analyses:\n",
    "* Overall changes in responses before exposure to articles and after exposure to articles\n",
    "  * Overall difference in responses between left and right biased articles exposure\n",
    "  * Overall differences in responses by question\n",
    "  * Differences in responses by political groups\n",
    "  * Differences in responses by article bias: which bias causes the greater changes and in which direction?\n",
    "  * Differences in responses by article bias by political group\n",
    "  * Differences in responses by article bias, by political group, and by question\n",
    "* Radicalization analysis:\n",
    "  * Reinforcement of right wing opinions if right wing users are exposed to right wing biased articles\n",
    "  * Reinforcement of left wing opinions if left wing are exposed to left wing biased articles\n",
    "* LLM to persona alignment before vs after exposure to articles: (is the llm more consistent with the actual person's responses after it was exposed to a. right biased article b. left biased articles.)\n",
    "* Significance test: is the change caused by the articles?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Extraction and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of load_json_data to print more details about malformation\n",
    "def load_json_data(filepath):\n",
    "    \"\"\"Loads JSON data from the provided file path and attempts to fix malformed sections.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # First attempt to load the JSON directly\n",
    "        try:\n",
    "            return json.loads(content)  # If well-formed, this will work\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Malformed JSON detected: {str(e)}\")\n",
    "            print(\"Attempting to fix...\")\n",
    "\n",
    "            # Fix common malformation: misplaced brackets (e.g., '][')\n",
    "            content_fixed = content.replace(\"][\", \"],[\")\n",
    "\n",
    "            # Attempt to load the fixed content\n",
    "            try:\n",
    "                return json.loads(f\"[{content_fixed}]\")  # Wrap content in square brackets\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error: Unable to decode JSON after attempt to fix. {str(e)}\")\n",
    "                return None\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {filepath} not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Failed to decode JSON from {filepath}. {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_after_responses(response_data):\n",
    "    \"\"\"\n",
    "    Flattens the nested structure of the after responses data.\n",
    "    Converts the list of questions for each user into individual rows.\n",
    "    \"\"\"\n",
    "    flattened_data = []\n",
    "    \n",
    "    for user_responses in response_data:\n",
    "        for entry in user_responses:\n",
    "            user_id = entry['user_id']\n",
    "            question_code = entry['question_code']\n",
    "            bias = entry['bias']\n",
    "            selected_option = entry['response'].get('selected_option')\n",
    "\n",
    "            # Append a flattened version of the entry\n",
    "            flattened_data.append({\n",
    "                'user_id': user_id,\n",
    "                'question_code': question_code,\n",
    "                'bias': bias,\n",
    "                'selected_option': selected_option,\n",
    "                'question': entry.get('question')\n",
    "            })\n",
    "    \n",
    "    return flattened_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_responses_path = \"../data/processed/before_responses.json\"\n",
    "after_responses_path = \"../data/processed/after_responses.json\"\n",
    "persona_prompts_path = '../data/processed/persona_prompts.json'\n",
    "question_codes_path = '../data/raw/question_codes.json'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_question_code_mapping(question_codes_path):\n",
    "    \"\"\"Load and filter the question-to-code mapping from the question_codes.json file.\"\"\"\n",
    "    question_codes_data = load_json_data(question_codes_path)\n",
    "    \n",
    "    # List of valid question codes relevant to the analysis\n",
    "    valid_question_codes = ['F1A10_1', 'F2A6', 'F2A7', 'F2A9', 'F3A3_1', 'F3A6_1', 'F3A7_1', 'F3A8_1']\n",
    "    \n",
    "    # Filter only questions that are in the valid_question_codes list\n",
    "    filtered_data = [entry for entry in question_codes_data if entry['code'] in valid_question_codes]\n",
    "\n",
    "    # Create a mapping for each question from text to numerical codes\n",
    "    question_code_mapping = {}\n",
    "    for entry in filtered_data:\n",
    "        question = entry['question']\n",
    "        code = entry['code']\n",
    "        options = entry['options']\n",
    "        \n",
    "        # Reverse the options dictionary to map response text to numerical values\n",
    "        reversed_options = {v: k for k, v in options.items()}\n",
    "        question_code_mapping[question] = {'code': code, 'options': reversed_options}\n",
    "\n",
    "    return question_code_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_after_responses_dataframe(flattened_data):\n",
    "    \"\"\"\n",
    "    Converts the flattened after responses into a structured DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    \n",
    "    # Set a multi-index using user_id, question_code, and bias\n",
    "    df.set_index(['user_id', 'question_code', 'bias'], inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actual_question(full_prompt):\n",
    "    \"\"\"Extracts the actual question from the full prompt in the 'question' field and cleans it.\"\"\"\n",
    "    \n",
    "    # Use regex to find text between **Question**: and **Options**:\n",
    "    match = re.search(r'\\*\\*Question\\*\\*[:\\s]*(.*?)\\*\\*Options\\*\\*', full_prompt, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        question = match.group(1).strip()  # Extracted question\n",
    "        # Clean the question: remove extra spaces and newlines\n",
    "        question_cleaned = re.sub(r'\\s+', ' ', question).strip()\n",
    "        return question_cleaned\n",
    "    \n",
    "    # If no structured question is found, return the full prompt (fallback)\n",
    "    return full_prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_clean_dataframe_with_codes(response_data, question_code_mapping, has_bias=False):\n",
    "    \"\"\"\n",
    "    Converts a list of response data dictionaries into a structured DataFrame with question codes.\n",
    "    Handles optional fields like bias if present in the data (for after_responses).\n",
    "    Parameters:\n",
    "    - response_data: List of dictionaries containing the response data.\n",
    "    - question_code_mapping: A dictionary that maps question texts to their corresponding codes.\n",
    "    - has_bias: Boolean flag to indicate if the data includes bias (for after_responses).\n",
    "    \"\"\"\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for entry in response_data:\n",
    "        user_id = entry['user_id']\n",
    "        full_prompt = entry['question']  # Full prompt in the before_responses\n",
    "        selected_option = entry['response']['selected_option']\n",
    "        bias = entry.get('bias', None) if has_bias else None\n",
    "\n",
    "        # Step 1: Extract the actual question from the full prompt\n",
    "        question = extract_actual_question(full_prompt)\n",
    "        \n",
    "        if question is None:\n",
    "            print(f\"Warning: Could not extract question from prompt for user {user_id}. Skipping entry.\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Look up the question_code from the extracted question text\n",
    "        question_code = question_code_mapping.get(question, {}).get('code')\n",
    "        \n",
    "        if question_code is None:\n",
    "            print(f\"Warning: No question_code found for question '{question}' for user {user_id}. Skipping entry.\")\n",
    "            continue\n",
    "\n",
    "        # Build the cleaned entry\n",
    "        cleaned_entry = {\n",
    "            'user_id': user_id,\n",
    "            'question_code': question_code,\n",
    "            'bias': bias,\n",
    "            'selected_option': selected_option,\n",
    "            'question': question\n",
    "        }\n",
    "        \n",
    "        cleaned_data.append(cleaned_entry)\n",
    "    \n",
    "    # Convert cleaned data into a DataFrame and use MultiIndex if bias is included\n",
    "    df = pd.DataFrame(cleaned_data)\n",
    "    if has_bias:\n",
    "        df.set_index(['user_id', 'question_code', 'bias'], inplace=True)\n",
    "    else:\n",
    "        df.set_index(['user_id', 'question_code'], inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_responses_to_numeric(df, question_code_mapping):\n",
    "    \"\"\"\n",
    "    Maps the string-based responses to their corresponding numerical values using the question code mapping.\n",
    "    \"\"\"\n",
    "    def map_response(row):\n",
    "        question_text = row['question']  # Use the cleaned question column\n",
    "        selected_option = row['selected_option']  # Directly access the selected option column\n",
    "        \n",
    "        # Find the mapping for the current question text\n",
    "        if question_text in question_code_mapping:\n",
    "            mapping = question_code_mapping[question_text]['options']\n",
    "            # Return the numerical code for the response\n",
    "            return mapping.get(selected_option, None)  # Return None if no match found\n",
    "        return None\n",
    "    \n",
    "    # Apply the mapping function to each row\n",
    "    df['numeric_response'] = df.apply(map_response, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_political_stance(persona_prompt):\n",
    "    \"\"\"Extracts the political stance from the persona prompt text.\"\"\"\n",
    "    match = re.search(r\"\\*\\*Your Own Political Position\\*\\*:\\s*You consider your political position to be\\s*'(.*?)'\\s*on the political scale\", persona_prompt)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()  # Extract the political position (e.g., 'Extreme Left')\n",
    "    return None\n",
    "\n",
    "\n",
    "# Load the persona prompts\n",
    "def load_persona_prompts(filepath):\n",
    "    \"\"\"Loads the persona prompts data and creates a mapping of user_id to political stance.\"\"\"\n",
    "    persona_data = load_json_data(filepath)  # Reusing load_json_data to load the file\n",
    "    if persona_data is None:\n",
    "        return {}\n",
    "\n",
    "    # Create a mapping of user_id to political stance\n",
    "    persona_mapping = {}\n",
    "    for entry in persona_data:\n",
    "        user_id = entry['user_id']\n",
    "        political_stance = extract_political_stance(entry['persona_prompt'])\n",
    "        persona_mapping[user_id] = political_stance\n",
    "    \n",
    "    return persona_mapping\n",
    "\n",
    "def add_political_stance_to_df(df, persona_mapping):\n",
    "    \"\"\"Adds political stance column to the given DataFrame based on user_id, and combines Far Left and Extreme Left.\"\"\"\n",
    "    # Map political stance from persona_mapping to the DataFrame\n",
    "    df['political_stance'] = df.index.get_level_values('user_id').map(persona_mapping)\n",
    "\n",
    "    # Merge Far Left and Extreme Left into a single group: Extreme Left\n",
    "    df['political_stance'] = df['political_stance'].replace({'Far Left': 'Extreme Left'})\n",
    "\n",
    "    # Debug: Check for missing political stance mappings\n",
    "    missing_stance_df = df[df['political_stance'].isna()]\n",
    "    if not missing_stance_df.empty:\n",
    "        print(\"These user_ids have no political stance mapped:\")\n",
    "        print(missing_stance_df.index.get_level_values('user_id').unique())\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malformed JSON detected: Extra data: line 178 column 2 (char 5656)\n",
      "Attempting to fix...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the question code mapping for both before and after responses\n",
    "question_code_mapping = load_question_code_mapping(question_codes_path)\n",
    "persona_mapping = load_persona_prompts(persona_prompts_path)\n",
    "\n",
    "## Process the BEFORE data\n",
    "\n",
    "before_responses = load_json_data(before_responses_path)\n",
    "before_responses_df = create_clean_dataframe_with_codes(before_responses, question_code_mapping)\n",
    "before_responses_df = map_responses_to_numeric(before_responses_df, question_code_mapping)\n",
    "before_responses_df = add_political_stance_to_df(before_responses_df, persona_mapping)\n",
    "\n",
    "## Process the AFTER data\n",
    "\n",
    "after_responses = load_json_data(after_responses_path)\n",
    "flattened_after_responses = flatten_after_responses(after_responses)\n",
    "after_responses_df = create_after_responses_dataframe(flattened_after_responses)\n",
    "after_responses_df = map_responses_to_numeric(after_responses_df, question_code_mapping)\n",
    "after_responses_df = add_political_stance_to_df(after_responses_df, persona_mapping)\n",
    "\n",
    "## Next Steps: Analyze the data\n",
    "\n",
    "# Print the first few rows of both DataFrames to verify the result\n",
    "#print(\"Before Responses DataFrame:\")\n",
    "#print(before_responses_df.head())\n",
    "\n",
    "#print(\"\\nAfter Responses DataFrame:\")\n",
    "#print(after_responses_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe for the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id question_code  numeric_response_before political_stance  \\\n",
      "0  IDUS103408       F1A10_1                        7    Extreme Right   \n",
      "1  IDUS103408       F1A10_1                        7    Extreme Right   \n",
      "2  IDUS103408          F2A6                        4    Extreme Right   \n",
      "3  IDUS103408          F2A6                        4    Extreme Right   \n",
      "4  IDUS103408          F2A7                        1    Extreme Right   \n",
      "\n",
      "   numeric_response_after   bias  response_change  \n",
      "0                       5   left               -2  \n",
      "1                       1  right               -6  \n",
      "2                       4   left                0  \n",
      "3                       5  right                1  \n",
      "4                       5   left                4  \n"
     ]
    }
   ],
   "source": [
    "# Ensure the numeric_response columns are in numeric format before merging\n",
    "def ensure_numeric_format(df, column):\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "before_responses_df = ensure_numeric_format(before_responses_df, 'numeric_response')\n",
    "after_responses_df = ensure_numeric_format(after_responses_df, 'numeric_response')\n",
    "\n",
    "# Merge the before and after responses DataFrames on user_id and question_code\n",
    "def merge_before_after_responses(before_df, after_df):\n",
    "    # Reset the index to bring 'user_id' and 'question_code' back as columns\n",
    "    before_df = before_df.reset_index()\n",
    "    after_df = after_df.reset_index()\n",
    "\n",
    "    merged_df = pd.merge(\n",
    "        before_df[['user_id', 'question_code', 'numeric_response', 'political_stance']],\n",
    "        after_df[['user_id', 'question_code', 'numeric_response', 'bias', 'political_stance']],\n",
    "        on=['user_id', 'question_code', 'political_stance'],  # Merge on user_id, question_code, and political_stance\n",
    "        suffixes=('_before', '_after')\n",
    "    )\n",
    "\n",
    "    # Calculate the change in response (after - before)\n",
    "    merged_df['response_change'] = merged_df['numeric_response_after'] - merged_df['numeric_response_before']\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_responses_df = merge_before_after_responses(before_responses_df, after_responses_df)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_responses_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Changes in Responses Before and After Exposure to Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step aims to assess how much the responses changed in general after exposure to articles. This involves calcultaing the average of the response_change across all agents and questions, and summarizing the overall shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Changes in Responses (Before vs After Exposure to Articles):\n",
      "change_summary: count    1872.000000\n",
      "mean       -0.162927\n",
      "std         1.849973\n",
      "min        -6.000000\n",
      "25%        -1.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         6.000000\n",
      "Name: response_change, dtype: float64\n",
      "avg_change: -0.16292735042735043\n",
      "positive_changes: 423\n",
      "negative_changes: 490\n",
      "no_changes: 959\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Summary of overall changes in responses before and after exposure to articles\n",
    "def overall_changes_analysis(merged_df):\n",
    "    \"\"\"\n",
    "    Analyze the overall changes in responses before and after exposure to articles.\n",
    "    \"\"\"\n",
    "    # Summary statistics of the response changes\n",
    "    change_summary = merged_df['response_change'].describe()\n",
    "\n",
    "    # Calculate the average response change\n",
    "    avg_change = merged_df['response_change'].mean()\n",
    "\n",
    "    # Count positive, negative, and no changes in response\n",
    "    positive_changes = (merged_df['response_change'] > 0).sum()\n",
    "    negative_changes = (merged_df['response_change'] < 0).sum()\n",
    "    no_changes = (merged_df['response_change'] == 0).sum()\n",
    "\n",
    "    return {\n",
    "        'change_summary': change_summary,\n",
    "        'avg_change': avg_change,\n",
    "        'positive_changes': positive_changes,\n",
    "        'negative_changes': negative_changes,\n",
    "        'no_changes': no_changes\n",
    "    }\n",
    "\n",
    "# Perform the overall changes analysis\n",
    "overall_change_results = overall_changes_analysis(merged_responses_df)\n",
    "\n",
    "# Display the results\n",
    "print(\"Overall Changes in Responses (Before vs After Exposure to Articles):\")\n",
    "for key, value in overall_change_results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall difference in responses between left and right biased articles exposure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Response Changes Between Left and Right Biased Articles:\n",
      "avg_left_change: -0.09188034188034189\n",
      "avg_right_change: -0.23397435897435898\n",
      "left_summary: count    936.000000\n",
      "mean      -0.091880\n",
      "std        1.728682\n",
      "min       -6.000000\n",
      "25%       -1.000000\n",
      "50%        0.000000\n",
      "75%        0.000000\n",
      "max        6.000000\n",
      "Name: response_change, dtype: float64\n",
      "right_summary: count    936.000000\n",
      "mean      -0.233974\n",
      "std        1.962144\n",
      "min       -6.000000\n",
      "25%       -1.000000\n",
      "50%        0.000000\n",
      "75%        0.000000\n",
      "max        6.000000\n",
      "Name: response_change, dtype: float64\n",
      "left_positive_changes: 217\n",
      "left_negative_changes: 242\n",
      "right_positive_changes: 206\n",
      "right_negative_changes: 248\n"
     ]
    }
   ],
   "source": [
    "# Function to compare response changes between left- and right-biased articles\n",
    "def compare_left_right_bias(merged_df):\n",
    "    # Separate left-biased and right-biased responses\n",
    "    left_bias_df = merged_df[merged_df['bias'] == 'left']\n",
    "    right_bias_df = merged_df[merged_df['bias'] == 'right']\n",
    "\n",
    "    # Calculate average response change for left and right bias\n",
    "    avg_left_change = left_bias_df['response_change'].mean()\n",
    "    avg_right_change = right_bias_df['response_change'].mean()\n",
    "\n",
    "    # Calculate summary statistics for each bias\n",
    "    left_summary = left_bias_df['response_change'].describe()\n",
    "    right_summary = right_bias_df['response_change'].describe()\n",
    "\n",
    "    # Count of positive and negative changes for each bias\n",
    "    left_positive_changes = (left_bias_df['response_change'] > 0).sum()\n",
    "    left_negative_changes = (left_bias_df['response_change'] < 0).sum()\n",
    "\n",
    "    right_positive_changes = (right_bias_df['response_change'] > 0).sum()\n",
    "    right_negative_changes = (right_bias_df['response_change'] < 0).sum()\n",
    "\n",
    "    return {\n",
    "        'avg_left_change': avg_left_change,\n",
    "        'avg_right_change': avg_right_change,\n",
    "        'left_summary': left_summary,\n",
    "        'right_summary': right_summary,\n",
    "        'left_positive_changes': left_positive_changes,\n",
    "        'left_negative_changes': left_negative_changes,\n",
    "        'right_positive_changes': right_positive_changes,\n",
    "        'right_negative_changes': right_negative_changes\n",
    "    }\n",
    "\n",
    "# Perform the comparison between left and right bias\n",
    "bias_comparison_results = compare_left_right_bias(merged_responses_df)\n",
    "\n",
    "# Display the results\n",
    "print(\"Comparison of Response Changes Between Left and Right Biased Articles:\")\n",
    "for key, value in bias_comparison_results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Overall differences in responses by question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Differences in Responses by Question:\n",
      "  question_code  count  avg_change  std_change  min_change  max_change  \\\n",
      "2          F2A7    234    0.619658    1.845014          -4           4   \n",
      "3          F2A9    234    0.064103    1.755480          -4           3   \n",
      "7        F3A8_1    234    0.047009    1.320819          -6           6   \n",
      "6        F3A7_1    234    0.008547    0.711593          -3           2   \n",
      "5        F3A6_1    234   -0.085470    0.675413          -6           1   \n",
      "1          F2A6    234   -0.175214    1.533344          -4           4   \n",
      "4        F3A3_1    234   -0.341880    2.589720          -6           6   \n",
      "0       F1A10_1    234   -1.440171    2.610364          -6           6   \n",
      "\n",
      "   median_change  \n",
      "2            0.0  \n",
      "3            0.0  \n",
      "7            0.0  \n",
      "6            0.0  \n",
      "5            0.0  \n",
      "1            0.0  \n",
      "4            0.0  \n",
      "0            0.0  \n"
     ]
    }
   ],
   "source": [
    "# Function to analyze overall differences in responses by question\n",
    "def analyze_differences_by_question(merged_df):\n",
    "    # Group by question_code and calculate statistics for each question\n",
    "    question_analysis = merged_df.groupby('question_code')['response_change'].agg(\n",
    "        count='count',\n",
    "        avg_change='mean',\n",
    "        std_change='std',\n",
    "        min_change='min',\n",
    "        max_change='max',\n",
    "        median_change='median'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort the results by the average change to identify the most impacted questions\n",
    "    question_analysis = question_analysis.sort_values(by='avg_change', ascending=False)\n",
    "\n",
    "    return question_analysis\n",
    "\n",
    "# Perform the analysis by question\n",
    "question_analysis_results = analyze_differences_by_question(merged_responses_df)\n",
    "\n",
    "# Display the results\n",
    "print(\"Overall Differences in Responses by Question:\")\n",
    "print(question_analysis_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences in responses by political groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences in Responses by Political Group:\n",
      "  political_stance  count  avg_change  std_change  min_change  max_change  \\\n",
      "1    Extreme Right   1088   -0.034926    2.147434          -6           6   \n",
      "0     Extreme Left    784   -0.340561    1.312125          -6           6   \n",
      "\n",
      "   median_change  \n",
      "1            0.0  \n",
      "0            0.0  \n"
     ]
    }
   ],
   "source": [
    "def analyze_differences_by_political_group(merged_df):\n",
    "    # Group by political stance and calculate statistics for each group\n",
    "    political_group_analysis = merged_df.groupby('political_stance')['response_change'].agg(\n",
    "        count='count',\n",
    "        avg_change='mean',\n",
    "        std_change='std',\n",
    "        min_change='min',\n",
    "        max_change='max',\n",
    "        median_change='median'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort the results by the average change to identify the groups with the most impacted responses\n",
    "    political_group_analysis = political_group_analysis.sort_values(by='avg_change', ascending=False)\n",
    "\n",
    "    return political_group_analysis\n",
    "\n",
    "# Perform the analysis by political group\n",
    "political_group_analysis_results = analyze_differences_by_political_group(merged_responses_df)\n",
    "\n",
    "# Display the results\n",
    "print(\"Differences in Responses by Political Group:\")\n",
    "print(political_group_analysis_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences in responses by article bias: which bias causes the greater changes and in which direction?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences in Responses by Article Bias:\n",
      "    bias  count  avg_change  std_change  min_change  max_change  median_change\n",
      "0   left    936   -0.091880    1.728682          -6           6            0.0\n",
      "1  right    936   -0.233974    1.962144          -6           6            0.0\n"
     ]
    }
   ],
   "source": [
    "def analyze_differences_by_article_bias_only(merged_df):\n",
    "    # Group by article bias and calculate statistics for response changes\n",
    "    bias_analysis = merged_df.groupby('bias')['response_change'].agg(\n",
    "        count='count',\n",
    "        avg_change='mean',\n",
    "        std_change='std',\n",
    "        min_change='min',\n",
    "        max_change='max',\n",
    "        median_change='median'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort the results by the average change to easily compare bias impact\n",
    "    bias_analysis = bias_analysis.sort_values(by='avg_change', ascending=False)\n",
    "\n",
    "    return bias_analysis\n",
    "\n",
    "# Perform the analysis by article bias only\n",
    "bias_only_analysis_results = analyze_differences_by_article_bias_only(merged_responses_df)\n",
    "\n",
    "# Display the results\n",
    "print(\"Differences in Responses by Article Bias:\")\n",
    "print(bias_only_analysis_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences in responses by article bias by political group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences in Responses by Article Bias and Political Group:\n",
      "    bias political_stance  count  avg_change  std_change  min_change  \\\n",
      "1   left    Extreme Right    544    0.014706    1.978653          -6   \n",
      "3  right    Extreme Right    544   -0.084559    2.304656          -6   \n",
      "0   left     Extreme Left    392   -0.239796    1.292769          -6   \n",
      "2  right     Extreme Left    392   -0.441327    1.325194          -6   \n",
      "\n",
      "   max_change  median_change  \n",
      "1           6            0.0  \n",
      "3           6            0.0  \n",
      "0           6            0.0  \n",
      "2           5            0.0  \n"
     ]
    }
   ],
   "source": [
    "# Function to analyze differences in responses by article bias and political group\n",
    "def analyze_differences_by_article_bias_and_group(merged_df):\n",
    "    # Group by article bias and political stance, then calculate statistics for response changes\n",
    "    bias_group_analysis = merged_df.groupby(['bias', 'political_stance'])['response_change'].agg(\n",
    "        count='count',\n",
    "        avg_change='mean',\n",
    "        std_change='std',\n",
    "        min_change='min',\n",
    "        max_change='max',\n",
    "        median_change='median'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort the results to easily compare bias impact\n",
    "    bias_group_analysis = bias_group_analysis.sort_values(by='avg_change', ascending=False)\n",
    "\n",
    "    return bias_group_analysis\n",
    "\n",
    "# Perform the analysis by article bias and political group\n",
    "bias_group_analysis_results = analyze_differences_by_article_bias_and_group(merged_responses_df)\n",
    "\n",
    "# Display the results\n",
    "print(\"Differences in Responses by Article Bias and Political Group:\")\n",
    "print(bias_group_analysis_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences in responses by article bias, by political group, and by question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences in Responses by Article Bias, Political Group, and Question:\n",
      "     bias political_stance question_code  count  avg_change  std_change  \\\n",
      "10   left    Extreme Right          F2A7     68    3.000000    0.646460   \n",
      "27  right    Extreme Right          F2A9     68    2.073529    1.083336   \n",
      "28  right    Extreme Right        F3A3_1     68    1.632353    2.278415   \n",
      "15   left    Extreme Right        F3A8_1     68    0.691176    1.659503   \n",
      "25  right    Extreme Right          F2A6     68    0.617647    1.051359   \n",
      "6    left     Extreme Left        F3A7_1     49    0.408163    0.761533   \n",
      "9    left    Extreme Right          F2A6     68    0.338235    1.140964   \n",
      "0    left     Extreme Left       F1A10_1     49    0.244898    1.010994   \n",
      "3    left     Extreme Left          F2A9     49    0.142857    0.500000   \n",
      "16  right     Extreme Left       F1A10_1     49    0.102041    1.159111   \n",
      "4    left     Extreme Left        F3A3_1     49    0.061224    1.375502   \n",
      "29  right    Extreme Right        F3A6_1     68    0.029412    0.170214   \n",
      "13   left    Extreme Right        F3A6_1     68    0.014706    0.121268   \n",
      "14   left    Extreme Right        F3A7_1     68    0.000000    0.000000   \n",
      "31  right    Extreme Right        F3A8_1     68    0.000000    0.000000   \n",
      "26  right    Extreme Right          F2A7     68    0.000000    1.007435   \n",
      "30  right    Extreme Right        F3A7_1     68    0.000000    0.000000   \n",
      "7    left     Extreme Left        F3A8_1     49   -0.040816    1.171879   \n",
      "18  right     Extreme Left          F2A7     49   -0.142857    0.978945   \n",
      "20  right     Extreme Left        F3A3_1     49   -0.163265    1.280412   \n",
      "8    left    Extreme Right       F1A10_1     68   -0.176471    1.424112   \n",
      "5    left     Extreme Left        F3A6_1     49   -0.204082    1.098855   \n",
      "21  right     Extreme Left        F3A6_1     49   -0.265306    0.930401   \n",
      "22  right     Extreme Left        F3A7_1     49   -0.367347    1.253227   \n",
      "17  right     Extreme Left          F2A6     49   -0.693878    1.488905   \n",
      "23  right     Extreme Left        F3A8_1     49   -0.693878    1.446318   \n",
      "11   left    Extreme Right          F2A9     68   -1.014706    1.057915   \n",
      "2    left     Extreme Left          F2A7     49   -1.061224    1.231668   \n",
      "19  right     Extreme Left          F2A9     49   -1.306122    1.488905   \n",
      "1    left     Extreme Left          F2A6     49   -1.469388    1.608597   \n",
      "12   left    Extreme Right        F3A3_1     68   -2.735294    2.385389   \n",
      "24  right    Extreme Right       F1A10_1     68   -5.029412    1.209053   \n",
      "\n",
      "    min_change  max_change  median_change  \n",
      "10           2           4            3.0  \n",
      "27           0           3            3.0  \n",
      "28          -2           6            1.0  \n",
      "15           0           6            0.0  \n",
      "25          -1           4            0.0  \n",
      "6           -1           2            0.0  \n",
      "9           -1           4            0.0  \n",
      "0           -2           6            0.0  \n",
      "3           -1           1            0.0  \n",
      "16          -5           5            0.0  \n",
      "4           -3           5            0.0  \n",
      "29           0           1            0.0  \n",
      "13           0           1            0.0  \n",
      "14           0           0            0.0  \n",
      "31           0           0            0.0  \n",
      "26          -1           3            0.0  \n",
      "30           0           0            0.0  \n",
      "7           -2           4            0.0  \n",
      "18          -3           2            0.0  \n",
      "20          -3           4            0.0  \n",
      "8           -6           6            0.0  \n",
      "5           -6           1            0.0  \n",
      "21          -6           1            0.0  \n",
      "22          -3           2            0.0  \n",
      "17          -4           3           -1.0  \n",
      "23          -6           1            0.0  \n",
      "11          -3           1           -1.0  \n",
      "2           -4           1           -1.0  \n",
      "19          -4           2           -1.0  \n",
      "1           -4           1           -1.0  \n",
      "12          -6           2           -3.5  \n",
      "24          -6           2           -5.0  \n"
     ]
    }
   ],
   "source": [
    "# Function to analyze differences by article bias, political group, and question\n",
    "def analyze_differences_by_bias_group_question(merged_df):\n",
    "    # Group by article bias, political stance, and question_code, then calculate statistics for response changes\n",
    "    bias_group_question_analysis = merged_df.groupby(['bias', 'political_stance', 'question_code'])['response_change'].agg(\n",
    "        count='count',\n",
    "        avg_change='mean',\n",
    "        std_change='std',\n",
    "        min_change='min',\n",
    "        max_change='max',\n",
    "        median_change='median'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort the results by the average change to easily compare impact\n",
    "    bias_group_question_analysis = bias_group_question_analysis.sort_values(by='avg_change', ascending=False)\n",
    "\n",
    "    return bias_group_question_analysis\n",
    "\n",
    "# Perform the analysis by article bias, political group, and question\n",
    "bias_group_question_analysis_results = analyze_differences_by_bias_group_question(merged_responses_df)\n",
    "\n",
    "# Display the results\n",
    "print(\"Differences in Responses by Article Bias, Political Group, and Question:\")\n",
    "print(bias_group_question_analysis_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Responses Analysis\n",
    "Responses cannot get more extreme than what they already are, so they stay the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of stable responses at the extremes: 82.59%\n"
     ]
    }
   ],
   "source": [
    "# Update the code to avoid the SettingWithCopyWarning\n",
    "def check_stable_responses_at_extreme(merged_df, stable_threshold=0.5):\n",
    "    # Filter for responses where response_change is close to zero (stable responses)\n",
    "    stable_responses_df = merged_df[merged_df['response_change'].abs() <= stable_threshold].copy()\n",
    "    \n",
    "    # For F2 questions, the scale is 1-5, for others it's 1-7\n",
    "    def is_at_extreme(row):\n",
    "        if row['question_code'].startswith('F2'):\n",
    "            return row['numeric_response_before'] in [1, 5]  # Extreme values for F2 questions\n",
    "        else:\n",
    "            return row['numeric_response_before'] in [1, 7]  # Extreme values for non-F2 questions\n",
    "    \n",
    "    # Apply the extreme check function to each row\n",
    "    stable_responses_df.loc[:, 'at_extreme'] = stable_responses_df.apply(is_at_extreme, axis=1)\n",
    "    \n",
    "    # Calculate the number of stable responses that were at the extremes\n",
    "    stable_at_extreme_count = stable_responses_df['at_extreme'].sum()\n",
    "    \n",
    "    # Calculate the percentage of stable responses that were at the extremes\n",
    "    percentage_at_extreme = stable_at_extreme_count / len(stable_responses_df) * 100\n",
    "    \n",
    "    return {\n",
    "        'total_stable_responses': len(stable_responses_df),\n",
    "        'stable_at_extreme_count': stable_at_extreme_count,\n",
    "        'percentage_at_extreme': percentage_at_extreme\n",
    "    }\n",
    "\n",
    "# Re-run the updated analysis to avoid the warning\n",
    "extreme_stability_results = check_stable_responses_at_extreme(merged_responses_df)\n",
    "percentage = extreme_stability_results[\"percentage_at_extreme\"]\n",
    "print(f\"Percentage of stable responses at the extremes: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radicalization analysis:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement of right wing opinions if right wing users are exposed to right wing biased articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement of Right-Wing Opinions (Extreme Right Users, Right-Biased Articles):\n",
      "Total right-wing responses: 544\n",
      "Reinforced responses: 115\n",
      "Percentage of reinforced responses: 21.14%\n",
      "Non-reinforced responses: 429\n",
      "Stable non-reinforced responses at extremes: 312\n",
      "Percentage of stable non-reinforced responses at extremes: 72.73%\n",
      "Non-extreme responses reinforced: 102\n",
      "Percentage of non-extreme responses reinforced: 46.58%\n",
      "Non-extreme responses not reinforced: 117\n",
      "Percentage of non-extreme responses not reinforced: 53.42%\n"
     ]
    }
   ],
   "source": [
    "# Updated function to analyze reinforcement, stable extremes, and non-extreme responses not reinforced\n",
    "def analyze_reinforcement_and_stable_extremes(merged_df):\n",
    "    # Filter for Extreme Right users and right-wing biased articles\n",
    "    right_wing_df = merged_df[(merged_df['political_stance'] == 'Extreme Right') & (merged_df['bias'] == 'right')].copy()\n",
    "\n",
    "    # Function to check if the response was reinforced (became more extreme)\n",
    "    def is_reinforced(row):\n",
    "        if row['question_code'].startswith('F2'):  # F2 questions (concern scale)\n",
    "            return row['numeric_response_after'] > row['numeric_response_before']  # More concern\n",
    "        else:  # Agreement scale\n",
    "            return (row['numeric_response_before'] < 4 and row['numeric_response_after'] < row['numeric_response_before']) or \\\n",
    "                   (row['numeric_response_before'] > 4 and row['numeric_response_after'] > row['numeric_response_before'])  # More extreme\n",
    "\n",
    "    # Check if the non-reinforced response was already at an extreme (either 1 or 5 for F2, or 1 or 7 for others)\n",
    "    def was_stable_at_extreme(row):\n",
    "        if row['question_code'].startswith('F2'):\n",
    "            return row['numeric_response_before'] in [1, 5]  # Extreme for F2 questions\n",
    "        else:\n",
    "            return row['numeric_response_before'] in [1, 7]  # Extreme for other questions\n",
    "\n",
    "    # Apply the reinforcement check safely\n",
    "    right_wing_df['reinforced'] = right_wing_df.apply(is_reinforced, axis=1)\n",
    "\n",
    "    # Check if responses were stable at the extreme ends\n",
    "    right_wing_df['stable_at_extreme'] = right_wing_df.apply(was_stable_at_extreme, axis=1)\n",
    "\n",
    "    # Separate non-reinforced responses\n",
    "    non_reinforced_df = right_wing_df[right_wing_df['reinforced'] == False].copy()\n",
    "\n",
    "    # Check non-extreme responses and see how many were reinforced\n",
    "    non_extreme_responses_df = right_wing_df[right_wing_df['stable_at_extreme'] == False].copy()\n",
    "    non_extreme_reinforced_count = non_extreme_responses_df['reinforced'].sum()\n",
    "\n",
    "    # Calculate percentages safely\n",
    "    total_non_reinforced = len(non_reinforced_df)\n",
    "    stable_at_extreme_count = non_reinforced_df['stable_at_extreme'].sum()\n",
    "\n",
    "    # Prevent division by zero if no non-reinforced responses\n",
    "    percentage_stable_at_extreme = (stable_at_extreme_count / total_non_reinforced * 100) if total_non_reinforced > 0 else 0\n",
    "\n",
    "    total_non_extreme_responses = len(non_extreme_responses_df)\n",
    "    percentage_non_extreme_reinforced = (non_extreme_reinforced_count / total_non_extreme_responses * 100) if total_non_extreme_responses > 0 else 0\n",
    "\n",
    "    # Calculate non-extreme responses that were not reinforced\n",
    "    non_extreme_not_reinforced_count = total_non_extreme_responses - non_extreme_reinforced_count\n",
    "    percentage_non_extreme_not_reinforced = (non_extreme_not_reinforced_count / total_non_extreme_responses * 100) if total_non_extreme_responses > 0 else 0\n",
    "\n",
    "    # Return results rounded to two decimal places\n",
    "    return {\n",
    "        'total_right_wing_responses': len(right_wing_df),\n",
    "        'reinforced_responses': right_wing_df['reinforced'].sum(),\n",
    "        'percentage_reinforced': round((right_wing_df['reinforced'].sum() / len(right_wing_df)) * 100, 2),\n",
    "        'non_reinforced_responses': total_non_reinforced,\n",
    "        'stable_at_extreme_count': stable_at_extreme_count,\n",
    "        'percentage_stable_at_extreme': round(percentage_stable_at_extreme, 2),\n",
    "        'non_extreme_reinforced_count': non_extreme_reinforced_count,\n",
    "        'percentage_non_extreme_reinforced': round(percentage_non_extreme_reinforced, 2),\n",
    "        'non_extreme_not_reinforced_count': non_extreme_not_reinforced_count,\n",
    "        'percentage_non_extreme_not_reinforced': round(percentage_non_extreme_not_reinforced, 2)\n",
    "    }\n",
    "\n",
    "# Perform the extended radicalization analysis\n",
    "right_wing_extended_results = analyze_reinforcement_and_stable_extremes(merged_responses_df)\n",
    "\n",
    "# Display the results\n",
    "print(\"Reinforcement of Right-Wing Opinions (Extreme Right Users, Right-Biased Articles):\")\n",
    "print(f\"Total right-wing responses: {right_wing_extended_results['total_right_wing_responses']}\")\n",
    "print(f\"Reinforced responses: {right_wing_extended_results['reinforced_responses']}\")\n",
    "print(f\"Percentage of reinforced responses: {right_wing_extended_results['percentage_reinforced']}%\")\n",
    "print(f\"Non-reinforced responses: {right_wing_extended_results['non_reinforced_responses']}\")\n",
    "print(f\"Stable non-reinforced responses at extremes: {right_wing_extended_results['stable_at_extreme_count']}\")\n",
    "print(f\"Percentage of stable non-reinforced responses at extremes: {right_wing_extended_results['percentage_stable_at_extreme']}%\")\n",
    "print(f\"Non-extreme responses reinforced: {right_wing_extended_results['non_extreme_reinforced_count']}\")\n",
    "print(f\"Percentage of non-extreme responses reinforced: {right_wing_extended_results['percentage_non_extreme_reinforced']}%\")\n",
    "print(f\"Non-extreme responses not reinforced: {right_wing_extended_results['non_extreme_not_reinforced_count']}\")\n",
    "print(f\"Percentage of non-extreme responses not reinforced: {right_wing_extended_results['percentage_non_extreme_not_reinforced']}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement of right wing opinions if left wing users are exposed to left wing biased articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement of Left-Wing Opinions (Extreme Left Users, Left-Biased Articles):\n",
      "Total left-wing responses: 392\n",
      "Reinforced responses: 79\n",
      "Percentage of reinforced responses: 20.15%\n",
      "Non-reinforced responses: 313\n",
      "Stable non-reinforced responses at extremes: 231\n",
      "Percentage of stable non-reinforced responses at extremes: 73.8%\n",
      "Non-extreme responses reinforced: 79\n",
      "Percentage of non-extreme responses reinforced: 49.07%\n",
      "Non-extreme responses not reinforced: 82\n",
      "Percentage of non-extreme responses not reinforced: 50.93%\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze reinforcement, stable extremes, and non-extreme responses not reinforced for left-wing users\n",
    "def analyze_reinforcement_and_stable_extremes_left_wing(merged_df):\n",
    "    # Filter for Extreme Left users and left-wing biased articles\n",
    "    left_wing_df = merged_df[(merged_df['political_stance'] == 'Extreme Left') & (merged_df['bias'] == 'left')].copy()\n",
    "\n",
    "    # Function to check if the response was reinforced (became more extreme)\n",
    "    def is_reinforced(row):\n",
    "        if row['question_code'].startswith('F2'):  # F2 questions (concern scale)\n",
    "            return row['numeric_response_after'] > row['numeric_response_before']  # More concern\n",
    "        else:  # Agreement scale\n",
    "            return (row['numeric_response_before'] < 4 and row['numeric_response_after'] < row['numeric_response_before']) or \\\n",
    "                   (row['numeric_response_before'] > 4 and row['numeric_response_after'] > row['numeric_response_before'])  # More extreme\n",
    "\n",
    "    # Check if the non-reinforced response was already at an extreme (either 1 or 5 for F2, or 1 or 7 for others)\n",
    "    def was_stable_at_extreme(row):\n",
    "        if row['question_code'].startswith('F2'):\n",
    "            return row['numeric_response_before'] in [1, 5]  # Extreme for F2 questions\n",
    "        else:\n",
    "            return row['numeric_response_before'] in [1, 7]  # Extreme for other questions\n",
    "\n",
    "    # Apply the reinforcement check safely\n",
    "    left_wing_df['reinforced'] = left_wing_df.apply(is_reinforced, axis=1)\n",
    "\n",
    "    # Check if responses were stable at the extreme ends\n",
    "    left_wing_df['stable_at_extreme'] = left_wing_df.apply(was_stable_at_extreme, axis=1)\n",
    "\n",
    "    # Separate non-reinforced responses\n",
    "    non_reinforced_df = left_wing_df[left_wing_df['reinforced'] == False].copy()\n",
    "\n",
    "    # Check non-extreme responses and see how many were reinforced\n",
    "    non_extreme_responses_df = left_wing_df[left_wing_df['stable_at_extreme'] == False].copy()\n",
    "    non_extreme_reinforced_count = non_extreme_responses_df['reinforced'].sum()\n",
    "\n",
    "    # Calculate percentages safely\n",
    "    total_non_reinforced = len(non_reinforced_df)\n",
    "    stable_at_extreme_count = non_reinforced_df['stable_at_extreme'].sum()\n",
    "\n",
    "    # Prevent division by zero if no non-reinforced responses\n",
    "    percentage_stable_at_extreme = (stable_at_extreme_count / total_non_reinforced * 100) if total_non_reinforced > 0 else 0\n",
    "\n",
    "    total_non_extreme_responses = len(non_extreme_responses_df)\n",
    "    percentage_non_extreme_reinforced = (non_extreme_reinforced_count / total_non_extreme_responses * 100) if total_non_extreme_responses > 0 else 0\n",
    "\n",
    "    # Calculate non-extreme responses that were not reinforced\n",
    "    non_extreme_not_reinforced_count = total_non_extreme_responses - non_extreme_reinforced_count\n",
    "    percentage_non_extreme_not_reinforced = (non_extreme_not_reinforced_count / total_non_extreme_responses * 100) if total_non_extreme_responses > 0 else 0\n",
    "\n",
    "    # Return results rounded to two decimal places\n",
    "    return {\n",
    "        'total_left_wing_responses': len(left_wing_df),\n",
    "        'reinforced_responses': left_wing_df['reinforced'].sum(),\n",
    "        'percentage_reinforced': round((left_wing_df['reinforced'].sum() / len(left_wing_df)) * 100, 2),\n",
    "        'non_reinforced_responses': total_non_reinforced,\n",
    "        'stable_at_extreme_count': stable_at_extreme_count,\n",
    "        'percentage_stable_at_extreme': round(percentage_stable_at_extreme, 2),\n",
    "        'non_extreme_reinforced_count': non_extreme_reinforced_count,\n",
    "        'percentage_non_extreme_reinforced': round(percentage_non_extreme_reinforced, 2),\n",
    "        'non_extreme_not_reinforced_count': non_extreme_not_reinforced_count,\n",
    "        'percentage_non_extreme_not_reinforced': round(percentage_non_extreme_not_reinforced, 2)\n",
    "    }\n",
    "\n",
    "# Perform the extended radicalization analysis for left-wing users\n",
    "left_wing_extended_results = analyze_reinforcement_and_stable_extremes_left_wing(merged_responses_df)\n",
    "\n",
    "# Display the results\n",
    "print(\"Reinforcement of Left-Wing Opinions (Extreme Left Users, Left-Biased Articles):\")\n",
    "print(f\"Total left-wing responses: {left_wing_extended_results['total_left_wing_responses']}\")\n",
    "print(f\"Reinforced responses: {left_wing_extended_results['reinforced_responses']}\")\n",
    "print(f\"Percentage of reinforced responses: {left_wing_extended_results['percentage_reinforced']}%\")\n",
    "print(f\"Non-reinforced responses: {left_wing_extended_results['non_reinforced_responses']}\")\n",
    "print(f\"Stable non-reinforced responses at extremes: {left_wing_extended_results['stable_at_extreme_count']}\")\n",
    "print(f\"Percentage of stable non-reinforced responses at extremes: {left_wing_extended_results['percentage_stable_at_extreme']}%\")\n",
    "print(f\"Non-extreme responses reinforced: {left_wing_extended_results['non_extreme_reinforced_count']}\")\n",
    "print(f\"Percentage of non-extreme responses reinforced: {left_wing_extended_results['percentage_non_extreme_reinforced']}%\")\n",
    "print(f\"Non-extreme responses not reinforced: {left_wing_extended_results['non_extreme_not_reinforced_count']}\")\n",
    "print(f\"Percentage of non-extreme responses not reinforced: {left_wing_extended_results['percentage_non_extreme_not_reinforced']}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM to persona alignment before vs after exposure to articles: \n",
    "(is the llm more consistent with the actual person's responses after it was exposed to a. right biased article b. left biased articles.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the filtered data (replace the path with the actual location of the CSV file)\n",
    "filtered_data_path = '../data/processed/filtered_data.csv'\n",
    "real_responses = pd.read_csv(filtered_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_difference(real_response_code, llm_response_code, weight=1):\n",
    "    \"\"\"\n",
    "    Calculates the weighted difference between real and LLM response codes.\n",
    "    \n",
    "    Parameters:\n",
    "    - real_response_code: The real response code (numeric or categorical).\n",
    "    - llm_response_code: The response code generated by the LLM (numeric or categorical).\n",
    "    - weight: A scaling factor for the difference. Defaults to 1 (no weighting).\n",
    "    \n",
    "    Returns:\n",
    "    - The weighted difference between the real and LLM response codes.\n",
    "    \"\"\"\n",
    "    # Ensure the codes are comparable by converting them to float (if they are numeric)\n",
    "    try:\n",
    "        real_response_code = float(real_response_code)\n",
    "        llm_response_code = float(llm_response_code)\n",
    "    except ValueError:\n",
    "        # If not numeric, calculate difference as 1 if codes are different, 0 if the same\n",
    "        return weight if real_response_code != llm_response_code else 0\n",
    "    \n",
    "    # Calculate the absolute difference between the codes, weighted by the provided factor\n",
    "    difference = abs(real_response_code - llm_response_code)\n",
    "    return difference * weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_llm_responses(after_responses_df):\n",
    "    \"\"\"\n",
    "    Extracts LLM responses from after_responses_df and formats them into a list of dictionaries.\n",
    "    Now includes 'bias' from the article.\n",
    "\n",
    "    Returns:\n",
    "    - llm_responses_mapped (list): A list of LLM response dictionaries, where each dictionary contains\n",
    "                                   'user_id', 'question_code', 'llm_response_code', and 'bias'.\n",
    "    \"\"\"\n",
    "    llm_responses_mapped = []\n",
    "    \n",
    "    # Extract index values from the multiindex in after_responses_df\n",
    "    for (user_id, question_code, bias), row in after_responses_df.iterrows():\n",
    "        llm_response_code = row['numeric_response']  # Assuming numeric_response contains the response\n",
    "        \n",
    "        # Append the response dictionary to the list, including the bias\n",
    "        llm_responses_mapped.append({\n",
    "            'user_id': user_id,\n",
    "            'question_code': question_code,\n",
    "            'llm_response_code': llm_response_code,\n",
    "            'bias': bias\n",
    "        })\n",
    "\n",
    "    return llm_responses_mapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_llm_to_real(llm_responses_mapped, filtered_data):\n",
    "    \"\"\"\n",
    "    Compare LLM responses to real responses and calculate the weighted differences.\n",
    "    Now includes the 'bias' in the final DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - llm_responses_mapped (list): A list of LLM response dictionaries, where each dictionary contains\n",
    "                                   'user_id', 'question_code', 'llm_response_code', and 'bias'.\n",
    "    - filtered_data (DataFrame): A pandas DataFrame containing real responses, with user_id as the index\n",
    "                                 and question_code as the columns.\n",
    "\n",
    "    Returns:\n",
    "    - comparison_df (DataFrame): A pandas DataFrame containing the comparison results, including 'bias'.\n",
    "    - average_weighted_difference (float): The average weighted difference between LLM and real responses.\n",
    "    \"\"\"\n",
    "    llm_vs_real_comparison = []\n",
    "\n",
    "    # Iterate through the LLM responses\n",
    "    for llm_response in llm_responses_mapped:\n",
    "        user_id = llm_response['user_id']\n",
    "        question_code = llm_response['question_code']\n",
    "        llm_response_code = llm_response['llm_response_code']\n",
    "        bias = llm_response['bias']\n",
    "        \n",
    "        # Check if the user_id and question_code exist in filtered_data\n",
    "        if user_id in filtered_data.index and question_code in filtered_data.columns:\n",
    "            # Fetch the real response\n",
    "            real_response = filtered_data.loc[user_id, question_code]\n",
    "            \n",
    "            # If real_response is a Series (multiple values), extract the first valid value\n",
    "            if isinstance(real_response, pd.Series):\n",
    "                real_response_code = real_response.iloc[0]  # Get the first value\n",
    "            else:\n",
    "                real_response_code = real_response\n",
    "            \n",
    "            # Check if the real response code is valid (not NaN)\n",
    "            if pd.notna(real_response_code):\n",
    "                # Calculate the weighted difference using the custom function\n",
    "                difference = weighted_difference(real_response_code, llm_response_code)\n",
    "                \n",
    "                # Store the comparison\n",
    "                llm_vs_real_comparison.append({\n",
    "                    'user_id': user_id,\n",
    "                    'question_code': question_code,\n",
    "                    'llm_response_code': llm_response_code,\n",
    "                    'real_response_code': real_response_code,\n",
    "                    'difference': difference,\n",
    "                    'bias': bias  # Include bias in the comparison DataFrame\n",
    "                })\n",
    "\n",
    "    # Convert the comparison results to a DataFrame for analysis\n",
    "    comparison_df = pd.DataFrame(llm_vs_real_comparison)\n",
    "\n",
    "    # Calculate the average weighted difference\n",
    "    if not comparison_df.empty:\n",
    "        average_weighted_difference = comparison_df['difference'].mean()\n",
    "    else:\n",
    "        average_weighted_difference = None  # Handle the case where there's no valid comparison\n",
    "\n",
    "    return comparison_df, average_weighted_difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id question_code  llm_response_code  real_response_code  \\\n",
      "0  IDUS103408       F1A10_1                  5                   2   \n",
      "1  IDUS103408       F1A10_1                  1                   2   \n",
      "2  IDUS103408          F2A7                  5                   3   \n",
      "3  IDUS103408          F2A7                  2                   3   \n",
      "4  IDUS103408          F2A9                  2                   2   \n",
      "\n",
      "   difference   bias  \n",
      "0         3.0   left  \n",
      "1         1.0  right  \n",
      "2         2.0   left  \n",
      "3         1.0  right  \n",
      "4         0.0   left  \n",
      "Average weighted difference between LLM and real responses: 1.9743589743589745\n"
     ]
    }
   ],
   "source": [
    "# Set the unique_id as the index for real_responses_df\n",
    "real_responses.set_index('unique_id', inplace=True)\n",
    "\n",
    "# Assuming real_responses is the DataFrame containing the actual (real) responses from filtered_data.csv\n",
    "comparison_df, avg_difference = compare_llm_to_real(llm_responses_mapped, real_responses)\n",
    "\n",
    "# Display the comparison DataFrame\n",
    "print(comparison_df.head())\n",
    "\n",
    "# Display the average weighted difference\n",
    "print(f\"Average weighted difference between LLM and real responses: {avg_difference}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id question_code  llm_response_code  real_response_code  \\\n",
      "0  IDUS103408       F1A10_1                  5                   2   \n",
      "1  IDUS103408       F1A10_1                  1                   2   \n",
      "2  IDUS103408          F2A7                  5                   3   \n",
      "3  IDUS103408          F2A7                  2                   3   \n",
      "4  IDUS103408          F2A9                  2                   2   \n",
      "\n",
      "   difference   bias  \n",
      "0         3.0   left  \n",
      "1         1.0  right  \n",
      "2         2.0   left  \n",
      "3         1.0  right  \n",
      "4         0.0   left  \n",
      "Average weighted difference between LLM and real responses: 1.9743589743589745\n"
     ]
    }
   ],
   "source": [
    "# Extract LLM responses from after_responses_df with bias\n",
    "llm_responses_mapped = extract_llm_responses(after_responses_df)\n",
    "\n",
    "# Compare LLM responses to real responses with bias tracking\n",
    "comparison_df, avg_difference = compare_llm_to_real(llm_responses_mapped, real_responses)\n",
    "\n",
    "# Display the results\n",
    "print(comparison_df.head())\n",
    "print(f\"Average weighted difference between LLM and real responses: {avg_difference}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference beteween LLM and Real Responses by Article Bias\n",
    "Comparing the responses of the real people with the responses of their LLM counterparts that had been exposed to politically biased articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_responses_by_bias(comparison_df, bias_type):\n",
    "    \"\"\"\n",
    "    Analyzes the comparison between real and LLM responses, filtered by bias.\n",
    "    \n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - bias_type (str): The bias type to filter by ('right' or 'left'). Default is 'right'.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_difference (float): The average weighted difference for the selected bias type.\n",
    "    - filtered_comparison_df (DataFrame): The filtered DataFrame containing only the selected bias type.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame based on the selected bias\n",
    "    filtered_comparison_df = comparison_df[comparison_df['bias'] == bias_type]\n",
    "    \n",
    "    # Calculate the average weighted difference for the selected bias\n",
    "    avg_difference = filtered_comparison_df['difference'].mean()\n",
    "    \n",
    "    return avg_difference, filtered_comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison for Right-Biased Articles:\n",
      "      user_id question_code  llm_response_code  real_response_code  \\\n",
      "1  IDUS103408       F1A10_1                  1                   2   \n",
      "3  IDUS103408          F2A7                  2                   3   \n",
      "5  IDUS103408          F2A9                  5                   2   \n",
      "7  IDUS103408        F3A3_1                  5                   6   \n",
      "9  IDUS103408        F3A6_1                  1                   4   \n",
      "\n",
      "   difference   bias  \n",
      "1         1.0  right  \n",
      "3         1.0  right  \n",
      "5         3.0  right  \n",
      "7         1.0  right  \n",
      "9         3.0  right  \n",
      "Average weighted difference for right-biased articles: 1.9462759462759462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage:\n",
    "# Analyze responses for right-biased articles\n",
    "avg_difference_right, filtered_right_df = analyze_responses_by_bias(comparison_df, bias_type='right')\n",
    "\n",
    "# Display the results\n",
    "print(\"Comparison for Right-Biased Articles:\")\n",
    "print(filtered_right_df.head())\n",
    "print(f\"Average weighted difference for right-biased articles: {avg_difference_right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison for Left-Biased Articles:\n",
      "      user_id question_code  llm_response_code  real_response_code  \\\n",
      "0  IDUS103408       F1A10_1                  5                   2   \n",
      "2  IDUS103408          F2A7                  5                   3   \n",
      "4  IDUS103408          F2A9                  2                   2   \n",
      "6  IDUS103408        F3A3_1                  1                   6   \n",
      "8  IDUS103408        F3A6_1                  2                   4   \n",
      "\n",
      "   difference  bias  \n",
      "0         3.0  left  \n",
      "2         2.0  left  \n",
      "4         0.0  left  \n",
      "6         5.0  left  \n",
      "8         2.0  left  \n",
      "Average weighted difference for left-biased articles: 2.0024420024420024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage:\n",
    "# Analyze responses for right-biased articles\n",
    "avg_difference_right, filtered_right_df = analyze_responses_by_bias(comparison_df, bias_type='left')\n",
    "\n",
    "# Display the results\n",
    "print(\"Comparison for Left-Biased Articles:\")\n",
    "print(filtered_right_df.head())\n",
    "print(f\"Average weighted difference for left-biased articles: {avg_difference_right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does Exposing left-wing mimicking LLMs to left biased articles improve their alignment with their real-human counterparts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_political_stance_to_real_responses(real_responses_df, after_responses_df):\n",
    "    \"\"\"\n",
    "    Adds the political stance to the real_responses_df by mapping from after_responses_df using 'user_id'.\n",
    "    \n",
    "    Parameters:\n",
    "    - real_responses_df (DataFrame): The real responses DataFrame that lacks political stance.\n",
    "    - after_responses_df (DataFrame): The after responses DataFrame that contains user_id and political_stance.\n",
    "    \n",
    "    Returns:\n",
    "    - real_responses_df (DataFrame): The real responses DataFrame with added political stance.\n",
    "    \"\"\"\n",
    "    # Reset index to bring 'user_id' and 'question_code' as columns in after_responses_df\n",
    "    after_responses_df_reset = after_responses_df.reset_index()\n",
    "    \n",
    "    # Ensure after_responses_df has political stance and user_id\n",
    "    stance_mapping = after_responses_df_reset[['user_id', 'political_stance']].drop_duplicates()\n",
    "    \n",
    "    # Merge the political stance into the real_responses_df using the index (which is unique_id)\n",
    "    real_responses_with_stance = real_responses_df.merge(stance_mapping, left_index=True, right_on='user_id', how='left')\n",
    "    \n",
    "    return real_responses_with_stance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_reinforcement_patterns(df, political_stance, bias_type):\n",
    "    \"\"\"\n",
    "    Track reinforcement patterns for users with a given political stance exposed to biased articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the real and LLM responses, along with political stance and bias.\n",
    "    - political_stance (str): The political stance to filter by (e.g., 'Extreme Left' or 'Extreme Right').\n",
    "    - bias_type (str): The type of article bias to filter by (e.g., 'left' or 'right').\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with statistics about reinforcement patterns.\n",
    "    \"\"\"\n",
    "    # Filter for the appropriate users and bias\n",
    "    filtered_df = df[(df['political_stance'] == political_stance) & (df['bias'] == bias_type)].copy()\n",
    "\n",
    "    # Determine reinforcement for each response\n",
    "    filtered_df.loc[:, 'reinforced'] = filtered_df.apply(is_reinforced, axis=1)\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_responses = len(filtered_df)\n",
    "    reinforced_responses = filtered_df['reinforced'].sum()\n",
    "    non_reinforced_responses = total_responses - reinforced_responses\n",
    "\n",
    "    return {\n",
    "        'total_responses': total_responses,\n",
    "        'reinforced_responses': reinforced_responses,\n",
    "        'percentage_reinforced': (reinforced_responses / total_responses * 100) if total_responses else 0,\n",
    "        'non_reinforced_responses': non_reinforced_responses\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses_for_left_users_and_left_bias(comparison_df, real_responses_with_stance):\n",
    "    \"\"\"\n",
    "    Compares the responses of left-wing real people with their LLM counterparts after exposure to left-wing articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - real_responses_with_stance (DataFrame): The real responses DataFrame with political stance.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_difference (float): The average weighted difference for left-wing users exposed to left-biased articles.\n",
    "    - filtered_comparison_df (DataFrame): The filtered DataFrame containing left-wing users and left-biased articles.\n",
    "    \"\"\"\n",
    "    # Step 1: Ensure the correct index for real_responses_with_stance is set to 'user_id'\n",
    "    real_responses_with_stance = real_responses_with_stance.set_index('user_id')\n",
    "    \n",
    "    # Step 2: Filter for left-wing users from real_responses_with_stance\n",
    "    left_wing_users = real_responses_with_stance[real_responses_with_stance['political_stance'] == 'Extreme Left'].index.unique()\n",
    "    \n",
    "    print(f\"Found {len(left_wing_users)} left-wing users:\")\n",
    "\n",
    "    # Step 3: Clean and ensure user ID matching (if necessary)\n",
    "    comparison_df['user_id'] = comparison_df['user_id'].astype(str).str.strip()  # Ensure 'user_id' is a string and strip whitespace\n",
    "    \n",
    "    # Step 4: Filter the comparison_df for those users and left-biased articles\n",
    "    filtered_comparison_df = comparison_df[\n",
    "        (comparison_df['user_id'].isin(left_wing_users)) &\n",
    "        (comparison_df['bias'] == 'left')\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(filtered_comparison_df)} entries with left bias for left-wing users.\")\n",
    "\n",
    "\n",
    "    # Step 5: Calculate the average weighted difference for left-wing users exposed to left-biased articles\n",
    "    avg_difference = filtered_comparison_df['difference'].mean()\n",
    "    \n",
    "    return avg_difference, filtered_comparison_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left Wing Agents Right Articles - Human Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses_for_left_users_and_right_bias(comparison_df, real_responses_with_stance):\n",
    "    \"\"\"\n",
    "    Compares the responses of left-wing real people with their LLM counterparts after exposure to right-wing articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - real_responses_with_stance (DataFrame): The real responses DataFrame with political stance.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_difference (float): The average weighted difference for left-wing users exposed to right-biased articles.\n",
    "    - filtered_comparison_df (DataFrame): The filtered DataFrame containing left-wing users and right-biased articles.\n",
    "    \"\"\"\n",
    "    # Step 1: Ensure the correct index for real_responses_with_stance is set to 'user_id'\n",
    "    real_responses_with_stance = real_responses_with_stance.set_index('user_id')\n",
    "    \n",
    "    # Step 2: Filter for left-wing users from real_responses_with_stance\n",
    "    left_wing_users = real_responses_with_stance[real_responses_with_stance['political_stance'] == 'Extreme Left'].index.unique()\n",
    "    \n",
    "    print(f\"Found {len(left_wing_users)} left-wing users:\")\n",
    "\n",
    "    # Step 3: Clean and ensure user ID matching (if necessary)\n",
    "    comparison_df['user_id'] = comparison_df['user_id'].astype(str).str.strip()  # Ensure 'user_id' is a string and strip whitespace\n",
    "    \n",
    "    # Step 4: Filter the comparison_df for those users and right-biased articles\n",
    "    filtered_comparison_df = comparison_df[\n",
    "        (comparison_df['user_id'].isin(left_wing_users)) &\n",
    "        (comparison_df['bias'] == 'right')\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(filtered_comparison_df)} entries with right bias for left-wing users.\")\n",
    "\n",
    "    # Step 5: Calculate the average weighted difference for left-wing users exposed to right-biased articles\n",
    "    avg_difference = filtered_comparison_df['difference'].mean()\n",
    "    \n",
    "    return avg_difference, filtered_comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does Exposing right-wing mimicking LLMs to left biased articles improve their alignment with their real-human counterparts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses_for_right_users_and_right_bias(comparison_df, real_responses_with_stance):\n",
    "    \"\"\"\n",
    "    Compares the responses of right-wing real people with their LLM counterparts after exposure to right-wing articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - real_responses_with_stance (DataFrame): The real responses DataFrame with political stance.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_difference (float): The average weighted difference for right-wing users exposed to right-biased articles.\n",
    "    - filtered_comparison_df (DataFrame): The filtered DataFrame containing right-wing users and right-biased articles.\n",
    "    \"\"\"\n",
    "    # Step 1: Ensure the correct index for real_responses_with_stance is set to 'user_id'\n",
    "    real_responses_with_stance = real_responses_with_stance.set_index('user_id')\n",
    "    \n",
    "    # Step 2: Filter for right-wing users from real_responses_with_stance\n",
    "    right_wing_users = real_responses_with_stance[real_responses_with_stance['political_stance'] == 'Extreme Right'].index.unique()\n",
    "    \n",
    "    print(f\"Found {len(right_wing_users)} right-wing users:\")\n",
    "\n",
    "    # Step 3: Clean and ensure user ID matching (if necessary)\n",
    "    comparison_df['user_id'] = comparison_df['user_id'].astype(str).str.strip()  # Ensure 'user_id' is a string and strip whitespace\n",
    "    \n",
    "    # Step 4: Filter the comparison_df for those users and right-biased articles\n",
    "    filtered_comparison_df = comparison_df[\n",
    "        (comparison_df['user_id'].isin(right_wing_users)) &\n",
    "        (comparison_df['bias'] == 'right')\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(filtered_comparison_df)} entries with right bias for right-wing users.\")\n",
    "\n",
    "    # Step 5: Calculate the average weighted difference for right-wing users exposed to right-biased articles\n",
    "    avg_difference = filtered_comparison_df['difference'].mean()\n",
    "    \n",
    "    return avg_difference, filtered_comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison_results(comparison_df, real_responses, after_responses_df):\n",
    "    \"\"\"\n",
    "    Compares and displays results for left-wing and right-wing users exposed to both left- and right-biased articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - real_responses (DataFrame): The real responses DataFrame that lacks political stance.\n",
    "    - after_responses_df (DataFrame): The after responses DataFrame that contains user_id and political_stance.\n",
    "    \"\"\"\n",
    "    # Step 1: Add political stance to real_responses\n",
    "    real_responses_with_stance = add_political_stance_to_real_responses(real_responses, after_responses_df)\n",
    "\n",
    "    # ---- Right-Wing Users ----\n",
    "    print(\"\\n--- Right-Wing Users Comparisons ---\\n\")\n",
    "\n",
    "    # Compare responses of right-wing users with right-biased articles\n",
    "    avg_difference_right, filtered_right_df = compare_responses_for_right_users_and_right_bias(comparison_df, real_responses_with_stance)\n",
    "    print(\"Comparison for Right-Wing Users Exposed to Right-Biased Articles:\")\n",
    "    print(f\"Average weighted difference for right-wing users and right-biased articles: {avg_difference_right}\")\n",
    "    print()\n",
    "\n",
    "    # Compare responses of right-wing users with left-biased articles\n",
    "    avg_difference_right_left, filtered_right_left_df = compare_responses_for_right_users_and_left_bias(comparison_df, real_responses_with_stance)\n",
    "    print(\"Comparison for Right-Wing Users Exposed to Left-Biased Articles:\")\n",
    "    print(f\"Average weighted difference for right-wing users and left-biased articles: {avg_difference_right_left}\")\n",
    "    print()\n",
    "\n",
    "    # ---- Left-Wing Users ----\n",
    "    print(\"\\n--- Left-Wing Users Comparisons ---\\n\")\n",
    "\n",
    "    # Compare responses of left-wing users with left-biased articles\n",
    "    avg_difference_left, filtered_left_df = compare_responses_for_left_users_and_left_bias(comparison_df, real_responses_with_stance)\n",
    "    print(\"Comparison for Left-Wing Users Exposed to Left-Biased Articles:\")\n",
    "    print(f\"Average weighted difference for left-wing users and left-biased articles: {avg_difference_left}\")\n",
    "    print()\n",
    "\n",
    "    # Compare responses of left-wing users with right-biased articles\n",
    "    avg_difference_left_right, filtered_left_right_df = compare_responses_for_left_users_and_right_bias(comparison_df, real_responses_with_stance)\n",
    "    print(\"Comparison for Left-Wing Users Exposed to Right-Biased Articles:\")\n",
    "    print(f\"Average weighted difference for left-wing users and right-biased articles: {avg_difference_left_right}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Right-Wing Users Comparisons ---\n",
      "\n",
      "Found 68 right-wing users:\n",
      "Found 476 entries with right bias for right-wing users.\n",
      "Comparison for Right-Wing Users Exposed to Right-Biased Articles:\n",
      "Average weighted difference for right-wing users and right-biased articles: 1.9453781512605042\n",
      "\n",
      "Found 68 right-wing users:\n",
      "Found 476 entries with left bias for right-wing users.\n",
      "Comparison for Right-Wing Users Exposed to Left-Biased Articles:\n",
      "Average weighted difference for right-wing users and left-biased articles: 2.0105042016806722\n",
      "\n",
      "\n",
      "--- Left-Wing Users Comparisons ---\n",
      "\n",
      "Found 49 left-wing users:\n",
      "Found 343 entries with left bias for left-wing users.\n",
      "Comparison for Left-Wing Users Exposed to Left-Biased Articles:\n",
      "Average weighted difference for left-wing users and left-biased articles: 1.9912536443148687\n",
      "\n",
      "Found 49 left-wing users:\n",
      "Found 343 entries with right bias for left-wing users.\n",
      "Comparison for Left-Wing Users Exposed to Right-Biased Articles:\n",
      "Average weighted difference for left-wing users and right-biased articles: 1.9475218658892128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_comparison_results(comparison_df, real_responses, after_responses_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Wing Users Exposed to Left-Biased Articles Human Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses_for_right_users_and_left_bias(comparison_df, real_responses_with_stance):\n",
    "    \"\"\"\n",
    "    Compares the responses of right-wing real people with their LLM counterparts after exposure to left-wing articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - comparison_df (DataFrame): The DataFrame containing real and LLM response comparisons.\n",
    "    - real_responses_with_stance (DataFrame): The real responses DataFrame with political stance.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_difference (float): The average weighted difference for right-wing users exposed to left-biased articles.\n",
    "    - filtered_comparison_df (DataFrame): The filtered DataFrame containing right-wing users and left-biased articles.\n",
    "    \"\"\"\n",
    "    # Step 1: Ensure the correct index for real_responses_with_stance is set to 'user_id'\n",
    "    real_responses_with_stance = real_responses_with_stance.set_index('user_id')\n",
    "    \n",
    "    # Step 2: Filter for right-wing users from real_responses_with_stance\n",
    "    right_wing_users = real_responses_with_stance[real_responses_with_stance['political_stance'] == 'Extreme Right'].index.unique()\n",
    "    \n",
    "    print(f\"Found {len(right_wing_users)} right-wing users:\")\n",
    "\n",
    "    # Step 3: Clean and ensure user ID matching (if necessary)\n",
    "    comparison_df['user_id'] = comparison_df['user_id'].astype(str).str.strip()  # Ensure 'user_id' is a string and strip whitespace\n",
    "    \n",
    "    # Step 4: Filter the comparison_df for those users and left-biased articles\n",
    "    filtered_comparison_df = comparison_df[\n",
    "        (comparison_df['user_id'].isin(right_wing_users)) &\n",
    "        (comparison_df['bias'] == 'left')\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(filtered_comparison_df)} entries with left bias for right-wing users.\")\n",
    "\n",
    "    # Step 5: Calculate the average weighted difference for right-wing users exposed to left-biased articles\n",
    "    avg_difference = filtered_comparison_df['difference'].mean()\n",
    "    \n",
    "    return avg_difference, filtered_comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radicalisation LLMs and Humans \n",
    "Instead of comparing LLMs to LLMs for radicalisation analysis, as done before, we now compare LLMs and humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_reinforced(row):\n",
    "    \"\"\"\n",
    "    Determines if the LLM response is a reinforcement of the user's original stance.\n",
    "    Reinforcement happens if the LLM response moves in the direction of the original political stance.\n",
    "    \n",
    "    For Extreme Left:\n",
    "      - Moving towards smaller numbers (more extreme left) is considered reinforcement.\n",
    "    For Extreme Right:\n",
    "      - Moving towards larger numbers (more extreme right) is considered reinforcement.\n",
    "    \n",
    "    Returns True if reinforcement is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    if row['political_stance'] == 'Extreme Left' and row['bias'] == 'left':\n",
    "        return row['llm_response_code'] < row['real_response_code']  # moving towards more extreme left\n",
    "    elif row['political_stance'] == 'Extreme Right' and row['bias'] == 'right':\n",
    "        return row['llm_response_code'] > row['real_response_code']  # moving towards more extreme right\n",
    "    return False  # No reinforcement otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_reinforcement_patterns(df, political_stance, bias_type):\n",
    "    \"\"\"\n",
    "    Track reinforcement patterns for users with a given political stance exposed to biased articles.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the real and LLM responses, along with political stance and bias.\n",
    "    - political_stance (str): The political stance to filter by (e.g., 'Extreme Left' or 'Extreme Right').\n",
    "    - bias_type (str): The type of article bias to filter by (e.g., 'left' or 'right').\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with statistics about reinforcement patterns.\n",
    "    \"\"\"\n",
    "    # Filter for the appropriate users and bias\n",
    "    filtered_df = df[(df['political_stance'] == political_stance) & (df['bias'] == bias_type)].copy()\n",
    "\n",
    "    # Determine reinforcement for each response\n",
    "    filtered_df.loc[:, 'reinforced'] = filtered_df.apply(is_reinforced, axis=1)\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_responses = len(filtered_df)\n",
    "    reinforced_responses = filtered_df['reinforced'].sum()\n",
    "    non_reinforced_responses = total_responses - reinforced_responses\n",
    "\n",
    "    return {\n",
    "        'total_responses': total_responses,\n",
    "        'reinforced_responses': reinforced_responses,\n",
    "        'percentage_reinforced': (reinforced_responses / total_responses * 100) if total_responses else 0,\n",
    "        'non_reinforced_responses': non_reinforced_responses\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement patterns for left-wing users exposed to left-biased articles:\n",
      "{'total_responses': 392, 'reinforced_responses': np.int64(109), 'percentage_reinforced': np.float64(27.806122448979593), 'non_reinforced_responses': np.int64(283)}\n",
      "Reinforcement patterns for right-wing users exposed to right-biased articles:\n",
      "{'total_responses': 544, 'reinforced_responses': np.int64(148), 'percentage_reinforced': np.float64(27.205882352941174), 'non_reinforced_responses': np.int64(396)}\n"
     ]
    }
   ],
   "source": [
    "# Ensure columns are correctly named in the DataFrame\n",
    "merged_responses_df.rename(columns={\n",
    "    'numeric_response_after': 'llm_response_code',\n",
    "    'numeric_response_before': 'real_response_code'\n",
    "}, inplace=True)\n",
    "\n",
    "# Example usage: Track reinforcement patterns for left-wing users exposed to left-biased articles\n",
    "reinforcement_stats_left = track_reinforcement_patterns(merged_responses_df, 'Extreme Left', 'left')\n",
    "reinforcement_stats_right = track_reinforcement_patterns(merged_responses_df, 'Extreme Right', 'right')\n",
    "\n",
    "# Output the reinforcement stats\n",
    "print(\"Reinforcement patterns for left-wing users exposed to left-biased articles:\")\n",
    "print(reinforcement_stats_left)\n",
    "\n",
    "print(\"Reinforcement patterns for right-wing users exposed to right-biased articles:\")\n",
    "print(reinforcement_stats_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do llms show similar patterns of radicalisation as humans? \n",
    "# TODO: Check alignment with humans question by question after article exposure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
